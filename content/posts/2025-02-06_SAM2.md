---
title: 'Segment Anything 2 vs. SAM1: What’s New and Why It Matters'
date: '2025-02-06T12:19:07-05:00'
draft: false
params:
  math: true
tags: [Segmentation]
---
![cover](https://about.fb.com/wp-content/uploads/2024/07/01_Dribbling_Carousel-02.gif?fit=800%2C697)

In my [last post](https://baampark.github.io/posts/2025-01-29_sam/), we explored how Segment Anything (SAM) works in image segmentation, breaking down the key components of its model architecture. SAM achieved great success in image segmentation, demonstrating two key strengths: its foundation as a large-scale model trained on an extensive dataset and its ability to be promptable, allowing users to generate segmentations with flexible inputs. These two strengths allow SAM to deliver impressive performance in a zero-shot setting. In Jul 2024, ["SAM 2: Segment Anything in Images and Videos"](https://arxiv.org/abs/2408.00714) was published. While SAM focuses solely on image segmentation, SAM2 takes things a step further. Not only does it improve performance in image segmentation, but it also introduces the ability to handle video segmentation, thanks to its own memory system. This enhancement allows SAM2 to track objects across frames, making it a powerful tool for dynamic and real-time applications. Personally, when I look at SAM2's zero-shot performance in tracking objects across video frames , I truly believe this is a game-changer in the world of object tracking. In this article, we are gonna mainly talk about **memory bank** system of SAM2 as the rest of the parts are built on top of SAM. 

## SAM2 - Model Architecture
![model_architecture](/images/2025-02-06_SAM2/model_architecture.png)
If you read my previous post, you might recognize some familiar components in the SAM2 architecture diagram: the image encoder, prompt decoder, and mask decoder—all key elements carried over from SAM. So, what’s new in SAM2? The introduction of three critical components: **memory attention module**, **memory encoder**, and **memory bank**. 

The memory encoder generates a memory by combining frame embedding and mask prediction across frames. The memory is sent to the memory bank, which retains the memory of past predictions for the target object. The memory attention module leverages the memory stored in the memory bank to enhance object recognition and segmentation across frames. These three components allows SAM2 to generate maskelt prediction, which means track of mask in a video. Even if you don’t provide prompts for the target object in the current video frame, SAM2 can still recognize and segment the target object based on previous prompts you provided in earlier frames. In addition, even if the target object is occluded in the past frame and reappears in the current frame, SAM2 can recover the segmentation. 

## Memory Encoder
No, the input to the MemoryEncoder in the provided code is spatial feature information, not explicitly spatio-temporal features.

## Reference
- [SAM 2: Segment Anything in Images and Videos](https://arxiv.org/abs/2408.00714)