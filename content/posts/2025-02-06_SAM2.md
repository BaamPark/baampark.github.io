---
title: 'Segment Anything 2 vs. SAM1: What’s New and Why It Matters'
date: '2025-02-06T12:19:07-05:00'
draft: false
params:
  math: true
tags: [Segmentation]
layout: "SAM"
---
![cover](https://about.fb.com/wp-content/uploads/2024/07/01_Dribbling_Carousel-02.gif?fit=800%2C697)

In my [last post](https://baampark.github.io/posts/2025-01-29_sam/), we explored how Segment Anything (SAM) works in image segmentation, breaking down the key components of its model architecture. SAM achieved great success in image segmentation, demonstrating two key strengths: its foundation as a large-scale model trained on an extensive dataset and its ability to be promptable, allowing users to generate segmentations with flexible inputs. These two strengths allow SAM to deliver impressive performance in a zero-shot setting. In Jul 2024, ["SAM 2: Segment Anything in Images and Videos"](https://arxiv.org/abs/2408.00714) was published. While SAM focuses solely on image segmentation, SAM2 takes things a step further. Not only does it improve performance in image segmentation, but it also introduces the ability to handle video segmentation, thanks to its own memory system. This enhancement allows SAM2 to track objects across frames, making it a powerful tool for dynamic and real-time applications. Personally, when I look at SAM2's zero-shot performance in tracking objects across video frames , I truly believe this is a game-changer in the world of object tracking. In this article, we are gonna mainly talk about **memory bank** system of SAM2 as the rest of the parts are built on top of SAM. 

## SAM2 - Model Architecture
![model_architecture](/images/2025-02-06_SAM2/model_architecture.png)
If you read my previous post, you might recognize some familiar components in the SAM2 architecture diagram: the image encoder, prompt decoder, and mask decoder—all key elements carried over from SAM. So, what’s new in SAM2? The introduction of three critical components: **memory attention module**, **memory encoder**, and **memory bank**. 

The memory encoder generates a memory by combining frame embedding and mask prediction across frames. The memory is sent to the memory bank, which retains the memory of past predictions for the target object. The memory attention module leverages the memory stored in the memory bank to enhance object recognition and segmentation across frames. These three components allows SAM2 to generate maskelt prediction, which means track of mask in a video. **Even if you don’t provide prompts for the target object in the current video frame, SAM2 can still recognize and segment the target object based on previous prompts you provided in earlier frames. In addition, even if the target object is occluded in the past frame and reappears in the current frame, SAM2 can recover the segmentation.** 

## Memory Encoder
![memory_encoder](/images/2025-02-06_SAM2/memory_encoder.png)
The Memory Encoder generates a memory representation by taking image embeddings and mask outputs as inputs. The image embedding \(I\) is produced by the image encoder, which processes the \(1024 \times 1024\) input image and embeds it into feature space. The mask output \(M\) is produced from the mask decoder. However, this mask does not come from the current frame—it is obtained from past frames. As the mask is a binary value tensor, it has one channel with the same height and width as input image (i.e. \(1 \times H \times W\)).
The dimensions for these two inputs are:
- Image Embedding \(I \in \mathbb{R} ^ {B \times 256 \times 64 \times 64}\)
- Mask Output \(M \in \mathbb{R} ^ {B \times 1 \times 1024 \times 1024} \)

As shown in the diagram, we need to perform element-wise addition between \(M\) and \(I\). We cannot add them directly because their dimensions are different. To resolve this, we use a down-sampler to project \(M\) into the same dimension as \(I\). However, element-wise addition alone is not sufficient to effectively combine these two inputs, as it only aligns them spatially without learning meaningful interactions. To better fuse the information from both inputs, the authors apply convolutional layers with a \(1 \times 1\) kernel, reducing the channel dimension. The generated memory \(\mathcal{M}\) will be used later in the memory attention module. In general, an attention mechanism requires not only an input feature but also its positional encoding to capture spatial relationships. Likewise, we need to compute the positional encoding of \(\mathcal{M}\) within the memory encoder block. Therefore, we have two outputs:
- Memory \(\mathcal{M} \in \mathbb{R} ^{B \times 64 \times 64 \times 64} \)
- Positional Embedding of Memory \(\text{PE}(\mathcal{M}) \in \mathbb{R} ^{B \times 64 \times 64 \times 64}\)

Refer implementation for details: [mask encoder](https://github.com/facebookresearch/sam2/blob/main/sam2/modeling/memory_encoder.py)

<!-- You might wonder why the memory encoder does not explicitly process a 5D tensor (B, C, T, H, W), where \(T\) represents the temporal dimension. When we hear the term "memory", we often associate it with spatio-temporal features. However, the Memory Encoder only operates within the spatial feature space, meaning each frame is processed independently.

Then, how does SAM2 incorporate temporal information despite working with spatial features? -->

## Memory Bank

## Memory Attention





## Application using SAM2: SAMURAI
SAMURAI is a zero-shot visual object tracking built on top of SAM2.
They added motion modeling that can filter high confident mask

For clarity, visual object tracking differs from multi-object tracking.
Visual object tracking (VOT) tracks a single object given in the first frame, while multi-object tracking (MOT) tracks multiple objects across frames while maintaining their identities.

## Reference
- [SAM 2: Segment Anything in Images and Videos](https://arxiv.org/abs/2408.00714)
- [Segment Anything 2: What Is the Secret Sauce? (A Deep Learner’s Guide)](https://towardsdatascience.com/segment-anything-2-what-is-the-secret-sauce-a-deep-learners-guide-1c43dd07a6f8/?source=rss----7f60cf5620c9---4)