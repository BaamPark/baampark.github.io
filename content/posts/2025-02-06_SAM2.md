---
title: 'Segment Anything 2 vs. SAM1: What’s New and Why It Matters'
date: '2025-02-06T12:19:07-05:00'
draft: false
params:
  math: true
tags: [Segmentation]
layout: "SAM"
---
![cover](https://about.fb.com/wp-content/uploads/2024/07/01_Dribbling_Carousel-02.gif?fit=800%2C697)

In my [last post](https://baampark.github.io/posts/2025-01-29_sam/), we explored how Segment Anything (SAM) works in image segmentation, breaking down the key components of its model architecture. SAM achieved great success in image segmentation, demonstrating two key strengths: its foundation as a large-scale model trained on an extensive dataset and its ability to be promptable, allowing users to generate segmentations with flexible inputs. These two strengths allow SAM to deliver impressive performance in a zero-shot setting. In Jul 2024, ["SAM 2: Segment Anything in Images and Videos"](https://arxiv.org/abs/2408.00714) was published. While SAM focuses solely on image segmentation, SAM2 takes things a step further. Not only does it improve performance in image segmentation, but it also introduces the ability to handle video segmentation, thanks to its own memory system. This enhancement allows SAM2 to track objects across frames, making it a powerful tool for dynamic and real-time applications. Personally, when I look at SAM2's zero-shot performance in tracking objects across video frames , I believe this is a game-changer in the world of object tracking. In this article, we are gonna mainly talk about **memory bank** system of SAM2 as the rest of the parts are built on top of SAM. 

## SAM2 - Model Architecture
![model_architecture](/images/2025-02-06_SAM2/model_architecture.png)
If you read my previous post, you might recognize some familiar components in the SAM2 architecture diagram: the image encoder, prompt decoder, and mask decoder—all key elements carried over from SAM. So, what’s new in SAM2? The introduction of three critical components: **memory attention module**, **memory encoder**, and **memory bank**. 

The memory encoder generates a memory by combining frame embedding and mask prediction across frames. The memory is sent to the memory bank, which retains the memory of past predictions for the target object. The memory attention module leverages the memory stored in the memory bank to enhance object recognition and segmentation across frames. These three components allows SAM2 to generate maskelt prediction, which means track of mask in a video. **Even if you don’t provide prompts for the target object in the current video frame, SAM2 can still recognize and segment the target object based on previous prompts you provided in earlier frames. In addition, even if the target object is occluded in the past frame and reappears in the current frame, SAM2 can recover the segmentation.** 

## Memory Encoder
![memory_encoder](/images/2025-02-06_SAM2/memory_encoder.png)
The Memory Encoder generates a memory representation by taking image embeddings and mask outputs as inputs. The image embedding \(I\) is produced by the image encoder, which processes the \(1024 \times 1024\) input image and embeds it into feature space. The mask output \(M\) is produced from the mask decoder. However, this mask does not come from the current frame—it is obtained from past frames. As the mask is a binary value tensor, it has one channel with the same height and width as input image (i.e. \(1 \times H \times W\)).
The dimensions for these two inputs are:
- Image Embedding \(I \in \mathbb{R} ^ {B \times 256 \times 64 \times 64}\)
- Mask Output \(M \in \mathbb{R} ^ {B \times 1 \times 1024 \times 1024} \)

As shown in the diagram, we need to perform element-wise addition between \(M\) and \(I\). We cannot add them directly because their dimensions are different. To resolve this, we use a down-sampler to project \(M\) into the same dimension as \(I\). However, element-wise addition alone is not sufficient to effectively combine these two inputs, as it only aligns them spatially without learning meaningful interactions. To better fuse the information from both inputs, the authors apply convolutional layers with a \(1 \times 1\) kernel, reducing the channel dimension. The generated memory \(\mathcal{M}\) will be used later in the memory attention module. In general, an attention mechanism requires not only an input feature but also its positional encoding to capture spatial relationships. Likewise, we need to compute the positional encoding of \(\mathcal{M}\) within the memory encoder block. Therefore, we have two outputs:
- Memory \(\mathcal{M} \in \mathbb{R} ^{B \times 64 \times 64 \times 64} \)
- Positional Embedding of Memory \(\text{PE}(\mathcal{M}) \in \mathbb{R} ^{B \times 64 \times 64 \times 64}\)

Refer to the implementation for details: [mask encoder](https://github.com/facebookresearch/sam2/blob/main/sam2/modeling/memory_encoder.py)

<!-- You might wonder why the memory encoder does not explicitly process a 5D tensor (B, C, T, H, W), where \(T\) represents the temporal dimension. When we hear the term "memory", we often associate it with spatio-temporal features. However, the Memory Encoder only operates within the spatial feature space, meaning each frame is processed independently.

Then, how does SAM2 incorporate temporal information despite working with spatial features? -->

## Memory Attention
> *The role of memory attention is to condition the current frame features on the past frames features and predictions as well as on any new prompts.*

The authors used the term *condition*. So what does condition mean? Conditioning in this context refers to incorporating past frame embeddings (both image and mask-based features) into the processing of the current frame. But how?

The memory attention module utilizes both self-attention and cross-attention mechanisms. Through self-attention, the current frame embedding \(I\) attends to itself internally. Through cross attention, \(I\) attends to memory \(\mathcal{M}\), integrating information from past frames. The memory attention layer consists of a self-attention block, followed by a cross-attention block. The inputs to this layer are:
- Image Embedding \(I \in \mathbb{R}^{B \times\ 4096 \times 256}\)
- Image Positional Embedding \(\text{PE}(I) \in \mathbb{R}^{B \times\ 4096 \times 256}\)
- Memory \(\mathcal{M} \in \mathbb{R}^{B \times 4010 \times 64}\)
- Memory Positional Embedding \(\text{PE}(\mathcal{M}) \in \mathbb{R}^{B \times 4010 \times 64}\)

The sequence dimension of \(I\) is \(4096\), reshaped from \(64 \times 64\). However, you might wonder why the sequence dimension of \(\mathcal{M}\) is \(4010\) (\(4096 + 4\)). Where does the additional four tokens come from? These four tokens come from **object pointers** and are concatenated to memory, resulting in \(4010\) senquence length. See what the authors talk about object pointer.
>*In addition to the spatial memory, we store a list of object pointers as lightweight vectors for high-level semantic information of the object to segment, based on mask decoder output tokens of each frame. Our memory attention cross-attends to both spatial memory features and these object pointers. ... Further, we project the memory features in our memory bank to a dimension of 64, and split the 256-dim object pointer into 4 tokens of 64-dim for cross-attention to the memory bank.*

![mask_attention_layer](/images/2025-02-06_SAM2/object_pointer.png)

The object pointer is derived from the first output token of the SAM mask decoder. Instead of relying solely on raw mask features from memory, object pointers provide a compressed representation of object instances. Consider an object that disappears behind another object for a few frames. If we only rely on spatial memory, the model might lose track of the object due to inconsistency between mask features across frames. In contrast, the object pointer provides a more stable and consistent representation of an object across frames.

Let's circle back to the memory attention layer. The diagram below shows a memory attention layer. For simplicity, normalization, dropout, and MLP layers are excluded from the diagram. By default, memory attention has four memory attention layers. 
Each layer first applies self-attention, allowing the current frame embedding to refine itself by attending to its own spatial features. Then, cross-attention enables the current frame to incorporate relevant information from the memory. **As a result, the memory attention module outputs the conditioned frame feature \(I_{I|\mathcal{M}} \in \mathbb{R}^{B \times 4096 \times 256}\)**, which is subsequently used as input to the mask decoder, instead of unconditioned image embedding \(I\).

![mask_attention_layer](/images/2025-02-06_SAM2/memory_attention_layer.png)

In the self-attention block, \(I + \text{PE}(I)\) for the query and key, while \(I\) is passed as the value. This raised a major question for me because, typically, the query, key, and value are the same—each being the input feature with added positional embedding. This query-key-value modeling was inspired from DETR (DEtection TRansformer).

>*It starts by computing so-called query, key and value embeddings after adding the query and key positional encodings - [DETR](https://arxiv.org/pdf/2005.12872)* 

![DETR](/images/2025-02-06_SAM2/DETR.png)

Positional encoding is essential for self-attention because transformers are inherently permutation-invariant—they lack an inherent sense of sequence order. By adding positional encoding to queries and keys, the model can learn spatial relationships and distinguish between positions. However, is it necessary to apply positional encoding to values if it doesn’t improve performance? If not, it would only introduce unnecessary computational overhead.

## Memory Bank
>The memory bank retains information about past predictions for the target object in the video by maintaining a FIFO queue of memories of up to N recent frames and stores information from prompts in a FIFO queue of up to M prompted frames. For instance, in the VOS task where the initial mask is the only prompt, the memory bank consistently retains the first frame’s memory along with memories of up to N recent (unprompted) frames. Both sets of memories are stored as spatial feature maps.

``` python
class SAM2Base(torch.nn.Module):
  def __init__(...):
    self.num_maskmem = num_maskmem  # Number of memories accessible

    # Temporal encoding of the memories (for distinguishing different frames)
    self.maskmem_tpos_enc = torch.nn.Parameter(
        torch.zeros(num_maskmem, 1, 1, self.mem_dim)
    )
    trunc_normal_(self.maskmem_tpos_enc, std=0.02)

    # Placeholder token for cases where there is no memory available
    self.no_mem_embed = torch.nn.Parameter(torch.zeros(1, 1, self.hidden_dim))
    self.no_mem_pos_enc = torch.nn.Parameter(torch.zeros(1, 1, self.hidden_dim))
    trunc_normal_(self.no_mem_embed, std=0.02)
    trunc_normal_(self.no_mem_pos_enc, std=0.02)

    # Memory bank updates within `_prepare_memory_conditioned_features`
    # This function selects past frames to condition the current frame
    def _prepare_memory_conditioned_features(...):
        ...
        # Step 1: Retrieve previously stored memory features
        if not is_init_cond_frame:
            to_cat_memory, to_cat_memory_pos_embed = [], []
            
            # Store the first conditioning frame's memory permanently
            assert len(output_dict["cond_frame_outputs"]) > 0
            cond_outputs = output_dict["cond_frame_outputs"]
            selected_cond_outputs, unselected_cond_outputs = select_closest_cond_frames(
                frame_idx, cond_outputs, self.max_cond_frames_in_attn
            )
            t_pos_and_prevs = [(0, out) for out in selected_cond_outputs.values()]
            
            # Keep the most recent `self.num_maskmem - 1` past frames
            stride = 1 if self.training else self.memory_temporal_stride_for_eval
            for t_pos in range(1, self.num_maskmem):
                t_rel = self.num_maskmem - t_pos  # How many frames before the current frame
                prev_frame_idx = frame_idx - t_rel if not track_in_reverse else frame_idx + t_rel
                out = output_dict["non_cond_frame_outputs"].get(prev_frame_idx, None)
                if out is None:
                    out = unselected_cond_outputs.get(prev_frame_idx, None)
                t_pos_and_prevs.append((t_pos, out))
            
            for t_pos, prev in t_pos_and_prevs:
                if prev is None:
                    continue
                # Retrieve memory feature maps from past frames and add temporal encoding
                feats = prev["maskmem_features"].to(device, non_blocking=True)
                to_cat_memory.append(feats.flatten(2).permute(2, 0, 1))
                maskmem_enc = prev["maskmem_pos_enc"][-1].to(device)
                maskmem_enc = maskmem_enc.flatten(2).permute(2, 0, 1)
                maskmem_enc = maskmem_enc + self.maskmem_tpos_enc[self.num_maskmem - t_pos - 1]
                to_cat_memory_pos_embed.append(maskmem_enc)
        
        # Step 2: Use memory features for attention conditioning
        memory = torch.cat(to_cat_memory, dim=0)
        memory_pos_embed = torch.cat(to_cat_memory_pos_embed, dim=0)
        pix_feat_with_mem = self.memory_attention(
            curr=current_vision_feats,
            curr_pos=current_vision_pos_embeds,
            memory=memory,
            memory_pos=memory_pos_embed,
            num_obj_ptr_tokens=num_obj_ptr_tokens,
        )
        return pix_feat_with_mem

    def _track_step(...):
        current_out = {"point_inputs": point_inputs, "mask_inputs": mask_inputs}
        # High-resolution feature maps for the SAM head, reshape (HW)BC => BCHW
        if len(current_vision_feats) > 1:
            high_res_features = [
                x.permute(1, 2, 0).view(x.size(1), x.size(2), *s)
                for x, s in zip(current_vision_feats[:-1], feat_sizes[:-1])
            ]
        else:
            high_res_features = None
        if mask_inputs is not None and self.use_mask_input_as_output_without_sam:
            # When use_mask_input_as_output_without_sam=True, we directly output the mask input
            # (see it as a GT mask) without using a SAM prompt encoder + mask decoder.
            pix_feat = current_vision_feats[-1].permute(1, 2, 0)
            pix_feat = pix_feat.view(-1, self.hidden_dim, *feat_sizes[-1])
            sam_outputs = self._use_mask_as_output(
                pix_feat, high_res_features, mask_inputs
            )
        else:
            # fused the visual feature with previous memory features in the memory bank
            pix_feat = self._prepare_memory_conditioned_features(
                frame_idx=frame_idx,
                is_init_cond_frame=is_init_cond_frame,
                current_vision_feats=current_vision_feats[-1:],
                current_vision_pos_embeds=current_vision_pos_embeds[-1:],
                feat_sizes=feat_sizes[-1:],
                output_dict=output_dict,
                num_frames=num_frames,
                track_in_reverse=track_in_reverse,
            )
            # apply SAM-style segmentation head
            # here we might feed previously predicted low-res SAM mask logits into the SAM mask decoder,
            # e.g. in demo where such logits come from earlier interaction instead of correction sampling
            # (in this case, any `mask_inputs` shouldn't reach here as they are sent to _use_mask_as_output instead)
            if prev_sam_mask_logits is not None:
                assert point_inputs is not None and mask_inputs is None
                mask_inputs = prev_sam_mask_logits
            multimask_output = self._use_multimask(is_init_cond_frame, point_inputs)
            sam_outputs = self._forward_sam_heads(
                backbone_features=pix_feat,
                point_inputs=point_inputs,
                mask_inputs=mask_inputs,
                high_res_features=high_res_features,
                multimask_output=multimask_output,
            )

        return current_out, sam_outputs, high_res_features, pix_feat
```

## Memory Attention





## Application using SAM2: SAMURAI
SAMURAI is a zero-shot visual object tracking built on top of SAM2.
They added motion modeling that can filter high confident mask

For clarity, visual object tracking differs from multi-object tracking.
Visual object tracking (VOT) tracks a single object given in the first frame, while multi-object tracking (MOT) tracks multiple objects across frames while maintaining their identities.

## Reference
- [SAM 2: Segment Anything in Images and Videos](https://arxiv.org/abs/2408.00714)
- [Segment Anything 2: What Is the Secret Sauce? (A Deep Learner’s Guide)](https://towardsdatascience.com/segment-anything-2-what-is-the-secret-sauce-a-deep-learners-guide-1c43dd07a6f8/?source=rss----7f60cf5620c9---4)