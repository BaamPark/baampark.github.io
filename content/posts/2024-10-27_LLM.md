---
title: 'Handling large document challenges for fine-tuning LLM'
date: '2024-10-27T23:10:58-04:00'
draft: true
params:
  math: true
tags: [LLM]
---

![Cover Image](/images/2024-10-27_LLM/cover.png)
In most LLM fine-tuning tutorials, they start with an available online dataset that you can easily use to play around with and fine-tune models. However, with real-world datasets, you might encounter tricky settings when handling large documents. Recently, I attended the Rutgers Health 2024 hackathon, where I had to deal with a document larger than an LLM's context window size. I couldn't find a solution back then, so our team chose a large-sized LLM with a larger context window. In this article, I will talk about fine-tuning small-sized LLMs with large documents.

## Context Window
![attention](/images/2024-10-27_LLM/attention.png)
Most large language models (LLMs) are based on transformer architectures. Transformers leverage a self-attention mechanism, enabling them to capture intricate dependencies between input token (words). In other words, the self-attention mechanism processes multiple tokens simultaneously. The ability to manage multiple tokens at once is referred to as the context window, and its size determines the maximum number of tokens the model can accommodate as input. A larger context window allows the LLM to capture more comprehensive global information from the input, enabling it to better understand and generate contextually relevant content across entire documents. The chatgpt 4o and Gemini 1.5 pro have a context window of 128k. 

So, does larger mean better? Yeah, when we are using trained LLMs. But what about training and fine-tuning LLMs with large context windows? You'd be surprised how easily you can run out of GPU DRAM when fine-tuning those LLMs with just a few documents. I was fine-tuning Microsoft's Phi-2 model, which has a 2k context window, on during the hackathon. The size of dataset was just 1.8 MB and I used two 24GB GPUs for fine-tuning. My computer stopped because GPUs ran out of DRAM. If we have abundant resources with multiple high-end GPUs, choosing LLMs with large context windows is always a good choice. However, if you have limited GPU resources, you might prefer LLMs with smaller context windows.


## Truncation

## Sliding Window

## Large Tokenization

