---
title: 'Segment Anything, the first large-scale foundation model for segmentation'
date: '2025-01-29T13:49:47-05:00'
draft: false
params:
  math: true
tags: [Segmentation]
---
![SAM](/images/2025-01-29_SAM/intro.png)
[Segment Anything (SAM)](https://openaccess.thecvf.com/content/ICCV2023/html/Kirillov_Segment_Anything_ICCV_2023_paper.html) has drawn massive attention in the computer vision community, accumulating an impressive 8,000 citations. Segmentation has long been a crucial yet challenging aspect of computer vision. One of the biggest hurdles? Annotation. Unlike simple bounding boxes, which only require marking the object’s general location, segmentation demands precise pixel-level annotations—an incredibly tedious and time-consuming task for annotators. SAM is one of the first large-scale foundation models for segmentation. What makes SAM truly great is that it’s a promptable model. This means you can use it for various tasks without the need for fine-tuning—also known as zero-shot learning. Unlike LLM, here the prompts for the SAM are points, bounding boxes, and masks. In October 2022, Meta's research team released [SAM2](https://arxiv.org/pdf/2408.00714), extending its capabilities from image segmentation to real-time video segmentation. Given my experience with SAM2, I think this is a game-changer in the object tracking domain, demonstrating long-term object tracking. In this post, we’re going to explore the key components of SAM and also dive into SAM2.

In this post, we’re going to explore the key components of SAM and also dive into SAM2. Whether you're new to these concepts or looking to deepen your understanding, this guide will break things down in a simple and easy-to-follow way. Let’s get started! 🚀.
<!-- I came to conclusion SAM doesn't have to do with interactive learning. -->
<!-- ## Interactive Learning
Interactive learning is a machine learning paradigm, similar to active learning, self-supervised learning, and semi-supervised learning. It focuses on models that continuously learn and improve through interaction with users or their environment. I have a good anology to describe interactive learning.

Let's say you are working on a homework assignment where you need to translate English sentences into Korean. On your first attempt, you have zero knowledge of Korean. The professor provides a partial solution, 10% of correct answer. After studying this incomplete solution, you try again. This time, the professor provides 20% of each translation. You continue this process, receiving progressively more of the correct translation with each attempt. Eventually, when the professor provides the full solution, you would be able to ace the assignment. This is how interactive learning works. The model learns from the teacher's feedback on the student's performance, allowing it to adapt and improve over time.

Now, you might have some sense of interactive learning. Here is the funny truth about the analogy. Actually, the professor didn't have the full solutions in the begining because he was too lazy. What's funnier is that the professor used some of the student's homework to progress the solution after each attempt. Eventually, the professor saved his efforts. I said "lazy," but actually, this is a common problem in the machine learning world. We don't have enough manpower to annotate labels for large datasets. Specifically, annotating segmentation is really painful and tedious work. This is why the authors of SAM employed interactive learning because they wanted to build a large-scale dataset. Specifically, this approach is called interactive segmentation. For interactive segmentation, the annotators need to provide points or boundning boxes as partial labels.  -->

<!-- ## Dataset Construction
The authors refer dataset construction environment as a "data engine" and divide it into three stages to streamline the annotation process:
> The data engine has three stages: (1) a model-assisted manual annotation stage, (2) a semi-automatic stage with a mix of automatically predicted masks and model-assisted annotation, and (3) a fully automatic stage in which our model generates masks without annotator input. 

In the first stage, annotators manually provided points and the model outputted masks. The annotators manually refined the masks. In the second stage, Model auto-generated masks, which have high certainty. Then, annotators labeled missing objects. In the third stage, the model generates masks without annotators' refinement. The input points are given by 32 x 32 grids.

Are they training or annotating? Actually, they are doing both in the same time. Until the second stage, they used 
 Let's see how they train the model. -->

# 1. SAM Architecture Overview
![SAM](/images/2025-01-29_SAM/SAM_architecture.png)
The SAM (Segment Anything Model) architecture consists of three main components: an image encoder, a prompt encoder, and a mask decoder. The image encoder processes the input image to generate an embedding, while the prompt encoder takes user-provided prompts (such as points, boxes, or text) to refine the segmentation. The mask decoder then combines these embeddings and prompts to produce multiple valid segmentation masks, each with an associated confidence score.

## 1.1. Image Encoder
![MAE](/images/2025-01-29_SAM/MAE.png)
The image encoder of SAM (Segment Anything Model) is quite straightforward. The authors pre-trained the Vision Transformer (ViT) using Masked Autoencoder (MAE)—both of which are widely recognized techniques in the computer vision community.

ViT is one of the pioneering large-scale foundation models for image classification. Meanwhile, MAE is well known for its effectiveness in pre-training models. The idea behind MAE is simple yet powerful: it randomly applies zero-masking to some image patches before passing them through the encoder. The decoder then attempts to reconstruct the masked patches, forcing the model to develop a deeper understanding of image structures.

## 1.2. Prompt Encoder
The prompt encoder takes three types inputs: points, bounding boxes, text, and mask. Points, bounding boxes, and text are treated sparse prompt. The mask is treated as dense prompt. However, the authors said that text prompts are just an exploration, so we won't cover them in this article. The encoder econverts sparse prompts into an embedding representation and Dense prompts (masks) are embedded using convolutions and are element-wise summed with the image embedding.

### 1.2.1. Point Embedding
The points and boxes are encoded as "positional encoding". The concept of positional encoding was originated from transformer. Transformers use self-attention to process inputs, but unlike RNNs or CNNs, they do not inherently capture positional information (i.e. permutation-invariant). Positional encoding is added to the input embeddings to provide a sense of order in the sequence. The image below shows how positional embedding and image embedding are added in transformer.

![embeddings](/images/2025-01-29_SAM/embeddings.png)

What would be the dimension of positional encoding? It's \(64 \times 64 \times 256\), which matches the dimension of the image embedding. As the SAM architecture shows, the image content embedding and the positional encoding are added element-wise to produce the final image embedding. Since element-wise addition requires the same dimensions, the positional encoding and the image content embedding must correspond in size.

But how do we encode pixel coordinates into a positional embedding whose dimension is \(64 \times 64 \times 256\)? By the way this is why it's called sparse prompt. we are attempting to encode small information into a relatively large feature space. Let's see the part of the source code below.

``` python
class PromptEncoder(nn.Module):
    def __init__(
      #some arguements...
    )
    super().__init__()
        self.embed_dim = embed_dim
        self.input_image_size = input_image_size
        self.image_embedding_size = image_embedding_size
        self.pe_layer = PositionEmbeddingRandom(embed_dim // 2)
        point_embeddings = [nn.Embedding(1, embed_dim) for i in range(self.num_point_embeddings)]
        self.point_embeddings = nn.ModuleList(point_embeddings)

    def _embed_points(
        self,
        points: torch.Tensor,
        labels: torch.Tensor,
        pad: bool,
    ) -> torch.Tensor:
        """Embeds point prompts."""
        points = points + 0.5  # Shift to center of pixel
        if pad:
            padding_point = torch.zeros((points.shape[0], 1, 2), device=points.device)
            padding_label = -torch.ones((labels.shape[0], 1), device=labels.device)
            points = torch.cat([points, padding_point], dim=1)
            labels = torch.cat([labels, padding_label], dim=1)
        point_embedding = self.pe_layer.forward_with_coords(points, self.input_image_size)
        point_embedding[labels == -1] = 0.0
        point_embedding[labels == -1] += self.not_a_point_embed.weight
        point_embedding[labels == 0] += self.point_embeddings[0].weight
        point_embedding[labels == 1] += self.point_embeddings[1].weight
        return point_embedding

    def forward(
        self,
        points: Optional[Tuple[torch.Tensor, torch.Tensor]],
        boxes: Optional[torch.Tensor],
        masks: Optional[torch.Tensor],
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        bs = self._get_batch_size(points, boxes, masks)
        sparse_embeddings = torch.empty((bs, 0, self.embed_dim), device=self._get_device())
        if points is not None:
            coords, labels = points
            point_embeddings = self._embed_points(coords, labels, pad=(boxes is None))
            sparse_embeddings = torch.cat([sparse_embeddings, point_embeddings], dim=1)
        if boxes is not None:
            box_embeddings = self._embed_boxes(boxes)
            sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=1)
```

Okay, let's go into detail on how points and boxes are encoded.
`self.pe_layer`, which is an object of `PositionEmbeddingRandom`, maps coordinates into a higher-dimensional space. The member function forward_with_coords performs normalization, linear projection, and sinusoidal (periodic) transformation.

\[
\text{PE}(x, y) = \sin\left( 2\pi \cdot W \cdot \begin{bmatrix} 2x - 1 \\ 2y - 1 \end{bmatrix} \right) \oplus \cos\left( 2\pi \cdot W \cdot \begin{bmatrix} 2x - 1 \\ 2y - 1 \end{bmatrix} \right)
\]

where:
- \(x,y \in \mathbb{R}^{B \times N \times 2}\) is input coordinates (batch of 
𝑁
N points per image).
- \(W \in \mathbb{R}^{2 \times d}\) is the Gaussian projection matrix that maps 2D coordinates to a higher-dimensional space.
- \(\oplus\) refers to concatenation along the feature dimension.
- \(\text{PE}(x,y)\) is the final positional encoding after applying sinusoidal functions and concatenation.

Before we analyze what happens after executing `point_embedding = self.pe_layer.forward_with_coords(points, self.input_image_size)` is excuted, let's first discuss positive (foreground) points and negative (background) points. Actually, SAM has a feature that I haven't mentioned yet—you can provide background points. A background point is a point that you are not interested in and explicitly mark as not part of the object. See the image below, or you can test this in the [demo](https://segment-anything.com/demo#).
![foreground_background](/images/2025-01-29_SAM/foreground_background.png)

In the image, the blue dot represents a positive point, while the red dot represents a negative point. So now you know that SAM can take negative points. It makes sense that we update both negative and positive points, but why do we care about missing points `point_embedding[labels == -1]`? Let's say \(N=10\), but you only provide 5 points (including both positive and negative points). Behind the scenes, you are actually providing 5 real points plus 5 dummy (padding) points, so the total always adds up to 10. In other words, \(N\) must be the same across the batch. Since you are working with tensors—and a tensor is essentially a matrix—it must have a consistent shape across all dimensions.

### 1.2.2. Box Encoding
The bounding box has four coordinates \((x_1, y_1, x_2,y_2)\), top left coordinates and bottom left coordinates. 

```python
class PromptEncoder(nn.Module):
    def __init__(
      #some arguements...
    )

    def _embed_boxes(self, boxes: torch.Tensor) -> torch.Tensor:
        """Embeds box prompts."""
        boxes = boxes + 0.5  # Shift to center of pixel
        coords = boxes.reshape(-1, 2, 2)
        corner_embedding = self.pe_layer.forward_with_coords(coords, self.input_image_size)
        corner_embedding[:, 0, :] += self.point_embeddings[2].weight
        corner_embedding[:, 1, :] += self.point_embeddings[3].weight
        return corner_embedding

    def forward(
        self,
        points: Optional[Tuple[torch.Tensor, torch.Tensor]],
        boxes: Optional[torch.Tensor],
        masks: Optional[torch.Tensor],
    ) -> Tuple[torch.Tensor, torch.Tensor]:

        if boxes is not None:
            box_embeddings = self._embed_boxes(boxes)
            sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=1)

```

Before we dive in, let me step back. When I first read the paper, I wondered how the model could take three different types of inputs interchangeably. Normally, if model requires \(n\) types of inputs, you have to provide \(n\) inputs. But with SAM, you can simply provide a point, a box, or a mask—just one is enough.

## Training Algorithm for SAM
Interactive segmentation reuqires point or bounidng box input as prmopts. The prompts are sampled from the ground truth 

Are we training the model or making the dataset

## Image Encoder

## Prompt Encoder

## Mask decoder

## Reference
- [Segment Anything](https://openaccess.thecvf.com/content/ICCV2023/html/Kirillov_Segment_Anything_ICCV_2023_paper.html)
- [A Comprehensive Survey on Segment Anything Model for Vision and Beyond](https://arxiv.org/pdf/2305.08196)
- [Explaining the Segment Anything Model - Network architecture, Dataset, Training](https://www.youtube.com/watch?v=OhxJkqD1vuE&t=280s)
- [Medical image segmentation using deep learning: A survey](https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/ipr2.12419)
- [How Does the Segment-Anything Model’s (SAM’s) Encoder Work?](https://towardsdatascience.com/how-does-the-segment-anything-models-sam-s-encoder-work-003a8a6e3f8b)

