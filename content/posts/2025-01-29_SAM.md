---
title: 'Segment Anything, the first large-scale foundation model for segmentation'
date: '2025-01-29T13:49:47-05:00'
draft: false
params:
  math: true
tags: [Segmentation]
---
![SAM](/images/2025-01-29_SAM/intro.png)
[Segment Anything (SAM)](https://openaccess.thecvf.com/content/ICCV2023/html/Kirillov_Segment_Anything_ICCV_2023_paper.html) has drawn massive attention in the computer vision community, accumulating an impressive 8,000 citations. Segmentation has long been a crucial yet challenging aspect of computer vision. One of the biggest hurdles? Annotation. Unlike simple bounding boxes, which only require marking the objectâ€™s general location, segmentation demands precise pixel-level annotationsâ€”an incredibly tedious and time-consuming task for annotators. SAM is one of the first large-scale foundation models for segmentation. What makes SAM truly great is that itâ€™s a promptable model. This means you can use it for various tasks without the need for fine-tuningâ€”also known as zero-shot learning. Unlike LLM, here the prompts for the SAM are points, bounding boxes, and masks. In October 2022, Meta's research team released [SAM2](https://arxiv.org/pdf/2408.00714), extending its capabilities from image segmentation to real-time video segmentation. Given my experience with SAM2, I think this is a game-changer in the object tracking domain, demonstrating long-term object tracking. In this post, weâ€™re going to explore the key components of SAM and also dive into SAM2.

In this post, weâ€™re going to explore the key components of SAM and also dive into SAM2. Whether you're new to these concepts or looking to deepen your understanding, this guide will break things down in a simple and easy-to-follow way. Letâ€™s get started! ðŸš€.
<!-- I came to conclusion SAM doesn't have to do with interactive learning. -->
<!-- ## Interactive Learning
Interactive learning is a machine learning paradigm, similar to active learning, self-supervised learning, and semi-supervised learning. It focuses on models that continuously learn and improve through interaction with users or their environment. I have a good anology to describe interactive learning.

Let's say you are working on a homework assignment where you need to translate English sentences into Korean. On your first attempt, you have zero knowledge of Korean. The professor provides a partial solution, 10% of correct answer. After studying this incomplete solution, you try again. This time, the professor provides 20% of each translation. You continue this process, receiving progressively more of the correct translation with each attempt. Eventually, when the professor provides the full solution, you would be able to ace the assignment. This is how interactive learning works. The model learns from the teacher's feedback on the student's performance, allowing it to adapt and improve over time.

Now, you might have some sense of interactive learning. Here is the funny truth about the analogy. Actually, the professor didn't have the full solutions in the begining because he was too lazy. What's funnier is that the professor used some of the student's homework to progress the solution after each attempt. Eventually, the professor saved his efforts. I said "lazy," but actually, this is a common problem in the machine learning world. We don't have enough manpower to annotate labels for large datasets. Specifically, annotating segmentation is really painful and tedious work. This is why the authors of SAM employed interactive learning because they wanted to build a large-scale dataset. Specifically, this approach is called interactive segmentation. For interactive segmentation, the annotators need to provide points or boundning boxes as partial labels.  -->

<!-- ## Dataset Construction
The authors refer dataset construction environment as a "data engine" and divide it into three stages to streamline the annotation process:
> The data engine has three stages: (1) a model-assisted manual annotation stage, (2) a semi-automatic stage with a mix of automatically predicted masks and model-assisted annotation, and (3) a fully automatic stage in which our model generates masks without annotator input. 

In the first stage, annotators manually provided points and the model outputted masks. The annotators manually refined the masks. In the second stage, Model auto-generated masks, which have high certainty. Then, annotators labeled missing objects. In the third stage, the model generates masks without annotators' refinement. The input points are given by 32 x 32 grids.

Are they training or annotating? Actually, they are doing both in the same time. Until the second stage, they used 
 Let's see how they train the model. -->

## SAM Architecture Overview
![SAM](/images/2025-01-29_SAM/SAM_architecture.png)
The SAM (Segment Anything Model) architecture consists of three main components: an image encoder, a prompt encoder, and a mask decoder. The image encoder processes the input image to generate an embedding, while the prompt encoder takes user-provided prompts (such as points, boxes, or text) to refine the segmentation. The mask decoder then combines these embeddings and prompts to produce multiple valid segmentation masks, each with an associated confidence score.

### Image Encoder
![MAE(](/images/2025-01-29_SAM/MAE.png)
The image encoder of SAM (Segment Anything Model) is quite straightforward. The authors pre-trained the Vision Transformer (ViT) using Masked Autoencoder (MAE)â€”both of which are widely recognized techniques in the computer vision community.

ViT is one of the pioneering large-scale foundation models for image classification. Meanwhile, MAE is well known for its effectiveness in pre-training models. The idea behind MAE is simple yet powerful: it randomly applies zero-masking to some image patches before passing them through the encoder. The decoder then attempts to reconstruct the masked patches, forcing the model to develop a deeper understanding of image structures.

### Prompt Encoder
THe prompt encoder takes three types inputs: points, bounding boxes,

## Training Algorithm for SAM
Interactive segmentation reuqires point or bounidng box input as prmopts. The prompts are sampled from the ground truth 

Are we training the model or making the dataset

## Image Encoder

## Prompt Encoder

## Mask decoder

## Reference
- [Segment Anything](https://openaccess.thecvf.com/content/ICCV2023/html/Kirillov_Segment_Anything_ICCV_2023_paper.html)
- [A Comprehensive Survey on Segment Anything Model for Vision and Beyond](https://arxiv.org/pdf/2305.08196)
- [Explaining the Segment Anything Model - Network architecture, Dataset, Training](https://www.youtube.com/watch?v=OhxJkqD1vuE&t=280s)
- [Medical image segmentation using deep learning: A survey](https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/ipr2.12419)

