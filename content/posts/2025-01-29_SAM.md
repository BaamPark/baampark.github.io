---
title: 'Segment Anything, the first large-scale foundation model for segmentation'
date: '2025-01-29T13:49:47-05:00'
draft: false
params:
  math: true
tags: [Segmentation]
---
![SAM](/images/2025-01-29_SAM/intro.png)
[Segment Anything (SAM)](https://openaccess.thecvf.com/content/ICCV2023/html/Kirillov_Segment_Anything_ICCV_2023_paper.html) has drawn massive attention in the computer vision community, accumulating an impressive 8,000 citations. Segmentation has long been a crucial yet challenging aspect of computer vision. One of the biggest hurdles? Annotation. Unlike simple bounding boxes, which only require marking the object‚Äôs general location, segmentation demands precise pixel-level annotations‚Äîan incredibly tedious and time-consuming task for annotators. SAM is one of the first large-scale foundation models for segmentation. What makes SAM truly great is that it‚Äôs a promptable model. This means you can use it for various tasks without the need for fine-tuning‚Äîalso known as zero-shot learning. Unlike LLM, here the prompts for the SAM are points, bounding boxes, and masks. In October 2022, Meta's research team released [SAM2](https://arxiv.org/pdf/2408.00714), extending its capabilities from image segmentation to real-time video segmentation. Given my experience with SAM2, I think this is a game-changer in the object tracking domain, demonstrating long-term object tracking. In this post, we‚Äôre going to explore the key components of SAM and also dive into SAM2.

In this post, we‚Äôre going to explore the key components of SAM and also dive into SAM2. Whether you're new to these concepts or looking to deepen your understanding, this guide will break things down in a simple and easy-to-follow way. Let‚Äôs get started! üöÄ.
<!-- I came to conclusion SAM doesn't have to do with interactive learning. -->
<!-- ## Interactive Learning
Interactive learning is a machine learning paradigm, similar to active learning, self-supervised learning, and semi-supervised learning. It focuses on models that continuously learn and improve through interaction with users or their environment. I have a good anology to describe interactive learning.

Let's say you are working on a homework assignment where you need to translate English sentences into Korean. On your first attempt, you have zero knowledge of Korean. The professor provides a partial solution, 10% of correct answer. After studying this incomplete solution, you try again. This time, the professor provides 20% of each translation. You continue this process, receiving progressively more of the correct translation with each attempt. Eventually, when the professor provides the full solution, you would be able to ace the assignment. This is how interactive learning works. The model learns from the teacher's feedback on the student's performance, allowing it to adapt and improve over time.

Now, you might have some sense of interactive learning. Here is the funny truth about the analogy. Actually, the professor didn't have the full solutions in the begining because he was too lazy. What's funnier is that the professor used some of the student's homework to progress the solution after each attempt. Eventually, the professor saved his efforts. I said "lazy," but actually, this is a common problem in the machine learning world. We don't have enough manpower to annotate labels for large datasets. Specifically, annotating segmentation is really painful and tedious work. This is why the authors of SAM employed interactive learning because they wanted to build a large-scale dataset. Specifically, this approach is called interactive segmentation. For interactive segmentation, the annotators need to provide points or boundning boxes as partial labels.  -->

<!-- ## Dataset Construction
The authors refer dataset construction environment as a "data engine" and divide it into three stages to streamline the annotation process:
> The data engine has three stages: (1) a model-assisted manual annotation stage, (2) a semi-automatic stage with a mix of automatically predicted masks and model-assisted annotation, and (3) a fully automatic stage in which our model generates masks without annotator input. 

In the first stage, annotators manually provided points and the model outputted masks. The annotators manually refined the masks. In the second stage, Model auto-generated masks, which have high certainty. Then, annotators labeled missing objects. In the third stage, the model generates masks without annotators' refinement. The input points are given by 32 x 32 grids.

Are they training or annotating? Actually, they are doing both in the same time. Until the second stage, they used 
 Let's see how they train the model. -->

# 1. SAM Architecture Overview
![SAM](/images/2025-01-29_SAM/SAM_architecture.png)
The SAM (Segment Anything Model) architecture consists of three main components: an image encoder, a prompt encoder, and a mask decoder. The image encoder processes the input image to generate an embedding, while the prompt encoder takes user-provided prompts (such as points, boxes, or text) to refine the segmentation. The mask decoder then combines these embeddings and prompts to produce multiple valid segmentation masks, each with an associated confidence score.

## 1.1. Image Encoder
![MAE](/images/2025-01-29_SAM/MAE.png)
The image encoder of SAM (Segment Anything Model) is quite straightforward. The authors pre-trained the Vision Transformer (ViT) using Masked Autoencoder (MAE)‚Äîboth of which are widely recognized techniques in the computer vision community.

ViT is one of the pioneering large-scale foundation models for image classification. Meanwhile, MAE is well known for its effectiveness in pre-training models. The idea behind MAE is simple yet powerful: it randomly applies zero-masking to some image patches before passing them through the encoder. The decoder then attempts to reconstruct the masked patches, forcing the model to develop a deeper understanding of image structures.

## 1.2. Prompt Encoder
The prompt encoder takes three types inputs: points, bounding boxes, text, and mask. Points, bounding boxes, and text are treated sparse prompt. The mask is treated as dense prompt. However, the authors said that text prompts are just an exploration, so we won't cover them in this article. The encoder econverts sparse prompts into an embedding representation and Dense prompts (masks) are embedded using convolutions and are element-wise summed with the image embedding.

### 1.2.1. Point Embedding
The points and boxes are encoded as "positional encoding". The concept of positional encoding was originated from transformer. Transformers use self-attention to process inputs, but unlike RNNs or CNNs, they do not inherently capture positional information (i.e. permutation-invariant). Positional encoding is added to the input embeddings to provide a sense of order in the sequence. The image below shows how positional embedding and image embedding are added in transformer.

![embeddings](/images/2025-01-29_SAM/embeddings.png)

What would be the dimension of positional encoding? It's \(64 \times 64 \times 256\), which matches the dimension of the image embedding. As the SAM architecture shows, the image content embedding and the positional encoding are added element-wise to produce the final image embedding. Since element-wise addition requires the same dimensions, the positional encoding and the image content embedding must correspond in size.

But how do we encode pixel coordinates into a positional embedding whose dimension is \(64 \times 64 \times 256\)? By the way this is why it's called sparse prompt. we are attempting to encode small information into a relatively large feature space. Let's see the part of the source code below.

``` python
class PromptEncoder(nn.Module):
    def __init__(
      #some arguements...
    )
    super().__init__()
        self.embed_dim = embed_dim
        self.input_image_size = input_image_size
        self.image_embedding_size = image_embedding_size
        self.pe_layer = PositionEmbeddingRandom(embed_dim // 2)
        point_embeddings = [nn.Embedding(1, embed_dim) for i in range(self.num_point_embeddings)]
        self.point_embeddings = nn.ModuleList(point_embeddings)

    def _embed_points(
        self,
        points: torch.Tensor,
        labels: torch.Tensor,
        pad: bool,
    ) -> torch.Tensor:
        """Embeds point prompts."""
        points = points + 0.5  # Shift to center of pixel
        if pad:
            padding_point = torch.zeros((points.shape[0], 1, 2), device=points.device)
            padding_label = -torch.ones((labels.shape[0], 1), device=labels.device)
            points = torch.cat([points, padding_point], dim=1)
            labels = torch.cat([labels, padding_label], dim=1)
        point_embedding = self.pe_layer.forward_with_coords(points, self.input_image_size)
        point_embedding[labels == -1] = 0.0
        point_embedding[labels == -1] += self.not_a_point_embed.weight
        point_embedding[labels == 0] += self.point_embeddings[0].weight
        point_embedding[labels == 1] += self.point_embeddings[1].weight
        return point_embedding

    def forward(
        self,
        points: Optional[Tuple[torch.Tensor, torch.Tensor]],
        boxes: Optional[torch.Tensor],
        masks: Optional[torch.Tensor],
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        bs = self._get_batch_size(points, boxes, masks)
        sparse_embeddings = torch.empty((bs, 0, self.embed_dim), device=self._get_device())
        if points is not None:
            coords, labels = points
            point_embeddings = self._embed_points(coords, labels, pad=(boxes is None))
            sparse_embeddings = torch.cat([sparse_embeddings, point_embeddings], dim=1)
```

Okay, let's go into detail on how points and boxes are encoded.
`self.pe_layer`, which is an object of `PositionEmbeddingRandom`, maps coordinates into a higher-dimensional space. The member function forward_with_coords performs normalization, linear projection, and sinusoidal (periodic) transformation.

\[
\text{PE}(x, y) = \sin\left( 2\pi \cdot W \cdot \begin{bmatrix} 2x - 1 \\ 2y - 1 \end{bmatrix} \right) \oplus \cos\left( 2\pi \cdot W \cdot \begin{bmatrix} 2x - 1 \\ 2y - 1 \end{bmatrix} \right)
\]

where:
- \(x,y \in \mathbb{R}^{B \times N \times 2}\) is input coordinates (batch of 
ùëÅ points per image).
- \(W \in \mathbb{R}^{2 \times d}\) is the Gaussian projection matrix that maps 2D coordinates to a higher-dimensional space.
- \(\oplus\) refers to concatenation along the feature dimension.
- \(\text{PE}(x,y) \in \mathbb{R} ^{B \times N \times 2d}\) is the final positional encoding after applying sinusoidal functions and concatenation.

Before we analyze what happens after executing `point_embedding = self.pe_layer.forward_with_coords(points, self.input_image_size)` is excuted, let's first discuss positive (foreground) points and negative (background) points. Actually, SAM has a feature that I haven't mentioned yet‚Äîyou can provide background points. A background point is a point that you are not interested in and explicitly mark as not part of the object. See the image below, or you can test this in the [demo](https://segment-anything.com/demo#).
![foreground_background](/images/2025-01-29_SAM/foreground_background.png)

In the image, the blue dot represents a positive point, while the red dot represents a negative point. So now you know that SAM can take negative points. It makes sense that we update both negative and positive points, but why do we care about missing points, `point_embedding[labels == -1]`? Let's say \(N=10\), but you only provide 5 points (including both positive and negative points). Behind the scenes, you are actually providing 5 real points plus 5 dummy (padding) points, so the total always adds up to 10. In other words, \(N\) must be the same across the batch. Since you are working with tensors‚Äîand a tensor is essentially a matrix‚Äîit must have a consistent shape across all dimensions.

But wait‚Äîwhy do we need labels for the forward pass? In this context, labels are not ground truth segmentation masks. Instead, they indicate whether each click is a positive (foreground) or negative (background) point when passing inputs to the prompt encoder. So, the label is an array of size \(N\), where each entry is either 1 (positive) or 0 (negative).

### 1.2.2. Box Encoding
The bounding box is defined by four coordinates \((x_1, y_1, x_2,y_2)\), representing the top-left and bottom-right corners.


```python
class PromptEncoder(nn.Module):
    def __init__(
      #some arguements...
    )

    def _embed_points(self, points, labels, pad):
         #skip...
        return point_embedding

    def _embed_boxes(self, boxes: torch.Tensor) -> torch.Tensor:
        """Embeds box prompts."""
        boxes = boxes + 0.5  # Shift to center of pixel
        coords = boxes.reshape(-1, 2, 2)
        corner_embedding = self.pe_layer.forward_with_coords(coords, self.input_image_size)
        corner_embedding[:, 0, :] += self.point_embeddings[2].weight
        corner_embedding[:, 1, :] += self.point_embeddings[3].weight
        return corner_embedding

    def forward(
        self,
        points: Optional[Tuple[torch.Tensor, torch.Tensor]],
        boxes: Optional[torch.Tensor],
        masks: Optional[torch.Tensor],
    ) -> Tuple[torch.Tensor, torch.Tensor]:

        bs = self._get_batch_size(points, boxes, masks)
        sparse_embeddings = torch.empty((bs, 0, self.embed_dim), device=self._get_device())

        if points is not None:
            coords, labels = points
            point_embeddings = self._embed_points(coords, labels, pad=(boxes is None))
            sparse_embeddings = torch.cat([sparse_embeddings, point_embeddings], dim=1)

        if boxes is not None:
            box_embeddings = self._embed_boxes(boxes)
            sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=1)

```

Similar to points, these coordinates are mapped to positional encodings using sinusoidal transformation. However, unlike `point_embedding`, which consists of `N` points, `corner_embedding` represents only one bounding box per image. You can see this from the line, `coords = boxes.reshape(-1, 2, 2)`, which reshapes the input into \(B \times 2\). Here, the last two dimensions represent the (x, y) coordinates of the two corners (top-left and bottom-right) of the bounding box. 
After mapping the box to positional embedding, we add learnable parameters to the top-left and bottom-right corners.
``` python
        corner_embedding[:, 0, :] += self.point_embeddings[2].weight
        corner_embedding[:, 1, :] += self.point_embeddings[3].weight
```
Eventually, `box_embeddings` will have the dimension of \(\mathbb{R}^{B,2,2d}\).

Let's take a look at how `sparse_embeddings` are udpated. `sparse_embedding` is initiallized with empty tensor with the dimension of \(\mathbb{R}^{B \times N \times C}\) where \(C=2d\). 
If both points and a box are provided as input, the prompt encoder concatenates `sparse_embeddings` with `point_embeddings` and `box_embeddings`, updating its shape accordingly. Eventually, the final sparse prompt embedding's dimension will be:
 \[
    \text{PE}_{\text{sparse}} \in \mathbb{R}^{B \times N+2 \times C}.
 \]

I have a question for you. What will be the dimension of \(\text{PE}_{\text{sparse}}\) when only points prompt is given without bounding box. What about only bounding box is given? We have three input scenarios
- \(N\) points prompts, No bounding box ‚û°Ô∏è \(\mathbb{R}^{B \times N \times C}\)
- No points prompts, one bounding box ‚û°Ô∏è \(\mathbb{R}^{B \times 2 \times C}\)
- \(N\) points prompts, one bounding box ‚û°Ô∏è \(\mathbb{R}^{B \times N + 2 \times C}\)

Something is odd. How we can forward pass tensor that has different sequence length (middle dimension) for each pass? If you can't answer this question, you can read my previous post. Simply put, there is no `nn.Linear` (project layer) in prompt encoder so we don't need to care about variable-length sequnces.

## Training Algorithm for SAM
Interactive segmentation reuqires point or bounidng box input as prmopts. The prompts are sampled from the ground truth 

Are we training the model or making the dataset

## Image Encoder

## Prompt Encoder

## Mask decoder

## Reference
- [Segment Anything](https://openaccess.thecvf.com/content/ICCV2023/html/Kirillov_Segment_Anything_ICCV_2023_paper.html)
- [A Comprehensive Survey on Segment Anything Model for Vision and Beyond](https://arxiv.org/pdf/2305.08196)
- [Explaining the Segment Anything Model - Network architecture, Dataset, Training](https://www.youtube.com/watch?v=OhxJkqD1vuE&t=280s)
- [Medical image segmentation using deep learning: A survey](https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/ipr2.12419)
- [How Does the Segment-Anything Model‚Äôs (SAM‚Äôs) Encoder Work?](https://towardsdatascience.com/how-does-the-segment-anything-models-sam-s-encoder-work-003a8a6e3f8b)
- [Transformer self-attention padding and causal masking technique](https://www.youtube.com/watch?v=n13-r_eStb0)

