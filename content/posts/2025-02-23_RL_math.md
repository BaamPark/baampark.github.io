---
title: 'Mathmatical Foundation for Reinforcement Learning'
date: '2025-02-23T15:04:51-05:00'
draft: False
params:
  math: true
tags: [Reinforcement Learning]
---

## 1. Markov Property
The Markov Property is a fundamental concept in probability theory that states that the future state of a process depends only on its current state and not on the sequence of events that preceded it.

---
### 1.1. Random Process
A random process (also known as a stochastic process) is a collection of random variables indexed by time. 
\[\{X_t, t \in [0, \infty)\}\] 

It’s often used to model real-world data that changes unpredictably. One common example of a real-world random process is stock prices. At any given moment \(t\), the price of a stock will vary due to these unpredictable influences, making it a prime example of a stochastic process.

---
### 1.2. Markov Process
A Markov process (markov chain) is a special type of random process that satisfies the Markov property. The Markov property states that the future state of the process depends only on the current state and not on any previous states. Formally:
\[
P(S_{t+1} = s' \mid S_t = s, S_{t-1} = s_{t-1}, \ldots, S_0 = s_0) = P(S_{t+1} = s' \mid S_t = s)
\]

Where
- \( S_t \) represents the state of at time \( t \).
- \(P(S_{t+1} = s' \mid S_t = s)\) denotes state probability.

Let's think how Markov came up with this modeling. We can assume that the stock price \(S_t+1\) might depend on previous stock price \(S_{t}, S_{t-1}, \cdots, S_{0}\). But Markov said no! The stock price at the next time step \(X_{t+1}\)only depends on the current price \(S_t\) and not on the entire history of previous prices. Markov believed that in many real-world processes, including finance, weather prediction, and other systems, the most recent information captures all the relevant data needed to predict future behavior. This assumption simplifies modeling because we don't need to consider complex historical dependencies.

---
### 1.3. State Transition Matrix
![state_transition_diagram](/images/2025-02-23_RL_math/state_transition_diagram.png)

The above figure is an example of state transition diagram used to visualize markov chain problem. The number between states is a transition probability \(P(S_{t+1} = s' \mid S_t = s)\). Let's say we start from state 1 (i.e. \(t=0\) and \(s=1\)). The transition probability \(P(S_{1} = 2 \mid S_{0} = 1)\) is 1/3. The example diagram can be represented as a state transition matrix:

\[
P = \begin{bmatrix}
\frac{1}{4} & \frac{1}{2} & \frac{1}{4} \\
\frac{1}{3} & 0 & \frac{2}{3} \\
\frac{1}{4} & 0 & \frac{1}{2}
\end{bmatrix}
\]

State transition matrix follows a property as follows:
\[\sum_{k=1}^{r} p_{ik} = \sum_{k=1}^{r} P(S_{t+1} = k \mid S_t = i) = 1\]
This means the sum of probabilities of transitioning to next state is equal to 1. 

Using the transition matrix, we can sample a sequence of states based on the transition probabilities:

- Example Sequence 1: \( 1 \rightarrow 2 \rightarrow 3 \rightarrow 3 \rightarrow 1 \)
- Example Sequence 2: \( 1 \rightarrow 1 \rightarrow 3 \rightarrow 1 \)

This type of sampling process is known as a random walk. In a random walk, the next state is chosen based on the current state and its associated transition probabilities.

---
### 1.4. Markov Decision Process
A Markov Decision Process (MDP) forms the foundation of reinforcement learning. It is an extension of a Markov process that introduces actions and rewards, enabling decision-making in stochastic environments.Reinforcement learning is based on MDP. An MDP provides a mathematical framework for modeling decision-making problems where an agent interacts with an environment to maximize a cumulative reward over time.

An MDP consists of parameters \( (S, A, P, R, \gamma) \), where:

- **\( S \)**: The set of possible states in the environment.
- **\( A \)**: The set of possible actions that the agent can take. 
- **\( P(s' \mid s, a) \)**: The transition probability function, which defines the probability of moving to state \( s' \) given that the agent takes action \( a \) in state \( s \).
- **\( R(s, a) \)**: The reward function, which defines the immediate reward received after taking action \( a \) in state \( s \).
- **\( \gamma \)**: The discount factor, a value between 0 and 1 that represents the importance of future rewards.


The action at each time stamp \(a_t \in A\) will be determined by a **policy** \(\pi (a|s)\).

Based on a policy, an agent generates a sequence of states and actions \(\tau\), called "state and action trajectory". The trajectory is expressed as \(\tau : (s_0, a_0, s_1, a_1, \ldots, s_t, a_t)\).

The goal of MDP is to maximize a cumulative reward, called "expected return". Technically, the expected return \( G_t \) represents the cumulative discounted reward starting from time step \( t \):
\[
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
\]

Here, \(R_{t+1}\) is the reward received after the transition from state \(S_t\) to state \(S_{t+1}\). \(t\) is time stamp, don't be confused with state. 

---
## 2. Background for Reinforcement Learning (RL)
### 2.1. Value Function
Again, the goal is to maximize the expected return \( G_t \). To maximize the return, we aim to find an optimal stochastic policy \(\pi(a|s)\). The value function \(V^{\pi}\) represents the expected return when starting from state \(s\) and following policy \(\pi\):

\[
V^\pi(s) \triangleq \mathbb{E}_\pi \left[ G_t \mid S_t = s \right] = \mathbb{E}_\pi \left[ \sum_{k=0}^\infty \gamma^k R_{t+k+1} \mid S_t = s \right]
\]

This value function has recursive relationship because of the nature of the return \(G_t\).
\[G_t = R_{t+1} + \gamma G_{t+1}\]

Then, we can rewrite the value function using this recursive chracteristic of the return. This recursive relationship is known as the **Bellman equation**.

\[
V^\pi(s) = \mathbb{E}_\pi \left[ R_{t+1} + \gamma G_{t+1} \mid S_t \right]
\]

We are not done yet. I want \(V^{\pi}\) to be both right and left sides of equation. We use **law of iterated expectations**:
\[
  \mathbb{E}_\pi \left[ G_{t+1} | S_t = s \right] = \mathbb{E}_\pi [\mathbb{E}_\pi [G_{t+1} | S_{t+1}] | S_t = s ]
  \]

But by the definition of the value function, we know:
\[
  V^{\pi}(s_{t+1}) = \mathbb{E}_{\pi} [G_{t+1} | S_{t+1}]
  \]

Now, replacing this in the earlier equation:

\[
V^\pi(s) = \mathbb{E}_\pi \left[ R_{t+1} + \gamma V^\pi(S_{t+1}) \mid S_t = s \right].
\]

---

### 2.2. State-Action value function (Q function)
The value function \(V^{\pi}(s)\) is missing something. It doesn't tell us which action \(a\) is best to take in that state. Therefore, we need to define a new function called "state-action value function or Q function.

\[
Q^\pi(s, a) \triangleq \mathbb{E}_\pi \left[ \sum_{k \geq 0} \gamma^k R_{t+k} \mid S_t = s, A_t = a \right] \\
= \mathbb{E}_\pi \left[ R_{t+1} + \gamma V^\pi(S_{t+1}) \mid S_t = s, A_t = a \right]
\]

The Q function \(Q^{\pi}(s,a)\) explicitly conditions on both state and action, which provides a more granular view of the agent's behavior and allows for better decision-making. Let's break the equation into two terms.


We denote the immediate reward expectation as \(r(s,a)\):
\[
  \mathbb{E}_\pi[R_{t+1} | S_t = s, A_t = a] = r(s,a).
  \]

The expected discounted value function of the next state \(S_{t+1}\) can be rewritten with the transition probability \(P(s'|s,a)\) where \(s'\) is next state of \(s\) (for simplicity \(s'\) will be used instead of \(S_{t+1}\)):

\[
  \mathbb{E}_\pi \left[ \gamma V^\pi(S_{.t+1}) \mid S_t = s, A_t = a \right] = \gamma \sum_{s'}P(s'|a,s)V^{\pi}(s')
\]

By combining these two terms:
\[
  Q^{\pi}(s,a) = \mathbb{E}_\pi \left[ R_{t+1} + \gamma V^\pi(S_{t+1}) \mid S_t = s, A_t = a \right] = r(s,a) + \gamma \sum_{s'}P(s'|a,s)V^{\pi}(s').
  \]

However, the euqation is not respect to the policy \(\pi(a|s)\) yet. Therefore, we substitute \(V^{\pi}(s')\) by writing relationship between value function \(V^{\pi}\) and Q function \(Q^\pi\).

\[V^\pi (s) = \mathbb{E}_\pi \left[ Q^\pi (s,a) \mid S_{t} = s \right]\]

We can rewrite the expected Q-value over all possible action \(A_t\).
\[V^\pi(s) = \sum_a \pi (a|s)Q^\pi (s,a)\]

Thus, we can replace \(V^{\pi}(s')\) in equation of \( Q^\pi(s, a) \) as:

\[
Q^\pi(s, a) = r(s, a) + \gamma \sum_{s'} P(s' \mid a, s) \sum_{a'} \pi(a' \mid s') Q^\pi(s', a')
\]

<!-- Now, we have a new goal - find an optimal policy by choosing the action that maximizes \(Q^*(s,a)\) for a given state \(s\):
\[
\pi^*(s) = \arg\max_{a} Q^*(s, a)
\]
We are switching from \(Q^\pi\) (value of given policy) to \(Q^*\) (value of the optimal policy), assuming we derive a policy from a value function.

This is known as the **greedy policy** with respect to \( Q^*(s, a) \). -->

---
### 2.3. Bellman Optimality Equation for \( Q^*(s, a) \)

The optimal Q-function, denoted as \( Q^*(s, a) \), follows a recursive relationship similar to the Bellman equation for \( V^*(s) \). The optimal Q-function satisfies:

\[
Q^*(s, a) = r(s, a) + \gamma \sum_{s'} P(s' \mid s, a) \max_{a'} Q^*(s', a')
\]

This equation states that the optimal Q-value for state-action pair \( (s, a) \) is the immediate reward plus the discounted expected future rewards assuming that the agent always follows the best possible action thereafter.

- Instead of averaging over actions as in the policy evaluation step, we now **maximize** over the next possible actions.
- This is a key component in **value iteration**, where the agent repeatedly updates \( Q^*(s, a) \) until convergence.
---
## 3. How We Classify RL Methods
Before we jump into specific RL algorithms, we better know that there are a few categories we can label those algorithms.

### 3.1. Model-based vs. Model-free methods
Here the **model** does not mean a statistical or machine learning model. It means a representation of the environment’s dynamics. Technically, a model refers two parts: transition function \(P(s'|s,a)\) and reward function \(R(s,a)\).
- Model-Based methods assume the agent can learn the environment’s dynamics 
  - i.e., estimate value \(V(s)\) or \(Q(s,a)\) based on envrionment \(P(s'|s,a)\) and \(R(s,a)\).
  - In autonomous driving, the car is equipped with a high-definition 3D map, traffic rules, and a physics simulator. The system can plan an entire route in advance without physically driving it first.
- Model-Free methods skip building the environment model and learn directly from experience 
  - i.e., estimate \(V(s)\) or \(Q(s,a)\) directly from samples without knowing \(P(s'|s,a)\) and \(R(s,a)\).
  - In autonomous driving, They don’t have a map, no traffic rules book, no physics simulator. Just the ability to try actions (steering, braking, accelerating) and see what happens.

### 3.2. Value-based vs. Policy-based methods vs. Actor-critic
- Value-Based methods focus on learning a value function (\(V(s)\) or \(Q(s,a)\)).and derive a policy from it
- Policy-based method skip value functions and directly learn the policy \(\pi (a|s)\).
- Actor–Critic methods are hybrids where the agent learn both  a policy (actor) and a value function (critic).

### 3.3. On-Policy vs. Off-Policy
- On-Policy methods learn from the actions actually taken by the current policy.
- Off-Policy methods learn the value of a different policy than the one used to collect data.

### 3.4. Existing RL algorithms categorization

| **Algorithm**                                               | **Model-Based / Model-Free**    | **Value-Based / Policy-Based / Actor–Critic** | **On-Policy / Off-Policy**                 |
| ----------------------------------------------------------- | ------------------------------- | --------------------------------------------- | ------------------------------------------ |
| **Dynamic Programming method** | Model-Based                     | Value-Based                                   | On-Policy                                  |
| **Monte Carlo method**                                     | Model-Free                      | Value-Based                                   | On-Policy                                  |
| **SARSA**                                                   | Model-Free                      | Value-Based                                   | On-Policy                                  |
| **Q-Learning**                                              | Model-Free                      | Value-Based                                   | Off-Policy                                 |
| **A2C (Advantage Actor–Critic)**                            | Model-Free                      | Actor–Critic                                  | On-Policy                                  |
| **PPO (Proximal Policy Optimization)**                      | Model-Free                      | Actor–Critic                                  | On-Policy                                  |
---

## 4. Q-learning
The goal of Q-learning is to learn the optimal action-value function \(Q^*(s,a)\) that maximizes the agent's expected cumulative discounted reward:
\[
\max_{\pi} \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^{t} R_{t+1} \right] = \max_{\pi}Q^\pi(s,a) = Q^*(s,a).
\]

If we know \(Q^* (s,a)\) from learning process, the optimal value for each state-action pair, we can derive the optimal policy \(\pi\):
\[
  \pi^*(s) = \arg\max_{a} Q^*(s, a).
\]
This is known as the **greedy policy** with respect to \( Q^*(s, a) \). This is why Q-learning is called value-based method. Value-based methods learn a value function \(Q(s,q)\) and derive a policy \(\pi^*(s)\).

Now, let's get back to the Bellman Optimality Equation.
\[
Q^*(s, a) = r(s, a) + \gamma \sum_{s'} P(s' \mid s, a) \max_{a'} Q^*(s', a')
\]

Q-learning is a **model-free** method so we don’t know \(P\). We can replace the expectation with a sample. If at time \(t\) we take action \(a_t\) in state \(s_t\), observe reward \(r_{t+1}\), and next state \(s_{t+1}\), then:

\[
\text{Target} = r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a')
\]

This is called the **TD (temporal difference) target** --- it's a single Monte Carlo sample of the expectation in the Bellman equation.

We want \(Q(s_t, a_t)\) to move toward the target. The general incremental update form is:

\[
  Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [\text{Target} - Q(s_t, a_t)]
\]

where \(\alpha [\text{Target} - Q(s_t, a_t)]\) is called Bellman error. By substituting the TD target:

\[
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \right]
\]

### 4.1. Q-learning algorithm flow
1. **Initialize**
   - For all states \(s\) and actions \(a\), set \(Q(s,a)\) (e.g., to 0).
   - Choose:
     - Learning rate \(\alpha \in (0,1]\)
     - Discount factor \(\gamma \in [0,1)\)
     - Exploration rate \(\varepsilon \in [0,1]\)

2. **For each episode** (repeat until convergence or max episodes):
   - Reset environment; get initial state \(s\).
   - **Loop** (until \(s\) is terminal):
      - **Action selection (behavior policy):**
        - With probability \(\varepsilon\), choose a random action.
        - Otherwise, choose \(a = \arg\max_{a'} Q(s,a')\). *(ε-greedy)*
      - **Act & observe:** execute \(a\); observe reward \(r\) and next state \(s'\).
      - **Target (off-policy, greedy):**
        \[
        \text{Target} = r + \gamma \max_{a'} Q(s', a')
        \]
      - **Update (TD step):**
        \[
        Q(s,a) \leftarrow Q(s,a) + \alpha \left[ \text{Target} - Q(s,a) \right]
        \]
      - **Advance:** set \(s \leftarrow s'\).

3. **Policy extraction** (can be done anytime):
   - Greedy policy:
     \[
     \pi(s) = \arg\max_{a} Q(s,a)
     \]

## Ref.
- https://www.probabilitycourse.com
- https://www.cs.toronto.edu/~rahulgk/courses/csc311_f23/lectures/lec12.pdf
- https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9904958
<!-- https://chatgpt.com/c/689bfade-c1a8-8331-8e3c-ce4dea6452ff -->
<!-- https://medium.com/@hsinhungw/intro-to-reinforcement-learning-monte-carlo-to-policy-gradient-1c7ede4eed6e -->
<!-- https://ieeexplore.ieee.org/abstract/document/9904958?casa_token=oqhWk1IHZCwAAAAA:hdPBdobLXwh3dq1Vonf3r6GEXIW2MwZTXBuio7ifUEJqViE62p2tGLLrD00TZKWtOnAFflTlsno -->