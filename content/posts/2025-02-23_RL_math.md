---
title: 'Mathmatical Foundation for Reinforcement Learning'
date: '2025-02-23T15:04:51-05:00'
draft: False
params:
  math: true
tags: [Reinforcement Learning]
---

## Markov Property
The Markov Property is a fundamental concept in probability theory that states that the future state of a process depends only on its current state and not on the sequence of events that preceded it.

---
### Random Process
A random process (also known as a stochastic process) is a collection of random variables indexed by time. 
\[\{X_t, t \in [0, \infty)\}\] 

Itâ€™s often used to model real-world data that changes unpredictably. One common example of a real-world random process is stock prices. At any given moment \(t\), the price of a stock will vary due to these unpredictable influences, making it a prime example of a stochastic process.

---
### Markov Process
A Markov process (markov chain) is a special type of random process that satisfies the Markov property. The Markov property states that the future state of the process depends only on the current state and not on any previous states. Formally:
\[
P(S_{t+1} = s' \mid S_t = s, S_{t-1} = s_{t-1}, \ldots, S_0 = s_0) = P(S_{t+1} = s' \mid S_t = s)
\]

Where
- \( S_t \) represents the state of at time \( t \).
- \(P(S_{t+1} = s' \mid S_t = s)\) denotes state probability.

Let's think how Markov came up with this modeling. We can assume that the stock price \(S_t+1\) might depend on previous stock price \(S_{t}, S_{t-1}, \cdots, S_{0}\). But Markov said no! The stock price at the next time step \(X_{t+1}\)only depends on the current price \(S_t\) and not on the entire history of previous prices. Markov believed that in many real-world processes, including finance, weather prediction, and other systems, the most recent information captures all the relevant data needed to predict future behavior. This assumption simplifies modeling because we don't need to consider complex historical dependencies.

---
### State Transition Matrix
![state_transition_diagram](/images/2025-02-23_RL_math/state_transition_diagram.png)

The above figure is an example of state transition diagram used to visualize markov chain problem. The number between states is a transition probability \(P(S_{t+1} = s' \mid S_t = s)\). Let's say we start from state 1 (i.e. \(t=0\) and \(s=1\)). The transition probability \(P(S_{1} = 2 \mid S_{0} = 1)\) is 1/3. The example diagram can be represented as a state transition matrix:

\[
P = \begin{bmatrix}
\frac{1}{4} & \frac{1}{2} & \frac{1}{4} \\
\frac{1}{3} & 0 & \frac{2}{3} \\
\frac{1}{4} & 0 & \frac{1}{2}
\end{bmatrix}
\]

State transition matrix follows a property as follows:
\[\sum_{k=1}^{r} p_{ik} = \sum_{k=1}^{r} P(S_{t+1} = k \mid S_t = i) = 1\]
This means the sum of probabilities of transitioning to next state is equal to 1. 

Using the transition matrix, we can sample a sequence of states based on the transition probabilities:

- Example Sequence 1: \( 1 \rightarrow 2 \rightarrow 3 \rightarrow 3 \rightarrow 1 \)
- Example Sequence 2: \( 1 \rightarrow 1 \rightarrow 3 \rightarrow 1 \)

This type of sampling process is known as a random walk. In a random walk, the next state is chosen based on the current state and its associated transition probabilities.

---
### Markov Decision Process
A Markov Decision Process (MDP) forms the foundation of reinforcement learning. It is an extension of a Markov process that introduces actions and rewards, enabling decision-making in stochastic environments.Reinforcement learning is based on MDP. An MDP provides a mathematical framework for modeling decision-making problems where an agent interacts with an environment to maximize a cumulative reward over time.

An MDP consists of parameters \( (S, A, P, R, \gamma) \), where:

- **\( S \)**: The set of possible states in the environment.
- **\( A \)**: The set of possible actions that the agent can take. 
- **\( P(s' \mid s, a) \)**: The transition probability function, which defines the probability of moving to state \( s' \) given that the agent takes action \( a \) in state \( s \).
- **\( R(s, a) \)**: The reward function, which defines the immediate reward received after taking action \( a \) in state \( s \).
- **\( \gamma \)**: The discount factor, a value between 0 and 1 that represents the importance of future rewards.


The action at each time stamp \(a_t \in A\) will be determined by a **policy** \(\pi (a|s)\).

Based on a policy, an agent generates a sequence of states and actions \(\tau\), called "state and action trajectory". The trajectory is expressed as \(\tau : (s_0, a_0, s_1, a_1, \ldots, s_t, a_t)\).

The goal of MDP is to maximize a cumulative reward, called "expected return". Technically, the expected return \( G_t \) represents the cumulative discounted reward starting from time step \( t \):
\[
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
\]

Here, \(R_{t+1}\) is the reward received after the transition from state \(S_t\) to state \(S_{t+1}\). \(t\) is time stamp, don't be confused with state. 

---

### Value Function
Again, the goal is to maximize the expected return \( G_t \). To maximize the return, we aim to find an optimal stochastic policy \(\pi(a|s)\). The value function \(V^{\pi}\) represents the expected return when starting from state \(s\) and following policy \(\pi\):

\[
V^\pi(s) \triangleq \mathbb{E}_\pi \left[ G_t \mid S_t = s \right] = \mathbb{E}_\pi \left[ \sum_{k=0}^\infty \gamma^k R_{t+k+1} \mid S_t = s \right]
\]

This value function has recursive relationship because of the nature of the return \(G_t\).
\[G_t = R_{t+1} + \gamma G_{t+1}\]

Then, we can rewrite the value function using this recursive chracteristic of the return. This recursive relationship is known as the **Bellman equation**.

\[
V^\pi(s) = \mathbb{E}_\pi \left[ R_{t+1} + \gamma G_{t+1} \mid S_t \right]
\]

We are not done yet. I want \(V^{\pi}\) to be both right and left sides of equation. We use **law of iterated expectations**:
\[
  \mathbb{E}_\pi \left[ G_{t+1} | S_t = s \right] = \mathbb{E}_\pi [\mathbb{E}_\pi [G_{t+1} | S_{t+1}] | S_t = s ]
  \]

But by the definition of the value function, we know:
\[
  V^{\pi}(s_{t+1}) = \mathbb{E}_{\pi} [G_{t+1} | S_{t+1}]
  \]

Now, replacing this in the earlier equation:

\[
V^\pi(s) = \mathbb{E}_\pi \left[ R_{t+1} + \gamma V^\pi(S_{t+1}) \mid S_t = s \right].
\]

---

### State-Action value function (Q function)
The value function \(V^{\pi}(s)\) is missing something. It doesn't tell us which action \(a\) is best to take in that state. Therefore, we need to define a new function called "state-action value function or Q function.

\[
Q^\pi(s, a) \triangleq \mathbb{E}_\pi \left[ \sum_{k \geq 0} \gamma^k R_{t+k} \mid S_t = s, A_t = a \right] \\
= \mathbb{E}_\pi \left[ R_{t+1} + \gamma V^\pi(S_{t+1}) \mid S_t = s, A_t = a \right]
\]

The Q function \(Q^{\pi}(s,a)\) explicitly conditions on both state and action, which provides a more granular view of the agent's behavior and allows for better decision-making. Let's break the equation into two terms.


We denote the immediate reward expectation as \(r(s,a)\):
\[
  \mathbb{E}_\pi[R_{t+1} | S_t = s, A_t = a] = r(s,a).
  \]

The expected discounted value function of the next state \(S_{t+1}\) can be rewritten with the transition probability \(P(s'|s,a)\) where \(s'\) is next state of \(s\) (for simplicity \(s'\) will be used instead of \(S_{t+1}\)):

\[
  \mathbb{E}_\pi \left[ \gamma V^\pi(S_{.t+1}) \mid S_t = s, A_t = a \right] = \gamma \sum_{s'}P(s'|a,s)V^{\pi}(s')
\]

By combining these two terms:
\[
  Q^{\pi}(s,a) = \mathbb{E}_\pi \left[ R_{t+1} + \gamma V^\pi(S_{t+1}) \mid S_t = s, A_t = a \right] = r(s,a) + \gamma \sum_{s'}P(s'|a,s)V^{\pi}(s').
  \]

However, the euqation is not respect to the policy \(\pi(a|s)\) yet. Therefore, we substitute \(V^{\pi}(s')\) by writing relationship between value function \(V^{\pi}\) and Q function \(Q^\pi\).

\[V^\pi (s) = \mathbb{E}_\pi \left[ Q^\pi (s,a) \mid S_{t} = s \right]\]

We can rewrite the expected Q-value over all possible action \(A_t\).
\[V^\pi(s) = \sum_a \pi (a|s)Q^\pi (s,a)\]

Thus, we can replace \(V^{\pi}(s')\) in equation of \( Q^\pi(s, a) \) as:

\[
Q^\pi(s, a) = r(s, a) + \gamma \sum_{s'} P(s' \mid a, s) \sum_{a'} \pi(a' \mid s') Q^\pi(s', a')
\]

Now, we have a new goal - find an optimal policy by choosing the action that maximizes \(Q^*(s,a)\) for a given state \(s\):
\[
\pi^*(s) = \arg\max_{a} Q^*(s, a)
\]

This is known as the **greedy policy** with respect to \( Q^*(s, a) \).

---
### Bellman Optimality Equation for \( Q^*(s, a) \)

The optimal Q-function, denoted as \( Q^*(s, a) \), follows a recursive relationship similar to the Bellman equation for \( V^*(s) \). The optimal Q-function satisfies:

\[
Q^*(s, a) = r(s, a) + \gamma \sum_{s'} P(s' \mid s, a) \max_{a'} Q^*(s', a')
\]

This equation states that the optimal Q-value for state-action pair \( (s, a) \) is the immediate reward plus the discounted expected future rewards assuming that the agent always follows the best possible action thereafter.

- Instead of averaging over actions as in the policy evaluation step, we now **maximize** over the next possible actions.
- This is a key component in **value iteration**, where the agent repeatedly updates \( Q^*(s, a) \) until convergence.
---
## Next
1. **Model-Free Learning with Q-Learning:**
   - Discuss how to approximate \( Q^*(s, a) \) when the transition probabilities \( P(s' \mid s, a) \) are unknown.
   - Introduce the **Q-learning update rule**:

     \[
     Q(s, a) \leftarrow Q(s, a) + \alpha \left[ R + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
     \]

     where \( \alpha \) is the learning rate.

2. **Deep Q-Networks (DQN):**
   - Explain how function approximation is used when the state-action space is too large.
   - Introduce neural networks as a way to approximate \( Q^*(s, a) \).


## Ref.
- https://www.probabilitycourse.com
- https://www.cs.toronto.edu/~rahulgk/courses/csc311_f23/lectures/lec12.pdf
