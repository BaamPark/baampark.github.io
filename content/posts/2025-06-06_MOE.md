---
title: 'Mixture-of-Experts (MoE)'
date: '2025-06-06T22:46:19-04:00'
draft: true
params:
  math: true
tags: []
---

I first heard mixutere-of-experts when DeepSeek first came out. DeepSeek released their MoE model, DeepSeekMoE 16B, as a variant and they claimed that DeepSeekMoE achieved the state-of-the-art performance significantly reducing computational cost. I believe taht 

## History of MoE
The idea was first introduced by Jacobs et al. (1991) in the paper: "Adaptive Mixtures of Local Experts"
Gaussian mixture

- A Survey on Mixture of Experts in Large Language Models, Cai et al. (2025)

## MoE in LLM
Why LLM community focuses on MoE
The Transformer architecture has a large number of parameters, which makes it expensive to train and scale.
To address this, the authors of GShard demonstrated that we can use a Mixture of Experts (MoE) to increase model capacity without linearly increasing compute cost.
Building on this idea, models like GLaM, Mixtral, and DeepSeekMoE showed that MoE is not only efficient but also effective for autoregressive text generation, making it suitable for large language models (LLMs).

Transformer, FFN
GShard
Text Generation LLM Chronicle: GLaM -> Mixtral -> Deepseek MoE

## How we align our question to expert: Gating experts
Gating function
Mention gating function makes MoE cost effective
Check


## Load balancing