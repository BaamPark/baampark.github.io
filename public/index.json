[{"content":"\rIn my last post, we explored how Segment Anything (SAM) works in image segmentation, breaking down the key components of its model architecture. SAM achieved great success in image segmentation, demonstrating two key strengths: its foundation as a large-scale model trained on an extensive dataset and its ability to be promptable, allowing users to generate segmentations with flexible inputs. These two strengths allow SAM to deliver impressive performance in a zero-shot setting. In Jul 2024, \u0026ldquo;SAM 2: Segment Anything in Images and Videos\u0026rdquo; was published. While SAM focuses solely on image segmentation, SAM2 takes things a step further. Not only does it improve performance in image segmentation, but it also introduces the ability to handle video segmentation, thanks to its own memory system. This enhancement allows SAM2 to track objects across frames, making it a powerful tool for dynamic and real-time applications. Personally, when I look at SAM2\u0026rsquo;s zero-shot performance in tracking objects across video frames , I truly believe this is a game-changer in the world of object tracking. In this article, we are gonna mainly talk about memory bank system of SAM2 as the rest of the parts are built on top of SAM.\nSAM2 - Model Architecture If you read my previous post, you might recognize some familiar components in the SAM2 architecture diagram: the image encoder, prompt decoder, and mask decoder‚Äîall key elements carried over from SAM. So, what‚Äôs new in SAM2? The introduction of three critical components: memory attention module, memory encoder, and memory bank.\nThe memory encoder generates a memory by combining frame embedding and mask prediction across frames. The memory is sent to the memory bank, which retains the memory of past predictions for the target object. The memory attention module leverages the memory stored in the memory bank to enhance object recognition and segmentation across frames. These three components allows SAM2 to generate maskelt prediction, which means track of mask in a video. Even if you don‚Äôt provide prompts for the target object in the current video frame, SAM2 can still recognize and segment the target object based on previous prompts you provided in earlier frames. In addition, even if the target object is occluded in the past frame and reappears in the current frame, SAM2 can recover the segmentation.\nMemory Encoder No, the input to the MemoryEncoder in the provided code is spatial feature information, not explicitly spatio-temporal features.\nApplication using SAM2: SAMURAI SAMURAI is a zero-shot visual object tracking built on top of SAM2. They added motion modeling that can filter high confident mask\nFor clarity, visual object tracking differs from multi-object tracking. Visual object tracking (VOT) tracks a single object given in the first frame, while multi-object tracking (MOT) tracks multiple objects across frames while maintaining their identities.\nReference SAM 2: Segment Anything in Images and Videos ","permalink":"http://localhost:1313/posts/2025-02-06_sam2/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://about.fb.com/wp-content/uploads/2024/07/01_Dribbling_Carousel-02.gif?fit=800%2C697\" alt=\"cover\"  /\u003e\r\n\u003c/p\u003e\n\u003cp\u003eIn my \u003ca href=\"https://baampark.github.io/posts/2025-01-29_sam/\"\u003elast post\u003c/a\u003e, we explored how Segment Anything (SAM) works in image segmentation, breaking down the key components of its model architecture. SAM achieved great success in image segmentation, demonstrating two key strengths: its foundation as a large-scale model trained on an extensive dataset and its ability to be promptable, allowing users to generate segmentations with flexible inputs. These two strengths allow SAM to deliver impressive performance in a zero-shot setting. In Jul 2024, \u003ca href=\"https://arxiv.org/abs/2408.00714\"\u003e\u0026ldquo;SAM 2: Segment Anything in Images and Videos\u0026rdquo;\u003c/a\u003e was published. While SAM focuses solely on image segmentation, SAM2 takes things a step further. Not only does it improve performance in image segmentation, but it also introduces the ability to handle video segmentation, thanks to its own memory system. This enhancement allows SAM2 to track objects across frames, making it a powerful tool for dynamic and real-time applications. Personally, when I look at SAM2\u0026rsquo;s zero-shot performance in tracking objects across video frames , I truly believe this is a game-changer in the world of object tracking. In this article, we are gonna mainly talk about \u003cstrong\u003ememory bank\u003c/strong\u003e system of SAM2 as the rest of the parts are built on top of SAM.\u003c/p\u003e","title":"Segment Anything 2 vs. SAM1: What‚Äôs New and Why It Matters"},{"content":"\rSegment Anything (SAM) has drawn massive attention in the computer vision community, accumulating an impressive 8,000 citations. Segmentation has long been a crucial yet challenging aspect of computer vision. One of the biggest hurdles? Annotation. Unlike simple bounding boxes, which only require marking the object‚Äôs general location, segmentation demands precise pixel-level annotations‚Äîan incredibly tedious and time-consuming task for annotators. SAM is one of the first large-scale foundation models for segmentation. What makes SAM truly great is that it‚Äôs a promptable model. This means you can use it for various tasks without the need for fine-tuning‚Äîalso known as zero-shot learning. Unlike LLM, here the prompts for the SAM are points, bounding boxes, and masks. In this post, we‚Äôre going to explore the key components of SAM. This guide will break things down in a simple and easy-to-follow way. Let‚Äôs get started! üöÄ.\n1. SAM Architecture Overview The SAM (Segment Anything Model) architecture consists of three main components: an image encoder, a prompt encoder, and a mask decoder. The image encoder processes the input image to generate an embedding, while the prompt encoder takes user-provided prompts (such as points, boxes, or text) to refine the segmentation. The mask decoder then combines these embeddings and prompts to produce multiple valid segmentation masks, each with an associated confidence score.\n2. Image Encoder The image encoder of SAM (Segment Anything Model) is quite straightforward. The authors pre-trained the Vision Transformer (ViT) using Masked Autoencoder (MAE)‚Äîboth of which are widely recognized techniques in the computer vision community.\nViT is one of the pioneering large-scale foundation models for image classification. Meanwhile, MAE is well known for its effectiveness in pre-training models. The idea behind MAE is simple yet powerful: it randomly applies zero-masking to some image patches before passing them through the encoder. The decoder then attempts to reconstruct the masked patches, forcing the model to develop a deeper understanding of image structures. Essentially, the image is embedded into feature space \\(\\mathbb{R}^{256 \\times 64 \\times 64}\\).\n3. Prompt Encoder The prompt encoder takes three types inputs: points, bounding boxes, text, and mask. Points, bounding boxes, and text are treated sparse prompt. The mask is treated as dense prompt. However, the authors said that text prompts are just an exploration, so we won\u0026rsquo;t cover them in this article. The prompt encoder has two jobs mainly: sparse prompt embedding and dense prompt embedding. However, if you see the PromptEncoder implementation, you will notice there is one more thing it returns, which is image positional embedding. We will learn how these three embeddings are processed.\n3.1. Image Positional Embedding To ensure the decoder has access to critical geometric information the positional encodings are added to the image embedding whenever they participate in an attention layer. - SAM, Segment Anything Model and Task Details, Lightweight mask decoder\nThe authors said positional encodings are added to the image embedding. It encodes positional information for the entire image feature grid. The concept of positional encoding was originated from transformer. Transformers use self-attention to process inputs, but unlike RNNs or CNNs, they do not inherently capture positional information (i.e. permutation-invariant). Positional encoding is added to the input embeddings to provide a sense of order in the sequence. The image below shows how positional embedding and image embedding are added in transformer.\nWhat would be the dimension of positional encoding? It\u0026rsquo;s \\(256 \\times 64 \\times 64\\), which matches the dimension of the image embedding. This is because the image embedding and image positional embedding are added together element-wise. Let\u0026rsquo;s review a part of the PromptEncoder implementation.\nclass PromptEncoder(nn.Module): def __init__(): #skip... self.pe_layer = PositionEmbeddingRandom(embed_dim // 2) def get_dense_pe(self) -\u0026gt; torch.Tensor: return self.pe_layer(self.image_embedding_size).unsqueeze(0) class PositionEmbeddingRandom(nn.Module): def __init__(self, num_pos_feats: int = 64, scale: Optional[float] = None) -\u0026gt; None: super().__init__() if scale is None or scale \u0026lt;= 0.0: scale = 1.0 self.register_buffer( \u0026#34;positional_encoding_gaussian_matrix\u0026#34;, scale * torch.randn((2, num_pos_feats)), ) def _pe_encoding(self, coords: torch.Tensor) -\u0026gt; torch.Tensor: coords = 2 * coords - 1 coords = coords @ self.positional_encoding_gaussian_matrix coords = 2 * np.pi * coords # outputs d_1 x ... x d_n x C shape return torch.cat([torch.sin(coords), torch.cos(coords)], dim=-1) def forward(self, size: Tuple[int, int]) -\u0026gt; torch.Tensor: h, w = size device: Any = self.positional_encoding_gaussian_matrix.device grid = torch.ones((h, w), device=device, dtype=torch.float32) y_embed = grid.cumsum(dim=0) - 0.5 x_embed = grid.cumsum(dim=1) - 0.5 y_embed = y_embed / h x_embed = x_embed / w pe = self._pe_encoding(torch.stack([x_embed, y_embed], dim=-1)) return pe.permute(2, 0, 1) # C x H x W In essence, get_dense_pe function returns image positional embedding, which later will be passed to the mask decoder. In forward function, it shows how image positional embeddings are constructed for the entire 64 x 64 feature grid into four steps\nCreates a coordinate grid for the feature map. Normalizes x and y coordinates to \\([0,1]\\). Centers the ccoordinates such that \\([-0.5, 0.5]\\) Apply positional encoding When creating the grid, it first initializes 2d tensor filled with ones. For x and y axis, cumsum computes cumulative sum along each axis. For example, if h=3, w = 3, then cumsum does:\nx_embed = grid.cumsum(dim=1) - 0.5 \\[\r\\begin{bmatrix}\r1 \u0026 2 \u0026 3 \\\\\r1 \u0026 2 \u0026 3 \\\\\r1 \u0026 2 \u0026 3\r\\end{bmatrix}\r\\]y_embed = grid.cumsum(dim=0) - 0.5 \\[\r\\begin{bmatrix}\r1 \u0026 1 \u0026 1 \\\\\r2 \u0026 2 \u0026 2 \\\\\r3 \u0026 3 \u0026 3\r\\end{bmatrix}\r\\]After normalization and certering the coordinates, it performs positional encoding. Mathmatically, postional encdoing is given by:\n\\[\r\\text{PE}(x,y) = \\sin(2\\pi W \\begin{bmatrix} x \\\\ y \\end{bmatrix}) \\oplus \\cos(2\\pi W \\begin{bmatrix} x \\\\ y \\end{bmatrix})\r\\] where:\n\\(\\begin{bmatrix} x \\\\ y \\end{bmatrix} \\in \\mathbb{R}^{B \\times H \\times W \\times 2}\\) is a stacked grid feature. \\(W \\in \\mathbb{R}^{2 \\times d}\\) is the Gaussian projection matrix that maps 2D coordinates to a higher-dimensional space. \\(\\oplus\\) refers to concatenation along the feature dimension. \\(\\text{PE}(x,y) \\in \\mathbb{R}^{B \\times H \\times W \\times 2d}\\) is the final positional encoding Now that we understand how image positional embeddings are computed. These embeddings, along with sparse and dense prompts, will be passed to the mask decoder to guide segmentation.\n3.2. Point Embedding We can pass \\(N\\) number of points per image to SAM. Each point acts as a spatial cue, helping the model focus on specific regions of interest within the image. These points are then transformed into high-dimensional-sparse embeddings.\nclass PromptEncoder(nn.Module): def __init__( #some arguements... ) super().__init__() self.embed_dim = embed_dim self.input_image_size = input_image_size self.image_embedding_size = image_embedding_size self.pe_layer = PositionEmbeddingRandom(embed_dim // 2) point_embeddings = [nn.Embedding(1, embed_dim) for i in range(self.num_point_embeddings)] self.point_embeddings = nn.ModuleList(point_embeddings) def _embed_points( self, points: torch.Tensor, labels: torch.Tensor, pad: bool, ) -\u0026gt; torch.Tensor: \u0026#34;\u0026#34;\u0026#34;Embeds point prompts.\u0026#34;\u0026#34;\u0026#34; points = points + 0.5 # Shift to center of pixel if pad: padding_point = torch.zeros((points.shape[0], 1, 2), device=points.device) padding_label = -torch.ones((labels.shape[0], 1), device=labels.device) points = torch.cat([points, padding_point], dim=1) labels = torch.cat([labels, padding_label], dim=1) point_embedding = self.pe_layer.forward_with_coords(points, self.input_image_size) point_embedding[labels == -1] = 0.0 point_embedding[labels == -1] += self.not_a_point_embed.weight point_embedding[labels == 0] += self.point_embeddings[0].weight point_embedding[labels == 1] += self.point_embeddings[1].weight return point_embedding def forward( self, points: Optional[Tuple[torch.Tensor, torch.Tensor]], boxes: Optional[torch.Tensor], masks: Optional[torch.Tensor], ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: bs = self._get_batch_size(points, boxes, masks) sparse_embeddings = torch.empty((bs, 0, self.embed_dim), device=self._get_device()) #place holder if points is not None: coords, labels = points point_embeddings = self._embed_points(coords, labels, pad=(boxes is None)) sparse_embeddings = torch.cat([sparse_embeddings, point_embeddings], dim=1) Okay, let\u0026rsquo;s go into detail on how points and boxes are encoded. self.pe_layer, which is an object of PositionEmbeddingRandom, maps coordinates into a higher-dimensional space. The member function forward_with_coords performs normalization, linear projection, and sinusoidal transformation, same as we did for image positional embedding.\n\\[\r\\text{PE}(x, y) = \\sin\\left( 2\\pi \\cdot W \\cdot \\begin{bmatrix} 2x - 1 \\\\ 2y - 1 \\end{bmatrix} \\right) \\oplus \\cos\\left( 2\\pi \\cdot W \\cdot \\begin{bmatrix} 2x - 1 \\\\ 2y - 1 \\end{bmatrix} \\right)\r\\]where:\n\\(x,y \\in \\mathbb{R}^{B \\times N \\times 2}\\) is input coordinates (batch of ùëÅ points per image). \\(W \\in \\mathbb{R}^{2 \\times d}\\) \\(\\text{PE}(x,y) \\in \\mathbb{R} ^{B \\times N \\times 2d}\\) Before we analyze what happens after executing point_embedding = self.pe_layer.forward_with_coords(points, self.input_image_size) is excuted, let\u0026rsquo;s first discuss positive (foreground) points and negative (background) points. Actually, SAM has a feature that I haven\u0026rsquo;t mentioned yet‚Äîyou can provide background points. A background point is a point that you are not interested in and explicitly mark as not part of the object. See the image below, or you can test this in the demo. In the image, the blue dot represents a positive point, while the red dot represents a negative point. But wait! why do we need labels for the forward pass? In this context, labels are not ground truth segmentation masks. Instead, they indicate whether each click is a positive (foreground) or negative (background) point when passing inputs to the prompt encoder. So, the label is an array of size \\(N\\), where each entry is either 1 (positive) or 0 (negative).\nAs sparse_embeddings is an empty tensor that has zero dimension in sequnence dimension, the concatenation with point_embeddings doesn\u0026rsquo;t affect the shape of tensor.\n3.3. Box Encoding The bounding box is defined by four coordinates \\((x_1, y_1, x_2,y_2)\\), representing the top-left and bottom-right corners.\nclass PromptEncoder(nn.Module): def __init__( #some arguements... ) def _embed_points(self, points, labels, pad): #skip... return point_embedding def _embed_boxes(self, boxes: torch.Tensor) -\u0026gt; torch.Tensor: \u0026#34;\u0026#34;\u0026#34;Embeds box prompts.\u0026#34;\u0026#34;\u0026#34; boxes = boxes + 0.5 # Shift to center of pixel coords = boxes.reshape(-1, 2, 2) corner_embedding = self.pe_layer.forward_with_coords(coords, self.input_image_size) corner_embedding[:, 0, :] += self.point_embeddings[2].weight corner_embedding[:, 1, :] += self.point_embeddings[3].weight return corner_embedding def forward( self, points: Optional[Tuple[torch.Tensor, torch.Tensor]], boxes: Optional[torch.Tensor], masks: Optional[torch.Tensor], ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: bs = self._get_batch_size(points, boxes, masks) sparse_embeddings = torch.empty((bs, 0, self.embed_dim), device=self._get_device()) if points is not None: coords, labels = points point_embeddings = self._embed_points(coords, labels, pad=(boxes is None)) sparse_embeddings = torch.cat([sparse_embeddings, point_embeddings], dim=1) if boxes is not None: box_embeddings = self._embed_boxes(boxes) sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=1) Similar to points, these coordinates are mapped to positional encodings using sinusoidal transformation. However, unlike point_embedding, which consists of N points, corner_embedding represents only one bounding box per image. You can see this from the line, coords = boxes.reshape(-1, 2, 2), which reshapes the input into \\(B \\times 2\\). Here, the last two dimensions represent the (x, y) coordinates of the two corners (top-left and bottom-right) of the bounding box. After mapping the box to positional embedding, we add learnable parameters to the top-left and bottom-right corners.\ncorner_embedding[:, 0, :] += self.point_embeddings[2].weight corner_embedding[:, 1, :] += self.point_embeddings[3].weight Eventually, box_embeddings will have the dimension of \\(\\mathbb{R}^{B,2,2d}\\).\n\\[\r\\text{PE}_{\\text{sparse}} \\in \\mathbb{R}^{B \\times N+2 \\times C}.\r\\]I have a question for you. What will be the dimension of \\(\\text{PE}_{\\text{sparse}}\\) when only points prompt is given without bounding box. What about only bounding box is given? We have three input scenarios\n\\(N\\) points prompts, No bounding box ‚û°Ô∏è \\(\\mathbb{R}^{B \\times N \\times C}\\) No points prompts, one bounding box ‚û°Ô∏è \\(\\mathbb{R}^{B \\times 2 \\times C}\\) \\(N\\) points prompts, one bounding box ‚û°Ô∏è \\(\\mathbb{R}^{B \\times N + 2 \\times C}\\) Something is odd. How we can forward pass tensor that has different sequence length (middle dimension) for each pass? If you can\u0026rsquo;t answer this question, you can read my previous post. In short, there is no nn.Linear (project layer) in prompt encoder so we don\u0026rsquo;t need to care about variable-length sequnces.\n3.4. Dense prompt encoding Unlike sparse prompts, which are first mapped to an embedding space using positional encoding, dense prompts are directly projected using convolutions and then summed element-wise with the image embedding.The input mask is a binary tensor \\( M \\) of shape:\n\\[\rM \\in \\mathbb{R}^{B \\times 1 \\times 256 \\times 256}\r\\] where:\n\\( B \\) is the batch size. The single channel (1) represents a binary mask (foreground vs. background). \\( 256 \\times 256 \\) is a fixed spatial resolution for masks in SAM. In the Prompt Encoder, the mask undergoes convolutional transformations to extract meaningful features. This is done in the _embed_masks function:\nclass PromptEncoder(nn.Module): def __init__( self, embed_dim: int, image_embedding_size: Tuple[int, int], input_image_size: Tuple[int, int], mask_in_chans: int, activation: Type[nn.Module] = nn.GELU, ) self.mask_downscaling = nn.Sequential( nn.Conv2d(1, mask_in_chans // 4, kernel_size=2, stride=2), LayerNorm2d(mask_in_chans // 4), activation(), nn.Conv2d(mask_in_chans // 4, mask_in_chans, kernel_size=2, stride=2), LayerNorm2d(mask_in_chans), activation(), nn.Conv2d(mask_in_chans, embed_dim, kernel_size=1), ) def _embed_masks(self, masks: torch.Tensor) -\u0026gt; torch.Tensor: \u0026#34;\u0026#34;\u0026#34;Embeds mask input.\u0026#34;\u0026#34;\u0026#34; mask_embedding = self.mask_downscaling(masks) return mask_embedding def forward( self, points: Optional[Tuple[torch.Tensor, torch.Tensor]], boxes: Optional[torch.Tensor], masks: Optional[torch.Tensor], ) #skip sparse prompt if masks is not None: dense_embeddings = self._embed_masks(masks) else: dense_embeddings = self.no_mask_embed.weight.reshape(1, -1, 1, 1).expand( bs, -1, self.image_embedding_size[0], self.image_embedding_size[1] ) return sparse_embeddings, dense_embeddings mask_downscaling is a learnable CNN module that reduces the resolution of the mask while increasing its feature depth. This converts the binary mask into an embedding space that aligns with the image features. The resulting mask embedding has the shape: \\(\\mathbb{R}^{B \\times C \\times H' \\times W'}\\), where \\(C=256, H'=64, W'=64\\). Now the mask embedding dimension matches the image embedding so that both can be used together in the mask decoder.\nHowever, if no mask is given (masks=None), SAM instead uses a learnable \u0026ldquo;no-mask\u0026rdquo; embedding. self.no_mask_embed.weight is a learnable tensor representing a default mask embedding when a mask is not given. It is reshaped and expanded to match the required shape, \\(\\mathbb{R}^{B \\times C \\times H' \\times W'}\\). This ensures that even when no mask is provided, the model still has a valid dense prompt embedding.\n4. Mask Decoder So far, we have got four embeddings before we pass them to mask decoder:\nimage embedding image positional embedding sparse prompt embedding dense prompt embedding The mask decoder returns two objects: a mask and an IoU confidence score. Before we go deeper, let me ask how familiar you are with the transformer decoder. Before I studid this paper, I was not familiar with the transformer decoder, as I mostly worked with ViT or Swin Transformer, which only use the encoder of a transformer. Let me give you a quick recap about transformer decoder. A Transformer decoder takes \u0026ldquo;output embedding\u0026rdquo; as input. The output embedding representation is refined through attention mechanism. In the next training step, the highest logit token is mapped back to an output embedding. In the decoder‚Äôs attention stage, the model attends to the encoder‚Äôs output. This process is called cross-attention.\nNow that we‚Äôve covered the basics of the Transformer decoder, let‚Äôs dive into SAM‚Äôs mask decoder. Unlike text generation models, where the decoder outputs a sequence of tokens, SAM\u0026rsquo;s mask decoder is designed to predict segmentation masks based on mask tokens. SAM‚Äôs mask decoder follows a similar structure to a Transformer decoder but is tailored for image segmentation. The key difference is that instead of processing text tokens, the decoder refines mask tokens to generate segmentation masks.\n4.1. Input Processing for Mask Decoder The decoder starts with a set of learnable mask tokens and an IoU token. These tokens act as placeholders, similar to how DETR initializes object queries for object detection.\nclass MaskDecoder(nn.Module): def __init__(): #skip parameters #skip self.iou_token = nn.Embedding(1, transformer_dim) self.num_mask_tokens = num_multimask_outputs + 1 self.mask_tokens = nn.Embedding(self.num_mask_tokens, transformer_dim) The IoU token has dimension of \\(\\mathbb{R}^{1 \\times 256}\\) and mask token has dimension of \\(\\mathbb{R}^{4 \\times 256}\\).\nYou might wonder why the sequence dimension of mask token is four. SAM produces three masks by default considering a single input prompt may be ambiguous. This means even if you provides single point as prompt, SAM will give you three masks. Then why four not three? The default mask token is added to the three tokens. This token is used when an user doesn\u0026rsquo;t want multi-mask option.\nmasks, _, _ = predictor.predict( point_coords=input_point, point_labels=input_label, multimask_output=False, ) This ensures that SAM always has a fallback \u0026ldquo;default mask\u0026rdquo; in addition to the three multimask outputs. The first token is used when multimask_output is off. The three tokens are used when multimask_output is on.\nclass MaskDecoder(nn.Module): def forward(): #skip parameters if multimask_output: mask_slice = slice(1, None) # Selects the three multimask outputs else: mask_slice = slice(0, 1) # Selects only the first mask (default) masks = masks[:, mask_slice, :, :] The IoU tokens and mask tokens are concatenated with sparse prompt embeddings before passing them through the Transformer.\nclass MaskDecoder(nn.Module): def forward(): #skip parameters masks, iou_pred = self.predict_masks( image_embeddings=image_embeddings, image_pe=image_pe, sparse_prompt_embeddings=sparse_prompt_embeddings, dense_prompt_embeddings=dense_prompt_embeddings, ) def predict_masks( self, image_embeddings: torch.Tensor, image_pe: torch.Tensor, sparse_prompt_embeddings: torch.Tensor, dense_prompt_embeddings: torch.Tensor, ): output_tokens = torch.cat([self.iou_token.weight, self.mask_tokens.weight], dim=0) output_tokens = output_tokens.unsqueeze(0).expand(sparse_prompt_embeddings.size(0), -1, -1) tokens = torch.cat((output_tokens, sparse_prompt_embeddings), dim=1) tokens = torch.cat((output_tokens, sparse_prompt_embeddings), dim=1) src = torch.repeat_interleave(image_embeddings, tokens.shape[0], dim=0) src = src + dense_prompt_embeddings pos_src = torch.repeat_interleave(image_pe, tokens.shape[0], dim=0) # Run the transformer hs, src = self.transformer(src, pos_src, tokens) tokens tensor has shape of \\(\\mathbb{R}^{B \\times (N + 5) \\times 256}\\) where \\(N\\) is the number of sparse prompts. src = torch.repeat_interleave(image_embeddings, tokens.shape[0], dim=0) expands image embedding in batch dimension from \\(\\mathbb{R}^{B \\times 256 \\times 64 \\times 64}\\) to \\(\\mathbb{R}^{B' \\times 256 \\times 64 \\times 64}\\) if tokens batch size and image_embeddings batch size are different. But I am still not sure why they wrote this line. I assume these two batch sizes are always the same. Lastly, it adds image_embeddings to dense_prompt_embeddings. Now, we are done for input processing before passing to the decoder. self.transformer takes three inputs:\nsrc: image embedding + dense prompt pos_src: image positional embedding tokens: mask token \\(\\oplus\\) IoU token \\(\\oplus\\)sparse prompt embeddings 4.2. TwoWayAttention Transformer SAM‚Äôs mask decoder utilizes a TwoWayTransformer, which differs from a standard transformer decoder by incorporating two cross-attention stages: (1) tokens attending to image features and (2) image features attending to tokens. This bidirectional attention mechanism allows the model to effectively refine mask predictions by leveraging both sparse and dense prompts.\nThe TwoWayTransformer consists of multiple layers (depth) of TwoWayAttentionBlock modules, followed by a final attention layer for mask prediction. Each TwoWayAttentionBlock contains the following components:\nSelf-Attention (Sparse Queries Only): The mask and IoU tokens attend to themselves, refining their representation.\nCross-Attention (Tokens ‚Üí Image Features): Mask tokens interact with the image embeddings to gather spatial context.\nMLP Feedforward Block: A standard feedforward network refines the transformed token features.\nCross-Attention (Image Features ‚Üí Tokens): The image embeddings attend back to the mask tokens, allowing a richer feature exchange.\nForward Pass\nThe TwoWayTransformer takes three main inputs:\nimage_embedding (B, 256, 64, 64): Image features from the encoder.\nimage_pe (B, 256, 64, 64): Positional encodings for image features.\npoint_embedding (B, N+5, 256): Encoded sparse prompts.\nThe image embedding is first flattened from (B, 256, H, W) ‚Üí (B, HW, 256) so that it can interact with the mask tokens.\nbs, c, h, w = image_embedding.shape image_embedding = image_embedding.flatten(2).permute(0, 2, 1) image_pe = image_pe.flatten(2).permute(0, 2, 1) Next, the query tokens (mask tokens + IoU token) interact with the image features via two stacked TwoWayAttentionBlock layers:\nqueries = point_embedding keys = image_embedding for layer in self.layers: queries, keys = layer( queries=queries, keys=keys, query_pe=point_embedding, key_pe=image_pe, ) We are passing image embedding and image positional embedding for key and key_pe. But query_pe is just copy of query_pe. Why are passing the two same values? Well, we don\u0026rsquo;t have a separate postional encoding for point_embedding, which is concatenation of IoU tokens, mask tokens, and sparse prompt embeddings. However, the sparse prompt embedding was computed using positional encoding. Even if we are passing the point_embedding itself as positional encdoing, it has chance to learn positional information through attention mechanism.\nInstead, the embeddings themselves serve both as features and positional encodings, hence query_pe = point_embedding.\nLet\u0026rsquo;s break down the two way attention block. The below diagram is a visualizaation of the two way attention block.\nSelf-Attention (Tokens)\nIf it\u0026rsquo;s the first layer, positional encoding is skipped. Otherwise, the positional encoding (query_pe) is added before passing through the self-attention layer. Cross-Attention (Tokens ‚Üí Image Embeddings)\nTokens (queries) attend to image embeddings (keys). This allows the sparse prompts (mask tokens, IoU tokens) to interact with image features. MLP Block\nThe sparse queries are passed through an MLP block for further refinement. Cross-Attention (Image Embeddings ‚Üí Tokens)\nNow, the image features (keys) attend back to the sparse queries (queries). This lets the image embeddings influence the sparse tokens. After the two layers, a final cross-attention layer is applied where queries and keys interact again:\nq = queries + point_embedding k = keys + image_pe attn_out = self.final_attn_token_to_image(q=q, k=k, v=keys) queries = queries + attn_out queries = self.norm_final_attn(queries) return queries, keys In the end, two way transformer returns tokens (IoU tokens, mask tokens, and sparse embedding) and image embedding. I recommends to check the implementation source code.\n4.3. Final Output class MaskDecoder(nn.Module): def init(): #skip self.output_upscaling = nn.Sequential( nn.ConvTranspose2d(transformer_dim, transformer_dim // 4, kernel_size=2, stride=2), LayerNorm2d(transformer_dim // 4), activation(), nn.ConvTranspose2d(transformer_dim // 4, transformer_dim // 8, kernel_size=2, stride=2), activation(), ) self.output_hypernetworks_mlps = nn.ModuleList( [ MLP(transformer_dim, transformer_dim, transformer_dim // 8, 3) for i in range(self.num_mask_tokens) ] ) self.iou_prediction_head = MLP( transformer_dim, iou_head_hidden_dim, self.num_mask_tokens, iou_head_depth ) def predict_masks(): #skip... hs, src = self.transformer(src, pos_src, tokens) iou_token_out = hs[:, 0, :] mask_tokens_out = hs[:, 1 : (1 + self.num_mask_tokens), :] # Upscale mask embeddings and predict masks using the mask tokens src = src.transpose(1, 2).view(b, c, h, w) upscaled_embedding = self.output_upscaling(src) hyper_in_list: List[torch.Tensor] = [] for i in range(self.num_mask_tokens): hyper_in_list.append(self.output_hypernetworks_mlps[i](mask_tokens_out[:, i, :])) hyper_in = torch.stack(hyper_in_list, dim=1) b, c, h, w = upscaled_embedding.shape masks = (hyper_in @ upscaled_embedding.view(b, c, h * w)).view(b, -1, h, w) iou_pred = self.iou_prediction_head(iou_token_out) return masks, iou_pred After transformer processed the image embedding and tokens, we extracts IoU token and mask token from tokens. The mask_tokens_out dimension is \\(\\mathbb{R}^{B \\times 4 \\times 256}\\). We want masks to be 4-dimensional shape, \\(\\mathbb{R}^{B \\times 4 \\times H \\times W}\\). The mask tokens are transformed into mask predictions via hypernetworks, and the upscaled image features are used for final mask refinement.\nsrc represents the transformed image embeddings after passing through the transformer. We reshape src dimension form (B, HW, C) to (B, C, H, W). self.output_upscaling(src) applies an upscaling operation using two transposed convolution layers.\nmask_tokens_out is of shape (B, 4, 256). Each mask token, mask_tokens_out[:, i, :], is passed through a hypernetwork MLP. self.output_hypernetworks_mlps[i] is an MLP that processes each mask token separately. The matrix multiplication of hyper_in by upscaled_embedding, followed by reshaping, results in masks shaped (B, 4, H, W). Another MLP maps it to the final IoU prediction scores, indicating the confidence of each mask.\nDiscusssion After reading the entire paper and exploring other references, I found myself wondering‚Äîwhy is SAM receiving so much praise? Given its high citation count and widespread adoption in both industry and academia, it‚Äôs clear that SAM is considered a game-changer. But why? Interactive segmentation and transformer-based architectures aren‚Äôt new concepts. Researchers have been exploring these areas for years. So, what makes SAM stand out?\nThe key lies in its large-scale dataset and model training. The team behind SAM didn‚Äôt just build another segmentation model; they demonstrated that scaling up both the dataset and the model itself leads to remarkable performance gains. This aligns with the proven scaling laws in deep learning, where larger models trained on massive datasets tend to generalize better and unlock new capabilities. SAM isn‚Äôt just an incremental improvement‚Äîit‚Äôs a demonstration of how foundation models in computer vision can follow the same trajectory as large language models, fundamentally shifting how we approach image segmentation.\nConclusion The Segment Anything Model (SAM) represents a significant advancement in the field of computer vision, particularly in image segmentation. By leveraging a promptable architecture, SAM eliminates the need for task-specific fine-tuning, enabling zero-shot learning across various segmentation tasks. Its three core components‚Äîthe image encoder, prompt encoder, and mask decoder‚Äîwork in harmony to generate precise segmentation masks based on user-provided prompts such as points, boxes, or masks. SAM\u0026rsquo;s ability to handle sparse and dense prompts, combined with its efficient use of positional embeddings and transformer-based decoding, makes it a versatile and powerful tool for segmentation.\nReference Segment Anything A Comprehensive Survey on Segment Anything Model for Vision and Beyond Explaining the Segment Anything Model - Network architecture, Dataset, Training Medical image segmentation using deep learning: A survey How Does the Segment-Anything Model‚Äôs (SAM‚Äôs) Encoder Work? Transformer self-attention padding and causal masking technique ","permalink":"http://localhost:1313/posts/2025-01-29_sam/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/2025-01-29_SAM/intro.png\" alt=\"SAM\"  /\u003e\r\n\n\u003ca href=\"https://openaccess.thecvf.com/content/ICCV2023/html/Kirillov_Segment_Anything_ICCV_2023_paper.html\"\u003eSegment Anything (SAM)\u003c/a\u003e has drawn massive attention in the computer vision community, accumulating an impressive 8,000 citations. Segmentation has long been a crucial yet challenging aspect of computer vision. One of the biggest hurdles? Annotation. Unlike simple bounding boxes, which only require marking the object‚Äôs general location, segmentation demands precise pixel-level annotations‚Äîan incredibly tedious and time-consuming task for annotators. SAM is one of the first large-scale foundation models for segmentation. What makes SAM truly great is that it‚Äôs a promptable model. This means you can use it for various tasks without the need for fine-tuning‚Äîalso known as zero-shot learning. Unlike LLM, here the prompts for the SAM are points, bounding boxes, and masks. In this post, we‚Äôre going to explore the key components of SAM. This guide will break things down in a simple and easy-to-follow way. Let‚Äôs get started! üöÄ.\u003c/p\u003e","title":"Segment Anything, the first large-scale foundation model for segmentation"},{"content":"\r\u0026ldquo;Transformer models don\u0026rsquo;t require a fixed sequence length.\u0026rdquo; Since most of my projects revolve around computer vision, this was very confusing to me. In computer vision models, images are always preprocessed to a fixed size before being fed into deep learning models. Otherwise, you will encounter matrix multiplication error. In this post, we will learn how transofrmer handles variable-length sequnces.\nSelf-attention - Q, K, V Linear Projection into Embedding Space Let\u0026rsquo;s see basic CNN code example.\nclass SimpleCNN(nn.Module): def __init__(self, input_channels=3, num_classes=10): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=16, kernel_size=3, padding=1) # (B, 16, H, W) self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1) # (B, 32, H/2, W/2) self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1) # (B, 64, H/4, W/4) self.pool = nn.MaxPool2d(kernel_size=2, stride=2) # Reduces spatial size by half self.fc1 = nn.Linear(64 * 4 * 4, 128) # Assuming input images are 32x32 self.fc2 = nn.Linear(128, num_classes) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = self.pool(F.relu(self.conv3(x))) B, C, H, W = x.shape x = x.view(B, C * H * W) x = F.relu(self.fc1(x)) x = self.fc2(x) # Output logits return x B, C, H, W = 32, 3, 32, 32 # Batch of 32 RGB images (32x32 pixels) num_classes = 10 # e.g., CIFAR-10 dataset model = SimpleCNN(input_channels=C, num_classes=num_classes) The line x = x.view(B, C * H * W) flattens height, and width dimension. If you pass input tensor torch.randn(B, C, 52, 33), you will see an error because self.fc1 layer is a matrix \\(W \\in \\mathbb{R}^{128, 1024}\\), which requires a specific feature dimension.\nIn natual langauge processing (NLP) mdoels, the input shape is \\(B, N, C\\) where \\(N\\) can be arbitrary. This type of input is called a variable-length sequence, which is more common in NLP. The model cannot handle variable-length input if the first dimension of nn.Linear weight matrix is \\(N \\times C\\). Let\u0026rsquo;s see how transofrmer handle variable-length sequences during the self-attention.\nclass SelfAttention(nn.Module): def __init__(self, embed_dim: int): # size is hidden size super(SelfAttention, self).__init__() self.query = nn.Linear(embed_dim, embed_dim) self.key = nn.Linear(embed_dim, embed_dim) self.value = nn.Linear(embed_dim, embed_dim) def forward(self, input_tensor: torch.Tensor): q, k, v = self.query(input_tensor), self.key(input_tensor), self.value(input_tensor) scale = q.size(1) ** 0.5 scores = torch.bmm(q, k.transpose(1, 2)) / scale scores = F.softmax(scores, dim=-1) output = torch.bmm(scores, v) return output The weight matrices \\(W_Q, W_K, W_V \\in \\mathbb{R}^{C, C}\\) mean that nn.Linear does not expect the feature tensor to be flattened. Since the linear projection layer\u0026rsquo;s Q, K, and V matrix dimensions depend on the feature embedding dimensionn \\(C\\), there will be no multiplication error. The linear projection of Q, K, and V preserves the sequence length, allowing the model to handle variable-length inputs.\nSelf-attention - Padding and Masking What about we include batch? Let\u0026rsquo;s say a batch sized four and forward pass to transformer.\n\u0026ldquo;This is a short sentence\u0026rdquo; \\(N=7\\) \u0026ldquo;This one is much longer and contains more words\u0026rdquo; \\(N=8\\) \u0026ldquo;Tiny\u0026rdquo; \\(N=3\\) \u0026ldquo;More words, more sequnces\u0026rdquo; \\(N=6\\) The sequence lengths vary within the batch. You can\u0026rsquo;t feed this batch to the model due to inconsistent sequence dimension. Transformers require input of shape \\(\\mathbb{R}^{B \\times T_{\\text{max}} \\times C}\\) where \\(T_{\\text{max}}\\) is the length of the longest sequence in the batch. We can simply address inconsistency by using padding. In our example, the longest sequence length within the batch is 8. We can add paddings to the shorter sequences so that all sequences have a uniform length of 8.\nHowever, padding introduces irrelevant tokens that should not contribute to the model\u0026rsquo;s computations. To handle this, transformers use attention masks, which indicate which tokens are real and which are padding. Let\u0026rsquo;s see how self-attention is performed from the below image.\nIn the given image, we see how padding masks are applied to ensure that padded tokens do not interfere with the self-attention mechanism. Since attention scores are computed using a dot product of queries and keys, padding tokens would otherwise contribute to the output and affect model performance. By adding a mask filled with negative infinity (-‚àû) for padding positions, the softmax function effectively zeroes out their influence. This ensures that only meaningful tokens participate in the attention computation while maintaining a uniform sequence length across the batch.\nConclussion Deep learning models don‚Äôt require strict input dimensions, but they do need careful design to handle variable-sized inputs effectively. By strategically using padding and attention masks, transformers can process sequences of different lengths without introducing errors in matrix operations. We learned how padding ensures uniform input sizes across a batch and how attention masks prevent padded tokens from affecting self-attention computations. Understanding these techniques is essential for efficiently training and deploying transformer-based models in NLP and beyond.\n","permalink":"http://localhost:1313/posts/2025-01-28_variable_sequence/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/2025-01-28_variable_sequence/cover.png\" alt=\"cover\"  /\u003e\r\n\n\u0026ldquo;Transformer models don\u0026rsquo;t require a fixed sequence length.\u0026rdquo; Since most of my projects revolve around computer vision, this was very confusing to me. In computer vision models, images are always preprocessed to a fixed size before being fed into deep learning models. Otherwise, you will encounter matrix multiplication error. In this post, we will learn how transofrmer handles variable-length sequnces.\u003c/p\u003e\n\u003ch2 id=\"self-attention---q-k-v-linear-projection-into-embedding-space\"\u003eSelf-attention - Q, K, V Linear Projection into Embedding Space\u003c/h2\u003e\n\u003cp\u003eLet\u0026rsquo;s see basic CNN code example.\u003c/p\u003e","title":"How Transformers Handle Variable-length Sequnces"},{"content":"\rGraph structures have been applied in many scientific fields, such as biology, computer science, and social network analysis. With the increasing popularity of machine learning, the graph representation learning (GRL) paradigm has emerged as effective methods. One example is the Graph Convolutional Network (GCN), which has shown remarkable success in tasks like node classification, graph generation and clustering by effectively capturing the complex relationships in graph data. GRL is also making big waves in modern computer vision.\nYou might wonder how GRL can be used in modern computer vision. The potential of graphs isn\u0026rsquo;t just about finding paths from point A to point B. For instance, you can restructure an image into a graph, transforming pixels into nodes and their relationships into edges. That\u0026rsquo;s not it. You can even restructure a complex label for an image into a graph and use it for graph learning. In this article, we will talk about how graph representation learning can be used in modern computer vision. We will also cover a pratice of graph representation learning using Pytorch.\nGraph Theory and Terminology Mathematically, a graph is a pair \\(G = (V ,E)\\) where \\(V\\) is a set of vertices and \\(E\\) is a set of ordered pairs of vertices called edges. In a weighted graph, graph can be represented \\(G = (V ,E, W)\\) where \\(W\\) is a adjecency matrix, \\(W \\in \\mathbb{R}^{n \\times n}\\). \\(W_{ij}=0\\), if there is no edge between vertices \\(i\\) and \\(j\\). In some graph theory books, \\(W_{ij} = \\infty \\) when there is no edge between vertices \\(i\\) and \\(j\\). However, in this article, we will stick to the former definition.\nAdjacency Matrix There are many representations for graph strucuters such as Adjacency Matrix, Adjacency Matrix, and Edge List, and Compressed Sparse Row. In this article, we only cover Adjacency Matrix. See an example in the below image to understand how weighted directed graph can be represented in Adjacency Matrix. Image to Graph We learned mathemtical background of graph theory. But still you might not clear how graph can be applied to computer vision i.e., image to graph. Let\u0026rsquo;s recall what a neural network does. Simply put, a neural network can be viewed as an encoder that maps data to low dimensional representation for further tasks. So the encoder will function if the data is represented in a vector space. The question is how we represent image to graph? Commonly, there are two types of graph representation.\nPixel graph representation Semantic graph representation Pixel graph representation is very intuitive. Pixel graph representation converts an image into a graph structure, where pixels or groups of pixels are treated as nodes, and edges represent relationships between them. Superpixeling is often used to reduce the redundant pixel-level data (node) as an image compression. Semantic graph representation can be referred to as an object-based graph or label graph. The objects in an image generally have some semantic relationships between them (unless it\u0026rsquo;s a random white noise image). The goal of semantic graph representation is to structure and model the semantic relationships between objects, capturing contextual and relational information in a structured manner.\nGraph Convolutional Network Graph convolutional neural networks https://mbernste.github.io/posts/gcn/\nConvolution works well on images as it aggregate neighboring features. In the same way, the graph convolution aggregate information from a node‚Äôs neighbors. The difference is that standard convolutions operate on local Euclidean in an image, graph convolution extend this concept to non-Euclidean data, where nodes are connected by an adjacency matrix. Let\u0026rsquo;s take a look at mathmatical definition of graph convolution.\n\\[\rH^{(l+1)} = \\sigma \\left( \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2} H^{(l)} W^{(l)} \\right)\r\\] \\(H^{(l)} \\in \\mathbb{R}^{n \\times d}\\) is the feature matrix at layer \\(l\\), where each row is a node\u0026rsquo;s feature vector \\(\\tilde{A} = A + I\\) is the adjacency matrix with identity matrix (self-loops) \\(\\tilde{D}\\) is the degree matrix of \\(A + I\\) \\(W^{(l)} \\in \\mathbb{R}^{d \\times d}\\) is the weight matrix at layer \\(l\\) \\(\\sigma\\) is the non-linear function (e.g., ReLU) The key concept of convolution is to aggregate information from neighbors. Let\u0026rsquo;s see how the equation is derived. It starts from \\(H' = AH\\), which means that each node\u0026rsquo;s new feature representation is obtained by summing the feature vectors of its direct neighbors. However, there are two major issues:\nIt doesn\u0026rsquo;t include the node\u0026rsquo;s own features It doesn\u0026rsquo;t normalize the contribution of neightbors, which can lead to exploding gradient or vanishing gradient To include the noide\u0026rsquo;s own features, we can add self-loops to the adjacency matrix \\(\\tilde{A} = A + I\\) such that \\(H' = \\tilde{A}H\\). To normalize the contribution of neighbors, we can use the degree matrix \\(\\tilde{D} = D\\). To normalize the feature aggregation, we introduce degree matrix \\(\\tilde{D} = \\sum_j\\tilde{A}_{ij}\\). The natual normalization approach is \\(H' = \\tilde{D}^{-1}\\tilde{A}H\\) because each node averages its neighbors\u0026rsquo; contributions. However, when normalizing adjacency matrix, the symmetric normalization approach is used such that \\(H' = \\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}H\\). The reason is \\(\\tilde{D}^{-1}\\tilde{A}\\) only ensures row noalization and \\(\\tilde{A}\\tilde{D}^{-1}\\) only ensures column normalization. The symmetric normalization approach is more stable.\nMulti-label Classification with GCN Now that we understand the definition of the graph convolution operation, let\u0026rsquo;s explore one of its most popular use cases. Imagine a random image of a tennis game. In this image, you‚Äôd likely see a person holding a racket and attempting to hit a tennis ball. These objects are not just randomly placed in the scene‚Äîthey are inherently connected. If there‚Äôs a tennis ball, it‚Äôs highly likely that a racket is nearby, and if there‚Äôs a racket, it‚Äôs probably being held by a person.\nMulti-Label Image Recognition With Graph Convolutional Networks was published in 2019 and cited more than 1k. The authors use GCN to capture semantic dependencies between object labels in multi-label image recognition. Yes, this approach uses semantic graph representation. Instead of treating labels as independent categories, their approach models them as nodes in a directed graph, where edges represent co-occurrence relationships learned from data. Then, are they ignoring images? Of course not, they use CNN to encode the image features.\nCode Review The below code is the source code of the ML-GCN proposed by the authors.\nimport torchvision.models as models from torch.nn import Parameter from util import * import torch import torch.nn as nn class GraphConvolution(nn.Module): def __init__(self, in_features, out_features, bias=False): super(GraphConvolution, self).__init__() self.in_features = in_features self.out_features = out_features self.weight = Parameter(torch.Tensor(in_features, out_features)) if bias: self.bias = Parameter(torch.Tensor(1, 1, out_features)) else: self.register_parameter(\u0026#39;bias\u0026#39;, None) self.reset_parameters() def reset_parameters(self): stdv = 1. / math.sqrt(self.weight.size(1)) self.weight.data.uniform_(-stdv, stdv) if self.bias is not None: self.bias.data.uniform_(-stdv, stdv) def forward(self, input, adj): support = torch.matmul(input, self.weight) output = torch.matmul(adj, support) if self.bias is not None: return output + self.bias else: return output def __repr__(self): return self.__class__.__name__ + \u0026#39; (\u0026#39; \\ + str(self.in_features) + \u0026#39; -\u0026gt; \u0026#39; \\ + str(self.out_features) + \u0026#39;)\u0026#39; class GCNResnet(nn.Module): def __init__(self, model, num_classes, in_channel=300, t=0, adj_file=None): super(GCNResnet, self).__init__() self.features = nn.Sequential( model.conv1, model.bn1, model.relu, model.maxpool, model.layer1, model.layer2, model.layer3, model.layer4, ) self.num_classes = num_classes self.pooling = nn.MaxPool2d(14, 14) self.gc1 = GraphConvolution(in_channel, 1024) self.gc2 = GraphConvolution(1024, 2048) self.relu = nn.LeakyReLU(0.2) _adj = gen_A(num_classes, t, adj_file) self.A = Parameter(torch.from_numpy(_adj).float()) # image normalization self.image_normalization_mean = [0.485, 0.456, 0.406] self.image_normalization_std = [0.229, 0.224, 0.225] def forward(self, feature, inp): feature = self.features(feature) feature = self.pooling(feature) feature = feature.view(feature.size(0), -1) inp = inp[0] adj = gen_adj(self.A).detach() x = self.gc1(inp, adj) x = self.relu(x) x = self.gc2(x, adj) x = x.transpose(0, 1) x = torch.matmul(feature, x) return x def get_config_optim(self, lr, lrp): return [ {\u0026#39;params\u0026#39;: self.features.parameters(), \u0026#39;lr\u0026#39;: lr * lrp}, {\u0026#39;params\u0026#39;: self.gc1.parameters(), \u0026#39;lr\u0026#39;: lr}, {\u0026#39;params\u0026#39;: self.gc2.parameters(), \u0026#39;lr\u0026#39;: lr}, ] def gen_adj(A): D = torch.pow(A.sum(1).float(), -0.5) D = torch.diag(D) adj = torch.matmul(torch.matmul(A, D).t(), D) return adj def gcn_resnet101(num_classes, t, pretrained=False, adj_file=None, in_channel=300): model = models.resnet101(pretrained=pretrained) return GCNResnet(model, num_classes, t=t, adj_file=adj_file, in_channel=in_channel) The first thing we should look at is that the forward function of GCNResnet. It takes feature and inp, which is word embedding. The authors stated that they adopted 300-dim GloVe for label representation. But why didn\u0026rsquo;t they just uses one hot encoding for the labels? One-hot encoding represents labels as discrete, independent categories, meaning it does not capture any semantic relationships between them. In contrast, GloVe embeddings encode words in a continuous space where semantically similar words have closer representations.\nThe next thing we look is adj = gen_adj(self.A).detach() in the forward function. Here, self.A is the adjacency matrix. The adjacency matrix is processed using gen_adj() function to generate the normalized adjacency matrix, \\( \\hat A = \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2}\\). The ML-GCN uses two graph convolutional layers. self.gc1(inp, adj) and self.gc2(x, adj). The inp represents the word embedding \\(H^{l}\\). The forward function of GraphConvolution performs \\(H^{l+1} = \\hat AH^{l}W^{l}\\)\nLastly, the graph embedding and image embedding are multiplied to generate the final multi-label predictions by torch.matmul(feature, x). The output of the GCNResnet has a dimension of \\((\\text{batch}, \\text{number of classes})\\), where each entry represents the probability score for each class in the image. The network is trained using multilabel classification loss (BCE) funciton.\nConclusion We dipped our toes into key concepts of graph theory and how graph representation learning finds its way into the field of computer vision. We explored Graph Convolutional Networks (GCNs) and their application to multi-label image classification. Graph learning continues to gain momentum in academic research. What we learned is just the tip of the adjacency matrix, but graph learning extends far beyond classification. Researchers have been unlocking breakthroughs in semantic segmentation, action recognition, person re-identification, object tracking, and visual question answering. Plus, with graph transformers making waves in both NLP and computer vision, graph representation learning is gearing up for even bigger roles. Thanks for reading!\nReference Graph Representation Learning Meets Computer Vision: A Survey Multi-Label Image Recognition with Graph Convolutional Networks, CVPR 2019 https://mbernste.github.io/posts/gcn/ https://www.youtube.com/watch?v=CwHNUX2GWvE https://math.stackexchange.com/questions/3035968/interpretation-of-symmetric-normalised-graph-adjacency-matrix ","permalink":"http://localhost:1313/posts/2024-07-25_gcn/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/2024-07-25_GCN/cover.png\" alt=\"Cover Image\"  /\u003e\r\n\nGraph structures have been applied in many scientific fields, such as biology, computer science, and social network analysis. With the increasing popularity of machine learning, the graph representation learning (GRL) paradigm has emerged as effective methods. One example is the Graph Convolutional Network (GCN), which has shown remarkable success in tasks like node classification, graph generation and clustering by effectively capturing the complex relationships in graph data. GRL is also making big waves in modern computer vision.\u003c/p\u003e","title":"The Power of Graph Representation Learning in Modern Computer Vision"},{"content":"Why Low Rank Adaptation Matters: A Closer Look at Its Impact on Machine Learning Low Rank Adaptation (LoRA) is a fine-tuning technique designed to efficiently update and adapt large pre-trained models, such as language or diffusion models, without retraining them entirely. Low Rank Adaptation was proposed in 2021 by Edward Hu et al. They demonstrated that LoRA significantly reduces the number of trainable parameters and GPU memory requirements. But how is that possible? In this blog post, we will explore LoRA and understand the foundational principles underlying its concept.\n1. Linear Algebra: Rank Before we delve into Low Rank Adaptation, we first should be familar with rank of the matrix. The rank of a matrix is a fundamental concept in linear algebra that measures the dimensionality of the vector space spanned by its rows or columns. More intuitively, it represents the maximum number of linearly independent row vectors or column vectors in the matrix. Let\u0026rsquo;s see a few examples.\nMatrix with Rank 1 The second row \\(A_2\\) is equal to \\(2A_1\\) The third row \\(A_3\\) is equal to \\(3A_1\\) Each row is linearly dependent on the other rows \\[ A = \\begin{bmatrix}\r1 \u0026 2 \u0026 3 \\\\\r2 \u0026 4 \u0026 6 \\\\\r3 \u0026 6 \u0026 9 \\\\\r\\end{bmatrix}\r\\] Matrix with Rank 2 \\(A_3 = A_1 + A_2\\) The first two rows are linearly independent but the third row is the sum of the first two rows. Since the number of independent rows are two, \\(rank(A)\\) = 2 \\[ A= \\begin{bmatrix}\r1 \u0026 0 \u0026 1 \\\\\r0 \u0026 1 \u0026 1 \\\\\r1 \u0026 1 \u0026 2 \\\\\r\\end{bmatrix}\r\\] Matrix with Full Rank Each row cannot be represented combination of other rows In other words, all rows are linearly independent to other rows \\(rank(A)\\) is full rank \\[\r\\begin{bmatrix}\r1 \u0026 2 \u0026 3 \\\\\r4 \u0026 5 \u0026 6 \\\\\r7 \u0026 8 \u0026 10 \\\\\r\\end{bmatrix}\r\\] Tip: You can use echelon forms to calculate the rank of the matrix.\n2. Fine-tuning a Large Model Insights from Finetuning LLMs with Low-Rank Adaptation - Youtube\nThe GPT models follow a two-stage training approach that consists of pre-training on a large corpus of text in an unsupervised manner and fine-tuning on a specific task in a supervised manner. It\u0026rsquo;s obvious to think that pre-training is expensive and time-consuming. According to Nvidia documentation, training a GPT-2 model with 1.5 billion parameters takes roughly 100,000 GPU hours on 16 A100 GPUs. But what about fine-tuning?\nLet\u0026rsquo;s first talk about full-fine tuning. Full-fine tuning is an approach to activate all the loaded parameters from pre-training. In other words, not freezing any layers of the model. The problem is the model is too large. Not only updating the parameters through epochs, but also loading the parameters into memory is expensive. For instance, GPT3 model has over 175 billion parameters and it requires 400GB of VRAM and takes minutes to load the model.\nYou may ask two questions reading above.\nDo we need to fine-tune all parameters? How expressive should the matrix updates be? We will find out the answers to these questions in the next section.\n3. Low Rank Adaptaion Let\u0026rsquo;s answer the first question. Do we need to fine-tune all parameters? The answer is no. In the paper, the author said that LoRA freezes the pre-trained weights. Actually, it\u0026rsquo;s a common approach to freeze some of layers in fine-tuning. Traditionally, the lower layers are frozen and top layers or newly added layers (often called adapter) for specific task are unfrozen. This is because we assume that the model already learned the low level feature of the model in deeper layers. However, you shouldn\u0026rsquo;t confuse this approach with LoRA.\nRestructuring the fine-tuning First, let\u0026rsquo;s formulate fine-tuning process mathematically.\n$$h = Wx + \\Delta W$$Where:\n\\(h\\) is output embedding \\(x\\) is the input vector to the model \\(W\\) is original weight matrix from pre-trained model \\(\\Delta W\\) represents the derivitive weight matrix from backpropagation Instead of directly computing gradient descent of \\(W\\) to obtain \\(\\Delta W\\), we can treat \\(\\Delta W\\) as a independent set of weight. You can create the linear layer matrix that has the same dimension with \\(W\\).\nIt might come off not clear until you see the code.\nclass DeltaW(nn.Module): def __init__(self, in_dim, out_dim): super().__init__() self.weight = nn.Parameter(torch.randn(in_dim, out_dim)) #weight without bias def forward(self, x): x = x @ self.weight return x # Model weight class Model(nn.Module): def __init__(self): super().__init__() self.linear1 = nn.Linear(100, 1000) #let\u0026#39;s say this layer is frozen and loaded self.delta_w1 = DeltaW(100, 1000) self.linear2 = nn.Linear(1000, 10) #frozen and loaded self.delta_w2 = DeltaW(1000, 10) def forward(self, x): x = self.relu(self.linear1(x) + self.delta_w1(x)) x = self.linear2(x) + self.delta_w2(x) return x In the above pseudo code, let\u0026rsquo;s say linear1 and linear2 layers are original pre-trained weights. We treat them as if these two layers are frozen. When you fine-tune this model, it will be identical to fine-tune the original model without delta_w1 and delta_w2.\nIdea of LoRA Again, fine-tuning \\(\\Delta W\\) in the model is expensive. But what if the change in weights during model adaptation also has a low intrisic rank? This is the key hypothesis of LoRA. Instead of directly updating \\(\\Delta W\\), we can decompose it into two smaller matrices, \\(A\\) and \\(B\\) such that:\n$$\\Delta W = A \\times B$$ Where \\(A\\) is a low-rank matrix and \\(B\\) projects it back to the original dimensions. The rank of these matrices is significantly lower than the original dimensions of \\(\\Delta W\\), leading to a substantial reduction in the number of trainable parameters.\nIf you need wrap your head around, assume the weight matrix \\(W\\) of specific layer has dimensions of \\(5000 \\times 10000\\), resulting in 50 million parameters in total. If you decompose \\(W\\) into \\(A\\) and \\(B\\) where \\(A\\) has dimensions of \\(5000 \\times 8\\) and \\(B\\) has dimensions of \\(8 \\times 10000\\), then the rank of \\(W\\) is 5000. Combined, these matrices account for only \\(80,000 + 40,000 = 120,000\\) parameters, which is 400 times fewer than the 50 million parameters involved in standard fine-tuning.\nHow to determine rank \\(r\\) The right figure in the above image shows LoRA approach. Here, \\(r\\) denotes the rank of \\(\\Delta W\\) and is a hyperparameter that determines the rank of \\(A\\) and \\(B\\). While I was reading the paper, I was confused because \\(r\\) represents the smaller dimension of \\(A\\) and \\(B\\), and also represents the rank of \\(\\Delta W\\). You need to understand the principle of low-rank factorization. The goal of Low-Rank Matrix Factorization is to approximate a high-dimensional matrix as a product of two smaller matrices \\(A\\) and \\(B\\) by constraining the dimension of \\(A\\) and \\(B\\) to \\(\\mathbb R^{n \\times r}\\) and \\(\\mathbb R^{r \\times m}\\) respectively. \\(r\\) is determined the rank of \\(\\Delta W\\). The motivation of low-rank approximation is that we keep the information of the original matrix by keeping rank.\nThink about this. The best way to reduce the number of parameters is just setting \\(r\\) as low as possible. The number of parameters would be as follows when \\(\\Delta W\\) has dimension of \\(5000 \\times 10000\\),\n\\(r\\) = 1, then num_param = 15000 \\(r\\) = 2, then num_param = 30000 \\(r\\) = 3, then num_param = 45000 Let me ask again: why can just set \\(r\\) to 1? This would result in a model with only 15,000 parameters. The reason is that setting \\(r\\) below the rank of \\(\\Delta W\\) can lead to the loss of significant information. Here, the critical factor is the rank of the matrix \\(\\Delta W\\). So, can we determine the rank of \\(\\Delta W\\) and shape \\(A\\) and \\(B\\) accordingly? We can but computing the rank of \\(\\Delta W\\) for all layers is intractable. In the paper the value of \\(r\\) is fixed at several predetermined levels: 1, 2, 8, 64, \u0026hellip; , up to 1024. So basically, the paper shows the investigation of the value of \\(r\\) to find what the rank of \\(\\Delta W\\) so we can decompose \\(\\Delta W\\) into \\(A\\) and \\(B\\).\nLoRA Implementation Let\u0026rsquo;s write the implementation of LoRA based on what we learned so far. There are a few things to note for implementation.\n\\(A\\) is initialized with a Gaussian distribution \\(B\\) is initialized with zero \\(\\alpha\\) is used for scaling the \\(\\Delta W\\) import torch.nn as nn class LoRALayer(nn.Module): def __init__(self, in_dim, out_dim, rank, alpha): super().__init__() std_dev = 1 / torch.sqrt(torch.tensor(rank).float()) self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev) self.B = nn.Parameter(torch.zeros(rank, out_dim)) self.alpha = alpha def forward(self, x): x = self.alpha * (x @ self.A @ self.B) return x With this implementation, we can replace pretrained linear layers with LoRA layers.\nConclusion Low Rank Adaptation (LoRA) offers an efficient way to fine-tune large pre-trained models by reducing the number of trainable parameters and memory requirements. It utilizes concepts from linear algebra to maintain model effectiveness while lowering computational demands. This method allows for deeper models to be fine-tuned more easily and with fewer resources. Overall, LoRA is a crucial advancement that enhances the usability and efficiency of machine learning models.\nReference https://www.youtube.com/watch?v=DhRoTONcyZE https://blog.ml6.eu/low-rank-adaptation-a-technical-deep-dive-782dec995772 https://www.youtube.com/watch?v=rgmJep4Sba4 https://www.youtube.com/watch?v=PXWYUTMt-AU https://lightning.ai/lightning-ai/studios/code-lora-from-scratch ","permalink":"http://localhost:1313/posts/2024-07-18_lora/","summary":"\u003ch1 id=\"why-low-rank-adaptation-matters-a-closer-look-at-its-impact-on-machine-learning\"\u003eWhy Low Rank Adaptation Matters: A Closer Look at Its Impact on Machine Learning\u003c/h1\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/2024-07-18_LoRA/trainingStep.png\" alt=\"Alt text\"  /\u003e\r\n\u003c/p\u003e\n\u003cp\u003eLow Rank Adaptation (LoRA) is a fine-tuning technique designed to efficiently update and adapt large pre-trained models, such as language or diffusion models, without retraining them entirely. Low Rank Adaptation was proposed in 2021 by Edward Hu et al. They demonstrated that LoRA significantly reduces the number of trainable parameters and GPU memory requirements. But how is that possible? In this blog post, we will explore LoRA and understand the foundational principles underlying its concept.\u003c/p\u003e","title":"Low Rank Adaptation"}]