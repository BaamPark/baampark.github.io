<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Baam&#39;s Techlog</title>
    <link>https://baampark.github.io/</link>
    <description>Recent content on Baam&#39;s Techlog</description>
    <generator>Hugo -- 0.122.0</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 03 Jun 2025 15:28:55 -0400</lastBuildDate>
    <atom:link href="https://baampark.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Inference in Autoregressive Language Models</title>
      <link>https://baampark.github.io/posts/autoregressive_inference/</link>
      <pubDate>Tue, 03 Jun 2025 15:28:55 -0400</pubDate>
      <guid>https://baampark.github.io/posts/autoregressive_inference/</guid>
      <description>Most large language models (LLMs) today are autoregressive models. Before LLMs, NLP was fragmented — different problems like text classification, translation, summarization, and question answering all needed their own models, datasets, and training tricks. But then came GPT-2, and everything changed. GPT-2 is an autoregressive model trained purely on text generation — predicting the next word in a sequence. Surprisingly, this simple setup made it capable of handling a wide range of NLP tasks, often without fine-tuning.</description>
    </item>
    <item>
      <title>Smoothed Particle Hydrodynamics Simulation with CUDA</title>
      <link>https://baampark.github.io/posts/2025-04-06_sph/</link>
      <pubDate>Sun, 06 Apr 2025 15:04:51 -0500</pubDate>
      <guid>https://baampark.github.io/posts/2025-04-06_sph/</guid>
      <description>In this blog post, I will share my journey with my final project for my computer graphics course at school. Computer graphics is used to generate images, animations, and visual effects. You might see mechanical engineering students doing CAD (Computer-Aided Design) work — that’s also a form of computer graphics, though it focuses more on precision modeling and simulation for physical systems. OpenGL is is an API for rendering 2D and 3D vector graphics, commonly used by engineers and architects for CAD behind the hood.</description>
    </item>
    <item>
      <title>Mathmatical Foundation for Reinforcement Learning</title>
      <link>https://baampark.github.io/posts/2025-02-23_rl_math/</link>
      <pubDate>Sun, 23 Feb 2025 15:04:51 -0500</pubDate>
      <guid>https://baampark.github.io/posts/2025-02-23_rl_math/</guid>
      <description>Markov Property The Markov Property is a fundamental concept in probability theory that states that the future state of a process depends only on its current state and not on the sequence of events that preceded it.
Random Process A random process (also known as a stochastic process) is a collection of random variables indexed by time. \[\{X_t, t \in [0, \infty)\}\] It’s often used to model real-world data that changes unpredictably.</description>
    </item>
    <item>
      <title>Segment Anything 2 vs. SAM1: What’s New and Why It Matters</title>
      <link>https://baampark.github.io/posts/2025-02-06_sam2/</link>
      <pubDate>Thu, 06 Feb 2025 12:19:07 -0500</pubDate>
      <guid>https://baampark.github.io/posts/2025-02-06_sam2/</guid>
      <description>In my last post, we explored how Segment Anything (SAM) works in image segmentation, breaking down the key components of its model architecture. SAM achieved great success in image segmentation, demonstrating two key strengths: its foundation as a large-scale model trained on an extensive dataset and its ability to be promptable, allowing users to generate segmentations with flexible inputs. These two strengths allow SAM to deliver impressive performance in a zero-shot setting.</description>
    </item>
    <item>
      <title>Segment Anything, the first large-scale foundation model for segmentation</title>
      <link>https://baampark.github.io/posts/2025-01-29_sam/</link>
      <pubDate>Wed, 29 Jan 2025 13:49:47 -0500</pubDate>
      <guid>https://baampark.github.io/posts/2025-01-29_sam/</guid>
      <description>Segment Anything (SAM) has drawn massive attention in the computer vision community, accumulating an impressive 8,000 citations. Segmentation has long been a crucial yet challenging aspect of computer vision. One of the biggest hurdles? Annotation. Unlike simple bounding boxes, which only require marking the object’s general location, segmentation demands precise pixel-level annotations—an incredibly tedious and time-consuming task for annotators. SAM is one of the first large-scale foundation models for segmentation.</description>
    </item>
    <item>
      <title>How Transformers Handle Variable-length Sequnces</title>
      <link>https://baampark.github.io/posts/2025-01-28_variable_sequence/</link>
      <pubDate>Mon, 27 Jan 2025 13:49:47 -0500</pubDate>
      <guid>https://baampark.github.io/posts/2025-01-28_variable_sequence/</guid>
      <description>&amp;ldquo;Transformer models don&amp;rsquo;t require a fixed sequence length.&amp;rdquo; Since most of my projects revolve around computer vision, this was very confusing to me. In computer vision models, images are always preprocessed to a fixed size before being fed into deep learning models. Otherwise, you will encounter matrix multiplication error. In this post, we will learn how transofrmer handles variable-length sequnces.
Self-attention - Q, K, V Linear Projection into Embedding Space Let&amp;rsquo;s see basic CNN code example.</description>
    </item>
    <item>
      <title>The Power of Graph Representation Learning in Modern Computer Vision</title>
      <link>https://baampark.github.io/posts/2024-07-25_gcn/</link>
      <pubDate>Thu, 25 Jul 2024 10:02:46 -0400</pubDate>
      <guid>https://baampark.github.io/posts/2024-07-25_gcn/</guid>
      <description>Graph structures have been applied in many scientific fields, such as biology, computer science, and social network analysis. With the increasing popularity of machine learning, the graph representation learning (GRL) paradigm has emerged as effective methods. One example is the Graph Convolutional Network (GCN), which has shown remarkable success in tasks like node classification, graph generation and clustering by effectively capturing the complex relationships in graph data. GRL is also making big waves in modern computer vision.</description>
    </item>
    <item>
      <title>Low Rank Adaptation</title>
      <link>https://baampark.github.io/posts/2024-07-18_lora/</link>
      <pubDate>Thu, 18 Jul 2024 04:05:02 -0400</pubDate>
      <guid>https://baampark.github.io/posts/2024-07-18_lora/</guid>
      <description>Why Low Rank Adaptation Matters: A Closer Look at Its Impact on Machine Learning Low Rank Adaptation (LoRA) is a fine-tuning technique designed to efficiently update and adapt large pre-trained models, such as language or diffusion models, without retraining them entirely. Low Rank Adaptation was proposed in 2021 by Edward Hu et al. They demonstrated that LoRA significantly reduces the number of trainable parameters and GPU memory requirements. But how is that possible?</description>
    </item>
  </channel>
</rss>
