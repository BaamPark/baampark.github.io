<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Baam&#39;s Techlog</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on Baam&#39;s Techlog</description>
    <generator>Hugo -- 0.123.7</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 29 Jan 2025 13:49:47 -0500</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Segment Anything, the first large-scale foundation model for segmentation</title>
      <link>http://localhost:1313/posts/2025-01-29_sam/</link>
      <pubDate>Wed, 29 Jan 2025 13:49:47 -0500</pubDate>
      <guid>http://localhost:1313/posts/2025-01-29_sam/</guid>
      <description>Segment Anything (SAM) has drawn massive attention in the computer vision community, accumulating an impressive 8,000 citations. Segmentation has long been a crucial yet challenging aspect of computer vision. One of the biggest hurdles? Annotation. Unlike simple bounding boxes, which only require marking the object’s general location, segmentation demands precise pixel-level annotations—an incredibly tedious and time-consuming task for annotators. SAM is one of the first large-scale foundation models for segmentation.</description>
    </item>
    <item>
      <title>Handling large document challenges for fine-tuning LLM</title>
      <link>http://localhost:1313/posts/2024-10-27_llm/</link>
      <pubDate>Sun, 27 Oct 2024 23:10:58 -0400</pubDate>
      <guid>http://localhost:1313/posts/2024-10-27_llm/</guid>
      <description>In most LLM fine-tuning tutorials, they start with an available online dataset that you can easily use to play around with and fine-tune models. However, with real-world datasets, you might encounter tricky settings when handling large documents. Recently, I attended the Rutgers Health 2024 hackathon, where I had to deal with a document larger than an LLM&amp;rsquo;s context window size. I couldn&amp;rsquo;t find a solution back then, so our team chose a large-sized LLM with a larger context window.</description>
    </item>
    <item>
      <title>The Power of Graph Representation Learning in Modern Computer Vision</title>
      <link>http://localhost:1313/posts/2024-07-25_gcn/</link>
      <pubDate>Thu, 25 Jul 2024 10:02:46 -0400</pubDate>
      <guid>http://localhost:1313/posts/2024-07-25_gcn/</guid>
      <description>Graph structures have been applied in many scientific fields, such as biology, computer science, and social network analysis. With the increasing popularity of machine learning, the graph representation learning (GRL) paradigm has emerged as effective methods. One example is the Graph Convolutional Network (GCN), which has shown remarkable success in tasks like node classification, graph generation and clustering by effectively capturing the complex relationships in graph data. GRL is also making big waves in modern computer vision.</description>
    </item>
    <item>
      <title>Low Rank Adaptation</title>
      <link>http://localhost:1313/posts/2024-07-18_lora/</link>
      <pubDate>Thu, 18 Jul 2024 04:05:02 -0400</pubDate>
      <guid>http://localhost:1313/posts/2024-07-18_lora/</guid>
      <description>Why Low Rank Adaptation Matters: A Closer Look at Its Impact on Machine Learning Low Rank Adaptation (LoRA) is a fine-tuning technique designed to efficiently update and adapt large pre-trained models, such as language or diffusion models, without retraining them entirely. Low Rank Adaptation was proposed in 2021 by Edward Hu et al. They demonstrated that LoRA significantly reduces the number of trainable parameters and GPU memory requirements. But how is that possible?</description>
    </item>
  </channel>
</rss>
