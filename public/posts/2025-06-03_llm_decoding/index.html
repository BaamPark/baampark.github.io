<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>LLM Decoding: Inference in Autoregressive Language Models | Baam&#39;s Techlog</title>
<meta name="keywords" content="LLM, Decoding">
<meta name="description" content="Most large language models (LLMs) today are autoregressive models. Before LLMs, NLP was fragmented — different problems like text classification, translation, summarization, and question answering all needed their own models, datasets, and training tricks. But then came GPT-2, and everything changed. GPT-2 is an autoregressive model trained purely on text generation — predicting the next word in a sequence — that’s called decoding.Surprisingly, this simple setup made it capable of handling a wide range of NLP tasks, often without fine-tuning.">
<meta name="author" content="">
<link rel="canonical" href="https://baampark.github.io/posts/2025-06-03_llm_decoding/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.54405a410796490bc874ab6181fac9b675753cc2b91375d8f882566459eca428.css" integrity="sha256-VEBaQQeWSQvIdKthgfrJtnV1PMK5E3XY&#43;IJWZFnspCg=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://baampark.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://baampark.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://baampark.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://baampark.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://baampark.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
<meta property="og:title" content="LLM Decoding: Inference in Autoregressive Language Models" />
<meta property="og:description" content="Most large language models (LLMs) today are autoregressive models. Before LLMs, NLP was fragmented — different problems like text classification, translation, summarization, and question answering all needed their own models, datasets, and training tricks. But then came GPT-2, and everything changed. GPT-2 is an autoregressive model trained purely on text generation — predicting the next word in a sequence — that’s called decoding.Surprisingly, this simple setup made it capable of handling a wide range of NLP tasks, often without fine-tuning." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://baampark.github.io/posts/2025-06-03_llm_decoding/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-06-03T15:28:55-04:00" />
<meta property="article:modified_time" content="2025-06-03T15:28:55-04:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="LLM Decoding: Inference in Autoregressive Language Models"/>
<meta name="twitter:description" content="Most large language models (LLMs) today are autoregressive models. Before LLMs, NLP was fragmented — different problems like text classification, translation, summarization, and question answering all needed their own models, datasets, and training tricks. But then came GPT-2, and everything changed. GPT-2 is an autoregressive model trained purely on text generation — predicting the next word in a sequence — that’s called decoding.Surprisingly, this simple setup made it capable of handling a wide range of NLP tasks, often without fine-tuning."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://baampark.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "LLM Decoding: Inference in Autoregressive Language Models",
      "item": "https://baampark.github.io/posts/2025-06-03_llm_decoding/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "LLM Decoding: Inference in Autoregressive Language Models",
  "name": "LLM Decoding: Inference in Autoregressive Language Models",
  "description": "Most large language models (LLMs) today are autoregressive models. Before LLMs, NLP was fragmented — different problems like text classification, translation, summarization, and question answering all needed their own models, datasets, and training tricks. But then came GPT-2, and everything changed. GPT-2 is an autoregressive model trained purely on text generation — predicting the next word in a sequence — that’s called decoding.Surprisingly, this simple setup made it capable of handling a wide range of NLP tasks, often without fine-tuning.",
  "keywords": [
    "LLM", "Decoding"
  ],
  "articleBody": "\rMost large language models (LLMs) today are autoregressive models. Before LLMs, NLP was fragmented — different problems like text classification, translation, summarization, and question answering all needed their own models, datasets, and training tricks. But then came GPT-2, and everything changed. GPT-2 is an autoregressive model trained purely on text generation — predicting the next word in a sequence — that’s called decoding.Surprisingly, this simple setup made it capable of handling a wide range of NLP tasks, often without fine-tuning. Once you can generate text well, solving other NLP problems becomes almost trivial. You might’ve heard terms like temperature, top-k, and top-p, which are parameters for LLM inference. In this blog post, we’ll explore how the model chooses from possible next words, how randomness is controlled, and what happens under the hood when you ask an LLM to generate text.\nAutoregressive Model An autoregressive model generates one token at a time, each conditioned on the tokens it has already generated. In other words, it builds sequences step by step, always looking at the past to predict the future. At its core, it’s a way to model time series data. Before the rise of Transformers, architectures like RNNs, LSTMs, and GRUs were the go-to choices for building autoregressive models. But today, especially in the context of large language models, decoder-only Transformers have taken over. In this blog, we’ll focus on how these modern autoregressive models are used for text generation.\nMathematical Formulation of Text Generation In autoregressive models, the goal is to find the most likely output sequence given the input. We can start estimating the conditional probability of a target sequence \\( \\mathbf{y} = (y_1, y_2, \\ldots, y_N) \\) given some input \\( \\mathbf{x} \\). This could be a prompt, a source sentence (in translation), or even empty input (as in pure language modeling).\nThe core idea is to decompose the joint probability of the output sequence into a product of conditional probabilities:\n\\[\rP(y_1, y_2, \\ldots, y_N \\mid \\mathbf{x}) = \\prod_{t=1}^{N} P(y_t \\mid y_1, \\ldots, y_{t-1}, \\mathbf{x})\r\\] Or more compactly:\n\\[\rP(\\mathbf{y} \\mid \\mathbf{x}) = \\prod_{t=1}^{N} P(y_t \\mid y_{\u003c t}, \\mathbf{x})\r\\] Here, \\( y_{\u003c t} \\) refers to all previous tokens before time step \\( t \\).\nAt each step, the language model scores all possible next words and assigns a number to each one — these are called logits \\( z_{t,i} \\). For each token \\( w_i \\) in the vocabulary at step \\( t \\), we can obtain the probability distribution over the next possible token by applying the softmax function:\n\\[\rP(y_t = w_i \\mid y_{\u003c t}, \\mathbf{x}) = \\text{softmax}(z_{t,i})\r\\] The goal of most decoding methods is to search for the most likely overall sequence by selecting a \\( \\hat{\\mathbf{y}} \\) that maximizes the conditional probability:\n\\[\r\\hat{\\mathbf{y}} = \\arg\\max_{\\mathbf{y}} P(\\mathbf{y} \\mid \\mathbf{x})\r\\] However, we have a problem. Finding \\( \\hat{\\mathbf{y}} \\) exactly would require evaluating all possible sequences, which is computationally infeasible due to the combinatorial explosion of possibilities. Think about how many tokens we are dealing with. In GPT-2, the vocabulary size is around 50,000 tokens, and generating even a moderately long sequence—say, 20 tokens—would involve evaluating \\(50,000^{20}\\) possible combinations.\nThis exponential growth makes exact search intractable. So we have to rely on approximation instead to generate output sequences. There are two main approches:\nDeterministic method, which deterministically selects tokens based on model confidence (e.g., greedy decoding, beam search) Stochastic method, which introduces randomness to explore multiple plausible continuations (e.g., top-k, top-p) Deterministic Methods - Greedy Search Decoding Greedy decoding is the simplest decoding strategy. At each timestep, the model selects the token with the highest probability — the one it’s most confident about — and adds it to the output sequence.\nFormally, at each time step \\( t \\), the next token \\( y_t \\) is selected as:\n\\[\ry_t = \\arg\\max_{w_i} P(y_t = w_i \\mid y_{\u003c t}, \\mathbf{x})\r\\] This process continues until the model generates an end-of-sequence token (i.e. ) or reaches a maximum length. While greedy decoding is fast and easy to implement, it has significant limitations. Because it always picks the most likely next token, it can get stuck in repetitive output sequences. For example, let’s say you are given with the following prompt.\nGPT2 are the most\nThe most likely next word would probably be a positive adjective like amazing, miraculous, or powerful. Let’s say the model picks “amazing”:\nGPT2 are the most amazing\nNow let’s think about what might come next. If you look up “amazing” in a dictionary, you’ll find synonyms like fantastic and incredible. In a greedy decoding setup, the model might keep choosing the next most probable word — which could just be another synonym. That’s how you end up with repetitive outputs like:\nGPT2 are the most amazing fantastic powerful\nThis happens because greedy decoding favors locally optimal choices at every step, without regard for sentence structure, coherence, or redundancy over the full sequence.\nDeterministic Methods - Beam Search Decoding ref. Decoding Methods for Generative AI\nBeam search decoding addresses a key limitation of greedy decoding: its tendency to get stuck in local optima by always picking the most probable token at each step.\nInstead of choosing just one token at each timestep, beam search keeps track of the top \\( k \\) most probable partial sequences — known as the beam width. At each step, it expands each of these sequences by all possible next tokens, then keeps the top \\( k \\) new sequences based on their cumulative probabilities.\nThis allows the model to explore multiple potential continuations in parallel, rather than committing to a single path too early. By the end, it selects the complete sequence with the highest overall probability.\nHowever, beam search is not completely free from repetitive sequence because it is still based on maximizing likelihood. Even if we have a very large \\(k\\), the algorithm may still favor sequences composed of high-probability tokens, which often include repeated words or phrases. The model might be confident for output sequences, but that may lack diversity or creativity.\nIn other words, beam search may underperform in open-ended text. For example, when generating a creative story or a conversational response, it often produces bland, repetitive, or overly generic outputs. Suppose a user prompts a model with, “Tell me a story about a robot who learns to paint.” Beam search might yield:\nThere was a robot. The robot wanted to paint. The robot learned to paint. The robot became a great painter.\nThis is coherent and grammatically correct, but it’s also dull and predictable. Even increasing the beam width doesn’t help much — it may just produce multiple variations of the same generic idea. To address this lack of diversity, sampling methods are employed to introduce randomness\nBefore we dive into sampling methods, it’s important to understand a key trade-off in text generation: coherence vs. diversity.\nIf we always choose the most likely next word (as in greedy or beam search), we get coherent but often dull and repetitive text. If we allow more randomness, we can generate more diverse and creative outputs — but at the risk of losing coherence or producing nonsensical sentences. Stochastic Methods - Top-k Sampling Decoding Instead of selecting the single most likely token, top-k sampling restricts the candidate pool to the top \\( k \\) tokens with the highest probabilities. Then, it randomly samples from that pool while low-probability tokens are completely ignored.\nWhat we expect from the degree of \\(k\\)?\nSmall \\( k \\) (e.g., 3): More coherent but less diverse. Large \\( k \\) (e.g., 50): More diverse but with increased risk of incoherence. Stochastice Methods - Top-p Sampling Decoding The top-p sampling, also called nucleus samplling, , is a more adaptive alternative to top-k sampling. Instead of selecting a fixed number of top tokens, top-p sampling chooses from the smallest possible set of tokens whose cumulative probability exceeds a threshold \\( p \\). This is how it works:\nSort all vocabulary tokens by their predicted probability in descending order. Starting from the top, include tokens in the candidate pool until their combined probability exceeds \\( p \\) (e.g., 0.9). Sample the next token from this dynamically sized set. This means the number of candidate tokens changes depending on the shape of the probability distribution. If the model is confident, the set might be very small; if it’s uncertain, the set might include more options.\nI have a question for you. Can \\(p\\) exceed 1? The answer is no. Because we already applied softmax to the logits \\(z_{t, i}\\), the cumulative cumulative probability always add up to 1.\nOkay so when should we consider using top-p sampling over top-k sampling? Top-p sampling is more flexible than top-k:\nWhen the model is very confident (probability mass is concentrated), top-p behaves like greedy decoding. When the model is unsure (probability mass is more spread out), top-p allows more exploration. This adaptiveness helps balance fluency and diversity better than top-k in many cases.\nStochastice Methods - Temperature Temperature controls the randomness of the model’s predictions during sampling. However, temperature is not a sampling method on its own. Temperature is a modifier that adjusts the shape of the probability distribution before sampling happens — whether you’re using top-k or top-p.\nBy default, a language model produces a probability distribution over the vocabulary using softmax. Temperature modifies the shape of this distribution before sampling. Mathematically, logits \\( z_i \\) are divided by a temperature value \\( T \\) before applying softmax:\n\\[\rP(y_t = w_i) = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)}\r\\] Low temperature (\u003c 1.0) sharpens the distribution: High-probability tokens become even more likely. Output is more deterministic and focused. High temperature (\u003e 1.0) flattens the distribution: Differences between token probabilities shrink. Output becomes more diverse and creative — but riskier. I have another question for you. Can we use temperature for greedy search or beam search? Well, there is no point of using temperature in this case. Both search algorithms will always choose the probable token anyway. The temperature can change the shape of the distribution, but it doesn’t change the relative ordering of token probabilities — so the top-1 token remains the same, regardless of the temperature.\nConclusion Autoregressive language models generate text one token at a time, predicting the next word based on everything generated so far.\nGreedy and beam search offer more deterministic but often result in repetitive or generic outputs. Sampling-based methods like top-k, top-p, and temperature introduce controlled randomness to improve diversity and creativity. By understanding these decoding strategies, you can better steer large language models toward the kind of output you want.\nReference https://dev.to/nareshnishad/gpt-2-and-gpt-3-the-evolution-of-language-models-15bh Natural Language Processing with Transformers; Lewis Tunstall et al. https://heidloff.net/article/greedy-beam-sampling/ ",
  "wordCount" : "1789",
  "inLanguage": "en",
  "datePublished": "2025-06-03T15:28:55-04:00",
  "dateModified": "2025-06-03T15:28:55-04:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://baampark.github.io/posts/2025-06-03_llm_decoding/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Baam's Techlog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://baampark.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://baampark.github.io/" accesskey="h" title="Baam&#39;s Techlog (Alt + H)">Baam&#39;s Techlog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://baampark.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://baampark.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      LLM Decoding: Inference in Autoregressive Language Models
    </h1>
    <div class="post-meta"><span title='2025-06-03 15:28:55 -0400 EDT'>June 3, 2025</span>

</div>
  </header> 
  <div class="post-content"><p><img loading="lazy" src="/images/2025-06-03_LLM_decoding/cover.png" alt="cover"  />

Most large language models (LLMs) today are autoregressive models. Before LLMs, NLP was fragmented — different problems like text classification, translation, summarization, and question answering all needed their own models, datasets, and training tricks. But then came GPT-2, and everything changed. GPT-2 is an autoregressive model trained purely on text generation — predicting the next word in a sequence — that’s called <strong>decoding</strong>.Surprisingly, this simple setup made it capable of handling a wide range of NLP tasks, often without fine-tuning. Once you can generate text well, solving other NLP problems becomes almost trivial. You might’ve heard terms like temperature, top-k, and top-p, which are parameters for LLM inference. In this blog post, we’ll explore how the model chooses from possible next words, how randomness is controlled, and what happens under the hood when you ask an LLM to generate text.</p>
<h2 id="autoregressive-model">Autoregressive Model<a hidden class="anchor" aria-hidden="true" href="#autoregressive-model">#</a></h2>
<p>An autoregressive model generates one token at a time, each conditioned on the tokens it has already generated. In other words, it builds sequences step by step, always looking at the past to predict the future. At its core, it&rsquo;s a way to model time series data. Before the rise of Transformers, architectures like RNNs, LSTMs, and GRUs were the go-to choices for building autoregressive models. But today, especially in the context of large language models, decoder-only Transformers have taken over. In this blog, we&rsquo;ll focus on how these modern autoregressive models are used for text generation.</p>
<h3 id="mathematical-formulation-of-text-generation">Mathematical Formulation of Text Generation<a hidden class="anchor" aria-hidden="true" href="#mathematical-formulation-of-text-generation">#</a></h3>
<p>In autoregressive models, the goal is to find the most likely output sequence given the input. We can start estimating the conditional probability of a target sequence \( \mathbf{y} = (y_1, y_2, \ldots, y_N) \) given some input \( \mathbf{x} \). This could be a prompt, a source sentence (in translation), or even empty input (as in pure language modeling).</p>
<p>The core idea is to decompose the joint probability of the output sequence into a product of conditional probabilities:</p>
\[
P(y_1, y_2, \ldots, y_N \mid \mathbf{x}) = \prod_{t=1}^{N} P(y_t \mid y_1, \ldots, y_{t-1}, \mathbf{x})
\]
<p>Or more compactly:</p>
\[
P(\mathbf{y} \mid \mathbf{x}) = \prod_{t=1}^{N} P(y_t \mid y_{< t}, \mathbf{x})
\]
<p>Here, \( y_{< t} \) refers to all previous tokens before time step \( t \).</p>
<p>At each step, the language model scores all possible next words and assigns a number to each one — these are called logits \( z_{t,i} \). For each token \( w_i \) in the vocabulary at step \( t \), we can obtain the probability distribution over the next possible token by applying the softmax function:</p>
\[
P(y_t = w_i \mid y_{< t}, \mathbf{x}) = \text{softmax}(z_{t,i})
\]
<p>The goal of most decoding methods is to search for the most likely overall sequence by selecting a \( \hat{\mathbf{y}} \) that maximizes the conditional probability:</p>
\[
\hat{\mathbf{y}} = \arg\max_{\mathbf{y}} P(\mathbf{y} \mid \mathbf{x})
\]
<p>However, we have a problem. Finding \( \hat{\mathbf{y}} \) exactly would require evaluating all possible sequences, which is computationally infeasible due to the combinatorial explosion of possibilities. Think about how many tokens we are dealing with. In GPT-2, the vocabulary size is around 50,000 tokens, and generating even a moderately long sequence—say, 20 tokens—would involve evaluating \(50,000^{20}\) possible combinations.</p>
<p>This exponential growth makes exact search intractable. So we have to rely on approximation instead to generate output sequences. There are two main approches:</p>
<ul>
<li><strong>Deterministic method</strong>, which deterministically selects tokens based on model confidence (e.g., greedy decoding, beam search)</li>
<li><strong>Stochastic method</strong>, which introduces randomness to explore multiple plausible continuations (e.g., top-k, top-p)</li>
</ul>
<h2 id="deterministic-methods---greedy-search-decoding">Deterministic Methods - Greedy Search Decoding<a hidden class="anchor" aria-hidden="true" href="#deterministic-methods---greedy-search-decoding">#</a></h2>
<p>Greedy decoding is the simplest decoding strategy. At each timestep, the model selects the token with the highest probability — the one it’s most confident about — and adds it to the output sequence.</p>
<p>Formally, at each time step \( t \), the next token \( y_t \) is selected as:</p>
\[
y_t = \arg\max_{w_i} P(y_t = w_i \mid y_{< t}, \mathbf{x})
\]
<p>This process continues until the model generates an end-of-sequence token (i.e. <code>&lt;eos&gt;</code>) or reaches a maximum length. While greedy decoding is fast and easy to implement, it has significant limitations. <strong>Because it always picks the most likely next token, it can get stuck in repetitive output sequences.</strong> For example, let&rsquo;s say you are given with the following prompt.</p>
<blockquote>
<p>GPT2 are the most</p>
</blockquote>
<p>The most likely next word would probably be a positive adjective like <em>amazing</em>, <em>miraculous</em>, or <em>powerful</em>. Let’s say the model picks &ldquo;amazing&rdquo;:</p>
<blockquote>
<p>GPT2 are the most amazing</p>
</blockquote>
<p>Now let’s think about what might come next. If you look up “amazing” in a dictionary, you’ll find synonyms like <em>fantastic</em> and <em>incredible</em>. In a greedy decoding setup, the model might keep choosing the next most probable word — which could just be another synonym. That’s how you end up with repetitive outputs like:</p>
<blockquote>
<p>GPT2 are the most amazing fantastic powerful</p>
</blockquote>
<p>This happens because greedy decoding favors locally optimal choices at every step, without regard for sentence structure, coherence, or redundancy over the full sequence.</p>
<h2 id="deterministic-methods---beam-search-decoding">Deterministic Methods - Beam Search Decoding<a hidden class="anchor" aria-hidden="true" href="#deterministic-methods---beam-search-decoding">#</a></h2>
<p><img loading="lazy" src="/images/2025-06-03_LLM_decoding/greedy_vs_beam.jpeg" alt="beam_search"  />

<em><a href="https://heidloff.net/article/greedy-beam-sampling/">ref. Decoding Methods for Generative AI</a></em></p>
<p>Beam search decoding addresses a key limitation of greedy decoding: its tendency to get stuck in local optima by always picking the most probable token at each step.</p>
<p>Instead of choosing just one token at each timestep, beam search keeps track of the top \( k \) most probable partial sequences — known as the <strong>beam width</strong>. At each step, it expands each of these sequences by all possible next tokens, then keeps the top \( k \) new sequences based on their cumulative probabilities.</p>
<p>This allows the model to explore multiple potential continuations in parallel, rather than committing to a single path too early. By the end, it selects the complete sequence with the highest overall probability.</p>
<p>However, beam search is not completely free from repetitive sequence because it is still based on maximizing likelihood. Even if we have a very large \(k\), the algorithm may still favor sequences composed of high-probability tokens, which often include repeated words or phrases.
The model might be confident for output sequences, but that may lack diversity or creativity.</p>
<p>In other words, beam search may underperform in open-ended text. For example, when generating a creative story or a conversational response, it often produces bland, repetitive, or overly generic outputs. Suppose a user prompts a model with, <strong>“Tell me a story about a robot who learns to paint.”</strong> Beam search might yield:</p>
<blockquote>
<p>There was a robot. The robot wanted to paint. The robot learned to paint. The robot became a great painter.</p>
</blockquote>
<p>This is coherent and grammatically correct, but it’s also dull and predictable. Even increasing the beam width doesn’t help much — it may just produce multiple variations of the same generic idea. To address this lack of diversity, sampling methods are employed to introduce randomness</p>
<p>Before we dive into sampling methods, it&rsquo;s important to understand a key trade-off in text generation: <strong>coherence vs. diversity</strong>.</p>
<ul>
<li>If we always choose the most likely next word (as in greedy or beam search), we get coherent but often dull and repetitive text.</li>
<li>If we allow more randomness, we can generate more diverse and creative outputs — but at the risk of losing coherence or producing nonsensical sentences.</li>
</ul>
<h2 id="stochastic-methods---top-k-sampling-decoding">Stochastic Methods - Top-k Sampling Decoding<a hidden class="anchor" aria-hidden="true" href="#stochastic-methods---top-k-sampling-decoding">#</a></h2>
<p><img loading="lazy" src="/images/2025-06-03_LLM_decoding/top_k.png" alt="top_k"  />

Instead of selecting the single most likely token, top-k sampling restricts the candidate pool to the <strong>top \( k \)</strong> tokens with the highest probabilities. Then, it randomly samples from that pool while low-probability tokens are completely ignored.</p>
<p>What we expect from the degree of \(k\)?</p>
<ul>
<li>Small \( k \) (e.g., 3): More coherent but less diverse.</li>
<li>Large \( k \) (e.g., 50): More diverse but with increased risk of incoherence.</li>
</ul>
<h2 id="stochastice-methods---top-p-sampling-decoding">Stochastice Methods - Top-p Sampling Decoding<a hidden class="anchor" aria-hidden="true" href="#stochastice-methods---top-p-sampling-decoding">#</a></h2>
<p>The top-p sampling, also called nucleus samplling, , is a more adaptive alternative to top-k sampling. Instead of selecting a fixed number of top tokens, top-p sampling chooses from the <strong>smallest possible set of tokens whose cumulative probability exceeds a threshold \( p \)</strong>. This is how it works:</p>
<ol>
<li>Sort all vocabulary tokens by their predicted probability in descending order.</li>
<li>Starting from the top, include tokens in the candidate pool until their combined probability exceeds \( p \) (e.g., 0.9).</li>
<li>Sample the next token from this dynamically sized set.</li>
</ol>
<p>This means the number of candidate tokens changes depending on the shape of the probability distribution. If the model is confident, the set might be very small; if it&rsquo;s uncertain, the set might include more options.</p>
<p>I have a question for you. Can \(p\) exceed 1? The answer is no. Because we already applied softmax to the logits \(z_{t, i}\), the cumulative cumulative probability always add up to 1.</p>
<p>Okay so when should we consider using top-p sampling over top-k sampling? Top-p sampling is <strong>more flexible</strong> than top-k:</p>
<ul>
<li>When the model is very confident (probability mass is concentrated), top-p behaves like greedy decoding.</li>
<li>When the model is unsure (probability mass is more spread out), top-p allows more exploration.</li>
</ul>
<p>This adaptiveness helps balance <strong>fluency and diversity</strong> better than top-k in many cases.</p>
<h2 id="stochastice-methods---temperature">Stochastice Methods - Temperature<a hidden class="anchor" aria-hidden="true" href="#stochastice-methods---temperature">#</a></h2>
<p><img loading="lazy" src="/images/2025-06-03_LLM_decoding/temperature.png" alt="top_k"  />

Temperature controls the <strong>randomness</strong> of the model&rsquo;s predictions during sampling. However, temperature is not a sampling method on its own. Temperature is a modifier that adjusts the shape of the probability distribution before sampling happens — whether you&rsquo;re using top-k or top-p.</p>
<p>By default, a language model produces a probability distribution over the vocabulary using softmax. Temperature modifies the shape of this distribution before sampling. Mathematically, logits \( z_i \) are divided by a temperature value \( T \) before applying softmax:</p>
\[
P(y_t = w_i) = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}
\]
<ul>
<li><strong>Low temperature (&lt; 1.0)</strong> sharpens the distribution:
<ul>
<li>High-probability tokens become even more likely.</li>
<li>Output is more deterministic and focused.</li>
</ul>
</li>
<li><strong>High temperature (&gt; 1.0)</strong> flattens the distribution:
<ul>
<li>Differences between token probabilities shrink.</li>
<li>Output becomes more diverse and creative — but riskier.</li>
</ul>
</li>
</ul>
<p>I have another question for you. Can we use temperature for greedy search or beam search? Well, there is no point of using temperature in this case. Both search algorithms will always choose the probable token anyway. The temperature can change the shape of the distribution, but it doesn&rsquo;t change the relative ordering of token probabilities — so the top-1 token remains the same, regardless of the temperature.</p>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>Autoregressive language models generate text one token at a time, predicting the next word based on everything generated so far.<br>
Greedy and beam search offer more deterministic but often result in repetitive or generic outputs. Sampling-based methods like top-k, top-p, and temperature introduce controlled randomness to improve diversity and creativity. By understanding these decoding strategies, you can better steer large language models toward the kind of output you want.</p>
<h2 id="reference">Reference<a hidden class="anchor" aria-hidden="true" href="#reference">#</a></h2>
<ul>
<li><a href="https://dev.to/nareshnishad/gpt-2-and-gpt-3-the-evolution-of-language-models-15bh">https://dev.to/nareshnishad/gpt-2-and-gpt-3-the-evolution-of-language-models-15bh</a></li>
<li>Natural Language Processing with Transformers; Lewis Tunstall et al.</li>
<li><a href="https://heidloff.net/article/greedy-beam-sampling/">https://heidloff.net/article/greedy-beam-sampling/</a></li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://baampark.github.io/tags/llm/">LLM</a></li>
      <li><a href="https://baampark.github.io/tags/decoding/">Decoding</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://baampark.github.io/">Baam&#39;s Techlog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
