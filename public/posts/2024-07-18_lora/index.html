<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Low Rank Adaptation | Baam&#39;s Techlog</title>
<meta name="keywords" content="ML, Efficient AI">
<meta name="description" content="Why Low Rank Adaptation Matters: A Closer Look at Its Impact on Machine Learning


Low Rank Adaptation (LoRA) is a fine-tuning technique designed to efficiently update and adapt large pre-trained models, such as language or diffusion models, without retraining them entirely. Low Rank Adaptation was proposed in 2021 by Edward Hu et al. They demonstrated that LoRA significantly reduces the number of trainable parameters and GPU memory requirements. But how is that possible? In this blog post, we will explore LoRA and understand the foundational principles underlying its concept.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/2024-07-18_lora/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.54405a410796490bc874ab6181fac9b675753cc2b91375d8f882566459eca428.css" integrity="sha256-VEBaQQeWSQvIdKthgfrJtnV1PMK5E3XY&#43;IJWZFnspCg=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/2024-07-18_lora/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Baam&#39;s Techlog (Alt + H)">Baam&#39;s Techlog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://example.org" title="Search">
                    <span>Search</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/cv/" title="CV">
                    <span>CV</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Low Rank Adaptation
    </h1>
    <div class="post-meta"><span title='2024-07-18 04:05:02 -0400 EDT'>July 18, 2024</span>

</div>
  </header> 
  <div class="post-content"><h1 id="why-low-rank-adaptation-matters-a-closer-look-at-its-impact-on-machine-learning">Why Low Rank Adaptation Matters: A Closer Look at Its Impact on Machine Learning<a hidden class="anchor" aria-hidden="true" href="#why-low-rank-adaptation-matters-a-closer-look-at-its-impact-on-machine-learning">#</a></h1>
<p><img loading="lazy" src="/images/2024-07-18_LoRA/trainingStep.png" alt="Alt text"  />
</p>
<p>Low Rank Adaptation (LoRA) is a fine-tuning technique designed to efficiently update and adapt large pre-trained models, such as language or diffusion models, without retraining them entirely. Low Rank Adaptation was proposed in 2021 by Edward Hu et al. They demonstrated that LoRA significantly reduces the number of trainable parameters and GPU memory requirements. But how is that possible? In this blog post, we will explore LoRA and understand the foundational principles underlying its concept.</p>
<h2 id="1-linear-algebra-rank">1. Linear Algebra: Rank<a hidden class="anchor" aria-hidden="true" href="#1-linear-algebra-rank">#</a></h2>
<p>Before we delve into Low Rank Adaptation, we first should be familar with rank of the matrix. The rank of a matrix is a fundamental concept in linear algebra that measures the dimensionality of the vector space spanned by its rows or columns. More intuitively, it represents the maximum number of linearly independent row vectors or column vectors in the matrix. Let&rsquo;s see a few examples.</p>
<h3 id="matrix-with-rank-1">Matrix with Rank 1<a hidden class="anchor" aria-hidden="true" href="#matrix-with-rank-1">#</a></h3>
<ul>
<li>The second row \(A_2\) is equal to \(2A_1\)</li>
<li>The third row \(A_3\) is equal to \(3A_1\)</li>
<li>Each row is linearly dependent on the other rows
\[ A = 
\begin{bmatrix}
1 & 2 & 3 \\
2 & 4 & 6 \\
3 & 6 & 9 \\
\end{bmatrix}
\]</li>
</ul>
<h3 id="matrix-with-rank-2">Matrix with Rank 2<a hidden class="anchor" aria-hidden="true" href="#matrix-with-rank-2">#</a></h3>
<ul>
<li>\(A_3 = A_1 + A_2\)</li>
<li>The first two rows are linearly independent but the third row is the sum of the first two rows.</li>
<li>Since the number of independent rows are two, \(rank(A)\) = 2
\[ A= 
\begin{bmatrix}
1 & 0 & 1 \\
0 & 1 & 1 \\
1 & 1 & 2 \\
\end{bmatrix}
\]</li>
</ul>
<h3 id="matrix-with-full-rank">Matrix with Full Rank<a hidden class="anchor" aria-hidden="true" href="#matrix-with-full-rank">#</a></h3>
<ul>
<li>Each row cannot be represented combination of other rows</li>
<li>In other words, all rows are linearly independent to other rows</li>
<li>\(rank(A)\) is full rank
\[
\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 10 \\
\end{bmatrix}
\]</li>
</ul>
<p><strong>Tip:</strong> You can use echelon forms to calculate the rank of the matrix.</p>
<h2 id="2-fine-tuning-a-large-model">2. Fine-tuning a Large Model<a hidden class="anchor" aria-hidden="true" href="#2-fine-tuning-a-large-model">#</a></h2>
<p><img loading="lazy" src="/images/2024-07-18_LoRA/finetuning_diagram.png" alt="Alt text"  />

<em><!-- raw HTML omitted -->Insights from Finetuning LLMs with Low-Rank Adaptation - Youtube<!-- raw HTML omitted --></em></p>
<p>The GPT models follow a two-stage training approach that consists of pre-training on a large corpus of text in an unsupervised manner and fine-tuning on a specific task in a supervised manner. It&rsquo;s obvious to think that pre-training is expensive and time-consuming. According to Nvidia documentation, training a GPT-2 model with 1.5 billion parameters takes roughly 100,000 GPU hours on 16 A100 GPUs. But what about fine-tuning?</p>
<p>Let&rsquo;s first talk about full-fine tuning. Full-fine tuning is an approach to activate all the loaded parameters from pre-training. In other words, not freezing any layers of the model. The problem is the model is too large. Not only updating the parameters through epochs, but also loading the parameters into memory is expensive. For instance, GPT3 model has over 175 billion parameters and it requires 400GB of VRAM and takes minutes to load the model.</p>
<p>You may ask two questions reading above.</p>
<ul>
<li>Do we need to fine-tune all parameters?</li>
<li>How expressive should the matrix updates be?</li>
</ul>
<p>We will find out the answers to these questions in the next section.</p>
<h2 id="3-low-rank-adaptaion">3. Low Rank Adaptaion<a hidden class="anchor" aria-hidden="true" href="#3-low-rank-adaptaion">#</a></h2>
<p>Let&rsquo;s answer the first question. Do we need to fine-tune all parameters? The answer is no. In the paper, the author said that LoRA freezes the pre-trained weights. Actually, it&rsquo;s a common approach to freeze some of layers in fine-tuning. Traditionally, the lower layers are frozen and top layers or newly added layers (often called adapter) for specific task are unfrozen. This is because we assume that the model already learned the low level feature of the model in deeper layers. However, you shouldn&rsquo;t confuse this approach with LoRA.</p>
<h3 id="restructuring-the-fine-tuning">Restructuring the fine-tuning<a hidden class="anchor" aria-hidden="true" href="#restructuring-the-fine-tuning">#</a></h3>
<p>First, let&rsquo;s formulate fine-tuning process mathematically.</p>
$$h = Wx + \Delta W$$<p>Where:</p>
<ul>
<li>\(h\) is output embedding</li>
<li>\(x\) is the input vector to the model</li>
<li>\(W\) is original weight matrix from pre-trained model</li>
<li>\(\Delta W\) represents the derivitive weight matrix from backpropagation</li>
</ul>
<p>Instead of directly computing gradient descent of \(W\) to obtain \(\Delta W\), we can treat \(\Delta W\) as a independent set of weight. You can create the linear layer matrix that has the same dimension with \(W\).</p>
<p>It might come off not clear until you see the code.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">DeltaW</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, in_dim, out_dim):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>weight <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Parameter(torch<span style="color:#f92672">.</span>randn(in_dim, out_dim)) <span style="color:#75715e">#weight without bias</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x <span style="color:#f92672">@</span> self<span style="color:#f92672">.</span>weight
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Model weight</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Model</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>linear1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">1000</span>) <span style="color:#75715e">#let&#39;s say this layer is frozen and loaded</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>delta_w1 <span style="color:#f92672">=</span> DeltaW(<span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">1000</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>linear2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">1000</span>, <span style="color:#ae81ff">10</span>) <span style="color:#75715e">#frozen and loaded</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>delta_w2 <span style="color:#f92672">=</span> DeltaW(<span style="color:#ae81ff">1000</span>, <span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>linear1(x) <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>delta_w1(x))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>linear2(x) <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>delta_w2(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><p>In the above pseudo code, let&rsquo;s say linear1 and linear2 layers are original pre-trained weights. We treat them as if these two layers are frozen. When you fine-tune this model, it will be identical to fine-tune the original model without delta_w1 and delta_w2.</p>
<h3 id="idea-of-lora">Idea of LoRA<a hidden class="anchor" aria-hidden="true" href="#idea-of-lora">#</a></h3>
<p>Again, fine-tuning \(\Delta W\) in the model is expensive. But what if the change in weights during model adaptation also has a low <strong>intrisic rank</strong>? This is the key hypothesis of LoRA. Instead of directly updating \(\Delta W\), we can decompose it into two smaller matrices, \(A\) and \(B\) such that:</p>
$$\Delta W = A \times B$$<p>
Where \(A\) is a low-rank matrix and \(B\) projects it back to the original dimensions. The rank of these matrices is significantly lower than the original dimensions of \(\Delta W\), leading to a substantial reduction in the number of trainable parameters.</p>
<p>If you need wrap your head around, assume the weight matrix \(W\) of specific layer has dimensions of \(5000 \times 10000\), resulting in 50 million parameters in total. If you decompose \(W\) into \(A\) and \(B\) where \(A\) has dimensions of \(5000 \times 8\) and \(B\) has dimensions of \(8 \times 10000\), then the rank of \(W\) is 5000. Combined, these matrices account for only \(80,000 + 40,000 = 120,000\) parameters, which is 400 times fewer than the 50 million parameters involved in standard fine-tuning.</p>
<h3 id="how-to-determine-rank-r">How to determine rank \(r\)<a hidden class="anchor" aria-hidden="true" href="#how-to-determine-rank-r">#</a></h3>
<p><img loading="lazy" src="/images/2024-07-18_LoRA/lora_diagram.png" alt="Alt text"  />

The right figure in the above image shows LoRA approach. Here, \(r\) denotes the rank of \(\Delta W\) and is a hyperparameter that determines the rank of \(A\) and \(B\). While I was reading the paper, I was confused because \(r\) represents the smaller dimension of \(A\) and \(B\), and also represents the rank of \(\Delta W\). You need to understand the principle of <strong>low-rank factorization</strong>. The goal of Low-Rank Matrix Factorization is to approximate a high-dimensional matrix as a product of two smaller matrices \(A\) and \(B\) by constraining the dimension of \(A\) and \(B\) to \(\mathbb R^{n \times r}\) and \(\mathbb R^{r \times m}\) respectively. \(r\) is determined the rank of \(\Delta W\). The motivation of low-rank approximation is that we keep the information of the original matrix by keeping rank.</p>
<p>Think about this. The best way to reduce the number of parameters is just setting \(r\) as low as possible. The number of parameters would be as follows when \(\Delta W\) has dimension of \(5000 \times 10000\),</p>
<ul>
<li>\(r\) = 1, then num_param = 15000</li>
<li>\(r\) = 2, then num_param = 30000</li>
<li>\(r\) = 3, then num_param = 45000</li>
</ul>
<p>Let me ask again: why can just set \(r\) to 1? This would result in a model with only 15,000 parameters. The reason is that setting \(r\) below the rank of \(\Delta W\) can lead to the loss of significant information. Here, the critical factor is the rank of the matrix \(\Delta W\). So, can we determine the rank of \(\Delta W\) and shape \(A\) and \(B\) accordingly? We can but computing the rank of \(\Delta W\) for all layers is intractable. In the paper the value of \(r\) is fixed at several predetermined levels: 1, 2, 8, 64, &hellip; , up to 1024. So basically, the paper shows the investigation of the value of \(r\) to find what the rank of \(\Delta W\) so we can decompose \(\Delta W\) into \(A\) and \(B\).</p>
<h3 id="lora-implementation">LoRA Implementation<a hidden class="anchor" aria-hidden="true" href="#lora-implementation">#</a></h3>
<p>Let&rsquo;s write the implementation of LoRA based on what we learned so far. There are a few things to note for implementation.</p>
<ul>
<li>\(A\) is initialized with a Gaussian distribution</li>
<li>\(B\) is initialized with zero</li>
<li>\(\alpha\) is used for scaling the \(\Delta W\)</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">LoRALayer</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, in_dim, out_dim, rank, alpha):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        std_dev <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> torch<span style="color:#f92672">.</span>sqrt(torch<span style="color:#f92672">.</span>tensor(rank)<span style="color:#f92672">.</span>float())
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>A <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Parameter(torch<span style="color:#f92672">.</span>randn(in_dim, rank) <span style="color:#f92672">*</span> std_dev)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>B <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Parameter(torch<span style="color:#f92672">.</span>zeros(rank, out_dim))
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>alpha <span style="color:#f92672">=</span> alpha
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>alpha <span style="color:#f92672">*</span> (x <span style="color:#f92672">@</span> self<span style="color:#f92672">.</span>A <span style="color:#f92672">@</span> self<span style="color:#f92672">.</span>B)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><p>With this implementation, we can replace pretrained linear layers with LoRA layers.</p>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>Low Rank Adaptation (LoRA) offers an efficient way to fine-tune large pre-trained models by reducing the number of trainable parameters and memory requirements. It utilizes concepts from linear algebra to maintain model effectiveness while lowering computational demands. This method allows for deeper models to be fine-tuned more easily and with fewer resources. Overall, LoRA is a crucial advancement that enhances the usability and efficiency of machine learning models.</p>
<h3 id="reference">Reference<a hidden class="anchor" aria-hidden="true" href="#reference">#</a></h3>
<ul>
<li><a href="https://www.youtube.com/watch?v=DhRoTONcyZE">https://www.youtube.com/watch?v=DhRoTONcyZE</a></li>
<li><a href="https://blog.ml6.eu/low-rank-adaptation-a-technical-deep-dive-782dec995772">https://blog.ml6.eu/low-rank-adaptation-a-technical-deep-dive-782dec995772</a></li>
<li><a href="https://www.youtube.com/watch?v=rgmJep4Sba4">https://www.youtube.com/watch?v=rgmJep4Sba4</a>
<a href="https://www.youtube.com/watch?v=PXWYUTMt-AU">https://www.youtube.com/watch?v=PXWYUTMt-AU</a></li>
<li><a href="https://lightning.ai/lightning-ai/studios/code-lora-from-scratch">https://lightning.ai/lightning-ai/studios/code-lora-from-scratch</a></li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/ml/">ML</a></li>
      <li><a href="http://localhost:1313/tags/efficient-ai/">Efficient AI</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">Baam&#39;s Techlog</a></span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
