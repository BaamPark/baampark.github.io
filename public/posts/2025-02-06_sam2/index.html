<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Segment Anything 2 vs. SAM1: What’s New and Why It Matters | Baam&#39;s Techlog</title>
<meta name="keywords" content="Segmentation">
<meta name="description" content="In my last post, we explored how Segment Anything (SAM) works in image segmentation, breaking down the key components of its model architecture. SAM achieved great success in image segmentation, demonstrating two key strengths: its foundation as a large-scale model trained on an extensive dataset and its ability to be promptable, allowing users to generate segmentations with flexible inputs. These two strengths allow SAM to deliver impressive performance in a zero-shot setting.">
<meta name="author" content="">
<link rel="canonical" href="https://baampark.github.io/posts/2025-02-06_sam2/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css" integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://baampark.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://baampark.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://baampark.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://baampark.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://baampark.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://baampark.github.io/posts/2025-02-06_sam2/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
<meta property="og:title" content="Segment Anything 2 vs. SAM1: What’s New and Why It Matters" />
<meta property="og:description" content="In my last post, we explored how Segment Anything (SAM) works in image segmentation, breaking down the key components of its model architecture. SAM achieved great success in image segmentation, demonstrating two key strengths: its foundation as a large-scale model trained on an extensive dataset and its ability to be promptable, allowing users to generate segmentations with flexible inputs. These two strengths allow SAM to deliver impressive performance in a zero-shot setting." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://baampark.github.io/posts/2025-02-06_sam2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-02-06T12:19:07-05:00" />
<meta property="article:modified_time" content="2025-02-06T12:19:07-05:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Segment Anything 2 vs. SAM1: What’s New and Why It Matters"/>
<meta name="twitter:description" content="In my last post, we explored how Segment Anything (SAM) works in image segmentation, breaking down the key components of its model architecture. SAM achieved great success in image segmentation, demonstrating two key strengths: its foundation as a large-scale model trained on an extensive dataset and its ability to be promptable, allowing users to generate segmentations with flexible inputs. These two strengths allow SAM to deliver impressive performance in a zero-shot setting."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://baampark.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Segment Anything 2 vs. SAM1: What’s New and Why It Matters",
      "item": "https://baampark.github.io/posts/2025-02-06_sam2/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Segment Anything 2 vs. SAM1: What’s New and Why It Matters",
  "name": "Segment Anything 2 vs. SAM1: What’s New and Why It Matters",
  "description": "In my last post, we explored how Segment Anything (SAM) works in image segmentation, breaking down the key components of its model architecture. SAM achieved great success in image segmentation, demonstrating two key strengths: its foundation as a large-scale model trained on an extensive dataset and its ability to be promptable, allowing users to generate segmentations with flexible inputs. These two strengths allow SAM to deliver impressive performance in a zero-shot setting.",
  "keywords": [
    "Segmentation"
  ],
  "articleBody": " In my last post, we explored how Segment Anything (SAM) works in image segmentation, breaking down the key components of its model architecture. SAM achieved great success in image segmentation, demonstrating two key strengths: its foundation as a large-scale model trained on an extensive dataset and its ability to be promptable, allowing users to generate segmentations with flexible inputs. These two strengths allow SAM to deliver impressive performance in a zero-shot setting. In Jul 2024, “SAM 2: Segment Anything in Images and Videos” was published. While SAM focuses solely on image segmentation, SAM2 takes things a step further. Not only does it improve performance in image segmentation, but it also introduces the ability to handle video segmentation, thanks to its own memory system. This enhancement allows SAM2 to track objects across frames, making it a powerful tool for dynamic and real-time applications. Personally, when I looked at SAM2’s zero-shot performance in tracking objects across video frames , I thought this could be a game-changer in the world of object tracking. In this article, we are gonna mainly talk about memory bank system of SAM2 as the rest of the parts are built on top of SAM.\nSAM2 - Model Architecture If you read my previous post, you might recognize some familiar components in the SAM2 architecture diagram: the image encoder, prompt decoder, and mask decoder—all key elements carried over from SAM. So, what’s new in SAM2? The introduction of three critical components: memory attention module, memory encoder, and memory bank.\nThe memory encoder generates a memory by combining frame embedding and mask prediction across frames. The memory is sent to the memory bank, which retains the memory of past predictions for the target object. The memory attention module leverages the memory stored in the memory bank to enhance object recognition and segmentation across frames. These three components allows SAM2 to generate maskelt prediction, which means track of mask in a video. Even if you don’t provide prompts for the target object in the current video frame, SAM2 can still recognize and segment the target object based on previous prompts you provided in earlier frames. In addition, even if the target object is occluded in the past frame and reappears in the current frame, SAM2 can recover the segmentation.\nMemory Encoder The Memory Encoder generates a memory representation by taking image embeddings and mask outputs as inputs. The image embedding \\(I\\) is produced by the image encoder, which processes the \\(1024 \\times 1024\\) input image and embeds it into feature space. The mask output \\(M\\) is produced from the mask decoder. However, this mask does not come from the current frame—it is obtained from past frames. As the mask is a binary value tensor, it has one channel with the same height and width as input image (i.e. \\(1 \\times H \\times W\\)). The dimensions for these two inputs are:\nImage Embedding \\(I \\in \\mathbb{R} ^ {B \\times 256 \\times 64 \\times 64}\\) Mask Output \\(M \\in \\mathbb{R} ^ {B \\times 1 \\times 1024 \\times 1024} \\) As shown in the diagram, we need to perform element-wise addition between \\(M\\) and \\(I\\). We cannot add them directly because their dimensions are different. To resolve this, we use a down-sampler to project \\(M\\) into the same dimension as \\(I\\). However, element-wise addition alone is not sufficient to effectively combine these two inputs, as it only aligns them spatially without learning meaningful interactions. To better fuse the information from both inputs, the authors apply convolutional layers with a \\(1 \\times 1\\) kernel, reducing the channel dimension. The generated memory \\(\\mathcal{M}\\) will be used later in the memory attention module. In general, an attention mechanism requires not only an input feature but also its positional encoding to capture spatial relationships. Likewise, we need to compute the positional encoding of \\(\\mathcal{M}\\) within the memory encoder block. Therefore, we have two outputs:\nMemory \\(\\mathcal{M} \\in \\mathbb{R} ^{B \\times 64 \\times 64 \\times 64} \\) Positional Embedding of Memory \\(\\text{PE}(\\mathcal{M}) \\in \\mathbb{R} ^{B \\times 64 \\times 64 \\times 64}\\) Refer to the implementation for details: mask encoder\nMemory Bank The memory bank consistently retains the first frame’s memory along with memories of up to N recent (unprompted) frames. Both sets of memories are stored as spatial feature maps.\nFirst, let’s consider why the memory bank stores the first frame’s memory. SAM is an interactive segmentation model, meaning it requires a user’s prompt to generate predictions. Since prompts may not be provided for subsequent frames during inference, it is essential to retain the first frame’s prompt to ensure consistent segmentation across the video. The memory bank is given by: \\[\\mathcal{B} = [\\text{First Frame Memory | Recent N Frames Memory}] \\] Each frame’s memory includes an object pointer, which is appended to its feature representation. First, the memory \\(\\mathcal{M}\\) is reshaped from \\(\\mathbb{R} ^{B \\times 64 \\times 64 \\times 64}\\) to \\( \\mathbb{R} ^{B \\times 4096 \\times 64} \\). The object pointer is added to \\(\\mathcal{M}\\) such that\n\\[\\mathcal{M} := [\\mathcal{M}, \\text{pointer}] \\in \\mathbb{R}^{B \\times (4096 + 4) \\times 64}\\] where the object pointer has a length of four and is concatenated along the sequence dimension. The object pointer is generated by the mask decoder, providing a more compact and stable representation of the object across frames. More details can be found in the SAM2 paper.\nIn addition to the spatial memory, we store a list of object pointers as lightweight vectors for high-level semantic information of the object to segment, based on mask decoder output tokens of each frame. Our memory attention cross-attends to both spatial memory features and these object pointers. … Further, we project the memory features in our memory bank to a dimension of 64, and split the 256-dim object pointer into 4 tokens of 64-dim for cross-attention to the memory bank.\nBut you might ask, what is the use of an object pointer? Instead of relying solely on raw mask features from memory, object pointers provide a compressed representation of object instances. Consider an object that disappears behind another object for a few frames. If we only rely on spatial memory, the model might lose track of the object due to inconsistency between mask features across frames. In contrast, the object pointer provides a more stable and consistent representation of an object across frames.\nThere is no explicit class definition for the memory bank in the source code. However, we can infer its role from the logic in sam2_base.py, where past frame memories (both conditioned and non-conditioned) are stored and retrieved for memory attention. Note that the shape of memory bank \\(\\mathcal{B}\\) changes based on current frame index and the number of memories \\(N\\) such that \\(\\mathcal{B} \\in \\mathbb{R}^{B \\times (N) \\cdot 4010 \\times 64}\\).\n#sam2_base.py https://github.com/facebookresearch/sam2/blob/main/sam2/modeling/sam2_base.py class SAM2Base(torch.nn.Module): def __init__(...): #... self.num_maskmem = num_maskmem # Number of memories accessible def _prepare_memory_conditioned_features(...): #... #list of memory per frame for t_pos, prev in t_pos_and_prevs: #... to_cat_memory.append(feats.flatten(2).permute(2, 0, 1)) to_cat_memory.append(obj_ptrs) memory = torch.cat(to_cat_memory, dim=0) #memory bank #forward memory through memory attention module pix_feat_with_mem = self.memory_attention( curr=current_vision_feats, curr_pos=current_vision_pos_embeds, memory=memory, memory_pos=memory_pos_embed, num_obj_ptr_tokens=num_obj_ptr_tokens, ) Memory Attention The role of memory attention is to condition the current frame features on the past frames features and predictions as well as on any new prompts.\nThe authors used the term condition. So what does condition mean? Conditioning in this context refers to incorporating past frame embeddings (both image and mask-based features) into the processing of the current frame. But how?\nThe memory attention module utilizes both self-attention and cross-attention mechanisms. Through self-attention, the current frame embedding \\(I\\) attends to itself internally. Through cross attention, \\(I\\) attends to memory bank \\(\\mathcal{B}\\), integrating information from past frames. The memory attention layer consists of a self-attention block, followed by a cross-attention block. The inputs to this layer are:\nImage Embedding \\(I \\in \\mathbb{R}^{B \\times\\ 4096 \\times 256}\\) Image Positional Embedding \\(\\text{PE}(I) \\in \\mathbb{R}^{B \\times\\ 4096 \\times 256}\\) Memory Bank \\(\\mathcal{B} \\in \\mathbb{R}^{B \\times N \\cdot 4010 \\times 64}\\) Memory Bank Positional Embedding \\(\\text{PE}(\\mathcal{B}) \\in \\mathbb{R}^{B \\times N \\cdot 4010 \\times 64}\\) The diagram below shows a memory attention layer. For simplicity, normalization, dropout, and MLP layers are excluded from the diagram. By default, memory attention has four memory attention layers. Each layer first applies self-attention, allowing the current frame embedding to refine itself by attending to its own spatial features. Then, cross-attention enables the current frame to incorporate relevant information from the memory. As a result, the memory attention module outputs the conditioned frame feature:\n\\[I_{I|\\mathcal{M}} \\in \\mathbb{R}^{B \\times 4096 \\times 256},\\] which is subsequently used as input to the mask decoder, instead of unconditioned image embedding \\(I\\).\nIn the self-attention block, \\(I + \\text{PE}(I)\\) for the query and key, while \\(I\\) is passed as the value. This raised a major question for me because, typically, the query, key, and value are the same—each being the input feature with added positional embedding. This query-key-value modeling was inspired from DETR (DEtection TRansformer).\nIt starts by computing so-called query, key and value embeddings after adding the query and key positional encodings - DETR\nPositional encoding is essential for self-attention because transformers are inherently permutation-invariant—they lack an inherent sense of sequence order. By adding positional encoding to queries and keys, the model can learn spatial relationships and distinguish between positions. However, is it necessary to apply positional encoding to values if it doesn’t improve performance? If not, it would only introduce unnecessary computational overhead.\nConclusion In this post, we reviewed the technical components of the memory encoder and memory attention module in SAM2. These components work in tandem to ensure consistent mask generation across video frames. The memory encoder captures past frame information, while the memory attention module conditions the current frame using stored memories. By retaining memory over time, the model can generate masks even in the absence of additional prompts. Moreover, the memory bank enables the model to handle occlusions, ensuring that objects remain identifiable even when temporarily hidden. This structured memory mechanism enhances segmentation robustness and adaptability in dynamic video environments. SAM2 represents a significant advancement in computer vision, particularly in video object segmentation. Its ability to maintain memory across frames and handle occlusions makes it a powerful tool for various applications. Given its robust performance, we can expect to see widespread adoption of SAM2 in fields such as medical imaging, autonomous driving, and video editing.\nReference SAM 2: Segment Anything in Images and Videos Segment Anything 2: What Is the Secret Sauce? (A Deep Learner’s Guide) ",
  "wordCount" : "1742",
  "inLanguage": "en",
  "datePublished": "2025-02-06T12:19:07-05:00",
  "dateModified": "2025-02-06T12:19:07-05:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://baampark.github.io/posts/2025-02-06_sam2/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Baam's Techlog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://baampark.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://baampark.github.io/" accesskey="h" title="Baam&#39;s Techlog (Alt + H)">Baam&#39;s Techlog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://baampark.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://baampark.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Segment Anything 2 vs. SAM1: What’s New and Why It Matters
    </h1>
    <div class="post-meta"><span title='2025-02-06 12:19:07 -0500 EST'>February 6, 2025</span>

</div>
  </header> 
  <div class="post-content"><p><img loading="lazy" src="https://about.fb.com/wp-content/uploads/2024/07/01_Dribbling_Carousel-02.gif?fit=800%2C697" alt="cover"  />
</p>
<p>In my <a href="https://baampark.github.io/posts/2025-01-29_sam/">last post</a>, we explored how Segment Anything (SAM) works in image segmentation, breaking down the key components of its model architecture. SAM achieved great success in image segmentation, demonstrating two key strengths: its foundation as a large-scale model trained on an extensive dataset and its ability to be promptable, allowing users to generate segmentations with flexible inputs. These two strengths allow SAM to deliver impressive performance in a zero-shot setting. In Jul 2024, <a href="https://arxiv.org/abs/2408.00714">&ldquo;SAM 2: Segment Anything in Images and Videos&rdquo;</a> was published. While SAM focuses solely on image segmentation, SAM2 takes things a step further. Not only does it improve performance in image segmentation, but it also introduces the ability to handle video segmentation, thanks to its own memory system. This enhancement allows SAM2 to track objects across frames, making it a powerful tool for dynamic and real-time applications. Personally, when I looked at SAM2&rsquo;s zero-shot performance in tracking objects across video frames , I thought this could be a game-changer in the world of object tracking. In this article, we are gonna mainly talk about <strong>memory bank</strong> system of SAM2 as the rest of the parts are built on top of SAM.</p>
<h2 id="sam2---model-architecture">SAM2 - Model Architecture<a hidden class="anchor" aria-hidden="true" href="#sam2---model-architecture">#</a></h2>
<p><img loading="lazy" src="/images/2025-02-06_SAM2/model_architecture.png" alt="model_architecture"  />

If you read my previous post, you might recognize some familiar components in the SAM2 architecture diagram: the image encoder, prompt decoder, and mask decoder—all key elements carried over from SAM. So, what’s new in SAM2? The introduction of three critical components: <strong>memory attention module</strong>, <strong>memory encoder</strong>, and <strong>memory bank</strong>.</p>
<p>The memory encoder generates a memory by combining frame embedding and mask prediction across frames. The memory is sent to the memory bank, which retains the memory of past predictions for the target object. The memory attention module leverages the memory stored in the memory bank to enhance object recognition and segmentation across frames. These three components allows SAM2 to generate maskelt prediction, which means track of mask in a video. <strong>Even if you don’t provide prompts for the target object in the current video frame, SAM2 can still recognize and segment the target object based on previous prompts you provided in earlier frames. In addition, even if the target object is occluded in the past frame and reappears in the current frame, SAM2 can recover the segmentation.</strong></p>
<h2 id="memory-encoder">Memory Encoder<a hidden class="anchor" aria-hidden="true" href="#memory-encoder">#</a></h2>
<p><img loading="lazy" src="/images/2025-02-06_SAM2/memory_encoder.png" alt="memory_encoder"  />

The Memory Encoder generates a memory representation by taking image embeddings and mask outputs as inputs. The image embedding \(I\) is produced by the image encoder, which processes the \(1024 \times 1024\) input image and embeds it into feature space. The mask output \(M\) is produced from the mask decoder. However, this mask does not come from the current frame—it is obtained from past frames. As the mask is a binary value tensor, it has one channel with the same height and width as input image (i.e. \(1 \times H \times W\)).
The dimensions for these two inputs are:</p>
<ul>
<li>Image Embedding \(I \in \mathbb{R} ^ {B \times 256 \times 64 \times 64}\)</li>
<li>Mask Output \(M \in \mathbb{R} ^ {B \times 1 \times 1024 \times 1024} \)</li>
</ul>
<p>As shown in the diagram, we need to perform element-wise addition between \(M\) and \(I\). We cannot add them directly because their dimensions are different. To resolve this, we use a down-sampler to project \(M\) into the same dimension as \(I\). However, element-wise addition alone is not sufficient to effectively combine these two inputs, as it only aligns them spatially without learning meaningful interactions. To better fuse the information from both inputs, the authors apply convolutional layers with a \(1 \times 1\) kernel, reducing the channel dimension. The generated memory \(\mathcal{M}\) will be used later in the memory attention module. In general, an attention mechanism requires not only an input feature but also its positional encoding to capture spatial relationships. Likewise, we need to compute the positional encoding of \(\mathcal{M}\) within the memory encoder block. Therefore, we have two outputs:</p>
<ul>
<li>Memory \(\mathcal{M} \in \mathbb{R} ^{B \times 64 \times 64 \times 64} \)</li>
<li>Positional Embedding of Memory \(\text{PE}(\mathcal{M}) \in \mathbb{R} ^{B \times 64 \times 64 \times 64}\)</li>
</ul>
<p>Refer to the implementation for details: <a href="https://github.com/facebookresearch/sam2/blob/main/sam2/modeling/memory_encoder.py">mask encoder</a></p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h2 id="memory-bank">Memory Bank<a hidden class="anchor" aria-hidden="true" href="#memory-bank">#</a></h2>
<blockquote>
<p><em>The memory bank consistently retains the first frame’s memory along with memories of up to N recent (unprompted) frames. Both sets of memories are stored as spatial feature maps.</em></p>
</blockquote>
<p>First, let&rsquo;s consider why the memory bank stores the first frame&rsquo;s memory. SAM is an interactive segmentation model, meaning it requires a user&rsquo;s prompt to generate predictions. Since prompts may not be provided for subsequent frames during inference, it is essential to retain the first frame&rsquo;s prompt to ensure consistent segmentation across the video. The memory bank is given by:
</p>
\[\mathcal{B} = [\text{First Frame Memory | Recent N Frames Memory}] \]
<p>Each frame&rsquo;s memory includes an object pointer, which is appended to its feature representation. First, the memory \(\mathcal{M}\) is reshaped from \(\mathbb{R} ^{B \times 64 \times 64 \times 64}\) to \( \mathbb{R} ^{B \times 4096 \times 64} \). The object pointer is added to \(\mathcal{M}\) such that</p>
\[\mathcal{M} := [\mathcal{M}, \text{pointer}] \in \mathbb{R}^{B \times (4096 + 4) \times 64}\]
<p>where the object pointer has a length of four and is concatenated along the sequence dimension.
The object pointer is generated by the mask decoder, providing a more compact and stable representation of the object across frames. More details can be found in the SAM2 paper.</p>
<blockquote>
<p><em>In addition to the spatial memory, we store a list of object pointers as lightweight vectors for high-level semantic information of the object to segment, based on mask decoder output tokens of each frame. Our memory attention cross-attends to both spatial memory features and these object pointers. … Further, we project the memory features in our memory bank to a dimension of 64, and split the 256-dim object pointer into 4 tokens of 64-dim for cross-attention to the memory bank.</em></p>
</blockquote>
<p><img loading="lazy" src="/images/2025-02-06_SAM2/object_pointer.png" alt="object_pointer"  />
</p>
<p>But you might ask, what is the use of an object pointer? Instead of relying solely on raw mask features from memory, object pointers provide a compressed representation of object instances. Consider an object that disappears behind another object for a few frames. If we only rely on spatial memory, the model might lose track of the object due to inconsistency between mask features across frames. In contrast, the object pointer provides a more stable and consistent representation of an object across frames.</p>
<p>There is no explicit class definition for the memory bank in the source code. However, we can infer its role from the logic in <code>sam2_base.py</code>, where past frame memories (both conditioned and non-conditioned) are stored and retrieved for memory attention. Note that the shape of memory bank \(\mathcal{B}\) changes based on current frame index and the number of memories \(N\) such that \(\mathcal{B} \in \mathbb{R}^{B \times (N) \cdot 4010 \times 64}\).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e">#sam2_base.py https://github.com/facebookresearch/sam2/blob/main/sam2/modeling/sam2_base.py</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SAM2Base</span>(torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(<span style="color:#f92672">...</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">#...</span>
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>num_maskmem <span style="color:#f92672">=</span> num_maskmem  <span style="color:#75715e"># Number of memories accessible</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_prepare_memory_conditioned_features</span>(<span style="color:#f92672">...</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#list of memory per frame</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> t_pos, prev <span style="color:#f92672">in</span> t_pos_and_prevs:
</span></span><span style="display:flex;"><span>            <span style="color:#75715e">#...</span>
</span></span><span style="display:flex;"><span>            to_cat_memory<span style="color:#f92672">.</span>append(feats<span style="color:#f92672">.</span>flatten(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        to_cat_memory<span style="color:#f92672">.</span>append(obj_ptrs)
</span></span><span style="display:flex;"><span>        memory <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat(to_cat_memory, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>) <span style="color:#75715e">#memory bank</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#forward memory through memory attention module</span>
</span></span><span style="display:flex;"><span>        pix_feat_with_mem <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>memory_attention(
</span></span><span style="display:flex;"><span>            curr<span style="color:#f92672">=</span>current_vision_feats,
</span></span><span style="display:flex;"><span>            curr_pos<span style="color:#f92672">=</span>current_vision_pos_embeds,
</span></span><span style="display:flex;"><span>            memory<span style="color:#f92672">=</span>memory,
</span></span><span style="display:flex;"><span>            memory_pos<span style="color:#f92672">=</span>memory_pos_embed,
</span></span><span style="display:flex;"><span>            num_obj_ptr_tokens<span style="color:#f92672">=</span>num_obj_ptr_tokens,
</span></span><span style="display:flex;"><span>        )
</span></span></code></pre></div><h2 id="memory-attention">Memory Attention<a hidden class="anchor" aria-hidden="true" href="#memory-attention">#</a></h2>
<blockquote>
<p><em>The role of memory attention is to condition the current frame features on the past frames features and predictions as well as on any new prompts.</em></p>
</blockquote>
<p>The authors used the term <em>condition</em>. So what does condition mean? Conditioning in this context refers to incorporating past frame embeddings (both image and mask-based features) into the processing of the current frame. But how?</p>
<p>The memory attention module utilizes both self-attention and cross-attention mechanisms. Through self-attention, the current frame embedding \(I\) attends to itself internally. Through cross attention, \(I\) attends to memory bank \(\mathcal{B}\), integrating information from past frames. The memory attention layer consists of a self-attention block, followed by a cross-attention block. The inputs to this layer are:</p>
<ul>
<li>Image Embedding \(I \in \mathbb{R}^{B \times\ 4096 \times 256}\)</li>
<li>Image Positional Embedding \(\text{PE}(I) \in \mathbb{R}^{B \times\ 4096 \times 256}\)</li>
<li>Memory Bank \(\mathcal{B} \in \mathbb{R}^{B \times N \cdot 4010 \times 64}\)</li>
<li>Memory Bank Positional Embedding \(\text{PE}(\mathcal{B}) \in \mathbb{R}^{B \times N \cdot 4010 \times 64}\)</li>
</ul>
<p>The diagram below shows a memory attention layer. For simplicity, normalization, dropout, and MLP layers are excluded from the diagram. By default, memory attention has four memory attention layers.
Each layer first applies self-attention, allowing the current frame embedding to refine itself by attending to its own spatial features. Then, cross-attention enables the current frame to incorporate relevant information from the memory. As a result, the memory attention module outputs the conditioned frame feature:</p>
\[I_{I|\mathcal{M}} \in \mathbb{R}^{B \times 4096 \times 256},\]
<p>which is subsequently used as input to the mask decoder, instead of unconditioned image embedding \(I\).</p>
<p><img loading="lazy" src="/images/2025-02-06_SAM2/memory_attention_layer.png" alt="mask_attention_layer"  />
</p>
<p>In the self-attention block, \(I + \text{PE}(I)\) for the query and key, while \(I\) is passed as the value. This raised a major question for me because, typically, the query, key, and value are the same—each being the input feature with added positional embedding. This query-key-value modeling was inspired from DETR (DEtection TRansformer).</p>
<blockquote>
<p><em>It starts by computing so-called query, key and value embeddings after adding the query and key positional encodings - <a href="https://arxiv.org/pdf/2005.12872">DETR</a></em></p>
</blockquote>
<p><img loading="lazy" src="/images/2025-02-06_SAM2/DETR.png" alt="DETR"  />
</p>
<p>Positional encoding is essential for self-attention because transformers are inherently permutation-invariant—they lack an inherent sense of sequence order. By adding positional encoding to queries and keys, the model can learn spatial relationships and distinguish between positions. However, is it necessary to apply positional encoding to values if it doesn’t improve performance? If not, it would only introduce unnecessary computational overhead.</p>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>In this post, we reviewed the technical components of the memory encoder and memory attention module in SAM2. These components work in tandem to ensure consistent mask generation across video frames. The memory encoder captures past frame information, while the memory attention module conditions the current frame using stored memories. By retaining memory over time, the model can generate masks even in the absence of additional prompts. Moreover, the memory bank enables the model to handle occlusions, ensuring that objects remain identifiable even when temporarily hidden. This structured memory mechanism enhances segmentation robustness and adaptability in dynamic video environments. SAM2 represents a significant advancement in computer vision, particularly in video object segmentation. Its ability to maintain memory across frames and handle occlusions makes it a powerful tool for various applications. Given its robust performance, we can expect to see widespread adoption of SAM2 in fields such as medical imaging, autonomous driving, and video editing.</p>
<h2 id="reference">Reference<a hidden class="anchor" aria-hidden="true" href="#reference">#</a></h2>
<ul>
<li><a href="https://arxiv.org/abs/2408.00714">SAM 2: Segment Anything in Images and Videos</a></li>
<li><a href="https://towardsdatascience.com/segment-anything-2-what-is-the-secret-sauce-a-deep-learners-guide-1c43dd07a6f8/?source=rss----7f60cf5620c9---4">Segment Anything 2: What Is the Secret Sauce? (A Deep Learner’s Guide)</a></li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://baampark.github.io/tags/segmentation/">Segmentation</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://baampark.github.io/">Baam&#39;s Techlog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
