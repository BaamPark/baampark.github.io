<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Segment Anything, the first large-scale foundation model for segmentation | Baam&#39;s Techlog</title>
<meta name="keywords" content="Segmentation">
<meta name="description" content="

Segment Anything (SAM) has drawn massive attention in the computer vision community, accumulating an impressive 8,000 citations. Segmentation has long been a crucial yet challenging aspect of computer vision. One of the biggest hurdles? Annotation. Unlike simple bounding boxes, which only require marking the object’s general location, segmentation demands precise pixel-level annotations—an incredibly tedious and time-consuming task for annotators. SAM is one of the first large-scale foundation models for segmentation. What makes SAM truly great is that it’s a promptable model. This means you can use it for various tasks without the need for fine-tuning—also known as zero-shot learning. Unlike LLM, here the prompts for the SAM are points, bounding boxes, and masks. In October 2022, Meta&rsquo;s research team released SAM2, extending its capabilities from image segmentation to real-time video segmentation. Given my experience with SAM2, I think this is a game-changer in the object tracking domain, demonstrating long-term object tracking. In this post, we’re going to explore the key components of SAM and also dive into SAM2.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/2025-01-29_sam/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.54405a410796490bc874ab6181fac9b675753cc2b91375d8f882566459eca428.css" integrity="sha256-VEBaQQeWSQvIdKthgfrJtnV1PMK5E3XY&#43;IJWZFnspCg=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/2025-01-29_sam/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Baam&#39;s Techlog (Alt + H)">Baam&#39;s Techlog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://example.org" title="Search">
                    <span>Search</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/cv/" title="CV">
                    <span>CV</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Segment Anything, the first large-scale foundation model for segmentation
    </h1>
    <div class="post-meta"><span title='2025-01-29 13:49:47 -0500 EST'>January 29, 2025</span>

</div>
  </header> 
  <div class="post-content"><p><img loading="lazy" src="/images/2025-01-29_SAM/intro.png" alt="SAM"  />

<a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kirillov_Segment_Anything_ICCV_2023_paper.html">Segment Anything (SAM)</a> has drawn massive attention in the computer vision community, accumulating an impressive 8,000 citations. Segmentation has long been a crucial yet challenging aspect of computer vision. One of the biggest hurdles? Annotation. Unlike simple bounding boxes, which only require marking the object’s general location, segmentation demands precise pixel-level annotations—an incredibly tedious and time-consuming task for annotators. SAM is one of the first large-scale foundation models for segmentation. What makes SAM truly great is that it’s a promptable model. This means you can use it for various tasks without the need for fine-tuning—also known as zero-shot learning. Unlike LLM, here the prompts for the SAM are points, bounding boxes, and masks. In October 2022, Meta&rsquo;s research team released <a href="https://arxiv.org/pdf/2408.00714">SAM2</a>, extending its capabilities from image segmentation to real-time video segmentation. Given my experience with SAM2, I think this is a game-changer in the object tracking domain, demonstrating long-term object tracking. In this post, we’re going to explore the key components of SAM and also dive into SAM2.</p>
<p>In this post, we’re going to explore the key components of SAM and also dive into SAM2. Whether you&rsquo;re new to these concepts or looking to deepen your understanding, this guide will break things down in a simple and easy-to-follow way. Let’s get started! 🚀.</p>
<h1 id="1-sam-architecture-overview">1. SAM Architecture Overview<a hidden class="anchor" aria-hidden="true" href="#1-sam-architecture-overview">#</a></h1>
<p><img loading="lazy" src="/images/2025-01-29_SAM/SAM_architecture.png" alt="SAM"  />

The SAM (Segment Anything Model) architecture consists of three main components: an image encoder, a prompt encoder, and a mask decoder. The image encoder processes the input image to generate an embedding, while the prompt encoder takes user-provided prompts (such as points, boxes, or text) to refine the segmentation. The mask decoder then combines these embeddings and prompts to produce multiple valid segmentation masks, each with an associated confidence score.</p>
<h2 id="11-image-encoder">1.1. Image Encoder<a hidden class="anchor" aria-hidden="true" href="#11-image-encoder">#</a></h2>
<p><img loading="lazy" src="/images/2025-01-29_SAM/MAE.png" alt="MAE"  />

The image encoder of SAM (Segment Anything Model) is quite straightforward. The authors pre-trained the Vision Transformer (ViT) using Masked Autoencoder (MAE)—both of which are widely recognized techniques in the computer vision community.</p>
<p>ViT is one of the pioneering large-scale foundation models for image classification. Meanwhile, MAE is well known for its effectiveness in pre-training models. The idea behind MAE is simple yet powerful: it randomly applies zero-masking to some image patches before passing them through the encoder. The decoder then attempts to reconstruct the masked patches, forcing the model to develop a deeper understanding of image structures.</p>
<h2 id="12-prompt-encoder">1.2. Prompt Encoder<a hidden class="anchor" aria-hidden="true" href="#12-prompt-encoder">#</a></h2>
<p>The prompt encoder takes three types inputs: points, bounding boxes, text, and mask. Points, bounding boxes, and text are treated sparse prompt. The mask is treated as dense prompt. However, the authors said that text prompts are just an exploration, so we won&rsquo;t cover them in this article. The encoder econverts sparse prompts into an embedding representation and Dense prompts (masks) are embedded using convolutions and are element-wise summed with the image embedding.</p>
<h3 id="121-point-embedding">1.2.1. Point Embedding<a hidden class="anchor" aria-hidden="true" href="#121-point-embedding">#</a></h3>
<p>The points and boxes are encoded as &ldquo;positional encoding&rdquo;. The concept of positional encoding was originated from transformer. Transformers use self-attention to process inputs, but unlike RNNs or CNNs, they do not inherently capture positional information (i.e. permutation-invariant). Positional encoding is added to the input embeddings to provide a sense of order in the sequence. The image below shows how positional embedding and image embedding are added in transformer.</p>
<p><img loading="lazy" src="/images/2025-01-29_SAM/embeddings.png" alt="embeddings"  />
</p>
<p>What would be the dimension of positional encoding? It&rsquo;s \(64 \times 64 \times 256\), which matches the dimension of the image embedding. As the SAM architecture shows, the image content embedding and the positional encoding are added element-wise to produce the final image embedding. Since element-wise addition requires the same dimensions, the positional encoding and the image content embedding must correspond in size.</p>
<p>But how do we encode pixel coordinates into a positional embedding whose dimension is \(64 \times 64 \times 256\)? By the way this is why it&rsquo;s called sparse prompt. we are attempting to encode small information into a relatively large feature space. Let&rsquo;s see the part of the source code below.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PromptEncoder</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(
</span></span><span style="display:flex;"><span>      <span style="color:#75715e">#some arguements...</span>
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>embed_dim <span style="color:#f92672">=</span> embed_dim
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>input_image_size <span style="color:#f92672">=</span> input_image_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>image_embedding_size <span style="color:#f92672">=</span> image_embedding_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>pe_layer <span style="color:#f92672">=</span> PositionEmbeddingRandom(embed_dim <span style="color:#f92672">//</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        point_embeddings <span style="color:#f92672">=</span> [nn<span style="color:#f92672">.</span>Embedding(<span style="color:#ae81ff">1</span>, embed_dim) <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>num_point_embeddings)]
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>point_embeddings <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleList(point_embeddings)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_embed_points</span>(
</span></span><span style="display:flex;"><span>        self,
</span></span><span style="display:flex;"><span>        points: torch<span style="color:#f92672">.</span>Tensor,
</span></span><span style="display:flex;"><span>        labels: torch<span style="color:#f92672">.</span>Tensor,
</span></span><span style="display:flex;"><span>        pad: bool,
</span></span><span style="display:flex;"><span>    ) <span style="color:#f92672">-&gt;</span> torch<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Embeds point prompts.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        points <span style="color:#f92672">=</span> points <span style="color:#f92672">+</span> <span style="color:#ae81ff">0.5</span>  <span style="color:#75715e"># Shift to center of pixel</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> pad:
</span></span><span style="display:flex;"><span>            padding_point <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros((points<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>), device<span style="color:#f92672">=</span>points<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>            padding_label <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>torch<span style="color:#f92672">.</span>ones((labels<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#ae81ff">1</span>), device<span style="color:#f92672">=</span>labels<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>            points <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([points, padding_point], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>            labels <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([labels, padding_label], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        point_embedding <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>pe_layer<span style="color:#f92672">.</span>forward_with_coords(points, self<span style="color:#f92672">.</span>input_image_size)
</span></span><span style="display:flex;"><span>        point_embedding[labels <span style="color:#f92672">==</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>        point_embedding[labels <span style="color:#f92672">==</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>not_a_point_embed<span style="color:#f92672">.</span>weight
</span></span><span style="display:flex;"><span>        point_embedding[labels <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>] <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>point_embeddings[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>weight
</span></span><span style="display:flex;"><span>        point_embedding[labels <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>] <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>point_embeddings[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>weight
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> point_embedding
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(
</span></span><span style="display:flex;"><span>        self,
</span></span><span style="display:flex;"><span>        points: Optional[Tuple[torch<span style="color:#f92672">.</span>Tensor, torch<span style="color:#f92672">.</span>Tensor]],
</span></span><span style="display:flex;"><span>        boxes: Optional[torch<span style="color:#f92672">.</span>Tensor],
</span></span><span style="display:flex;"><span>        masks: Optional[torch<span style="color:#f92672">.</span>Tensor],
</span></span><span style="display:flex;"><span>    ) <span style="color:#f92672">-&gt;</span> Tuple[torch<span style="color:#f92672">.</span>Tensor, torch<span style="color:#f92672">.</span>Tensor]:
</span></span><span style="display:flex;"><span>        bs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_get_batch_size(points, boxes, masks)
</span></span><span style="display:flex;"><span>        sparse_embeddings <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>empty((bs, <span style="color:#ae81ff">0</span>, self<span style="color:#f92672">.</span>embed_dim), device<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>_get_device())
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> points <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            coords, labels <span style="color:#f92672">=</span> points
</span></span><span style="display:flex;"><span>            point_embeddings <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_embed_points(coords, labels, pad<span style="color:#f92672">=</span>(boxes <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span>))
</span></span><span style="display:flex;"><span>            sparse_embeddings <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([sparse_embeddings, point_embeddings], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div><p>Okay, let&rsquo;s go into detail on how points and boxes are encoded.
<code>self.pe_layer</code>, which is an object of <code>PositionEmbeddingRandom</code>, maps coordinates into a higher-dimensional space. The member function forward_with_coords performs normalization, linear projection, and sinusoidal (periodic) transformation.</p>
\[
\text{PE}(x, y) = \sin\left( 2\pi \cdot W \cdot \begin{bmatrix} 2x - 1 \\ 2y - 1 \end{bmatrix} \right) \oplus \cos\left( 2\pi \cdot W \cdot \begin{bmatrix} 2x - 1 \\ 2y - 1 \end{bmatrix} \right)
\]<p>where:</p>
<ul>
<li>\(x,y \in \mathbb{R}^{B \times N \times 2}\) is input coordinates (batch of
𝑁 points per image).</li>
<li>\(W \in \mathbb{R}^{2 \times d}\) is the Gaussian projection matrix that maps 2D coordinates to a higher-dimensional space.</li>
<li>\(\oplus\) refers to concatenation along the feature dimension.</li>
<li>\(\text{PE}(x,y)\) is the final positional encoding after applying sinusoidal functions and concatenation.</li>
</ul>
<p>Before we analyze what happens after executing <code>point_embedding = self.pe_layer.forward_with_coords(points, self.input_image_size)</code> is excuted, let&rsquo;s first discuss positive (foreground) points and negative (background) points. Actually, SAM has a feature that I haven&rsquo;t mentioned yet—you can provide background points. A background point is a point that you are not interested in and explicitly mark as not part of the object. See the image below, or you can test this in the <a href="https://segment-anything.com/demo#">demo</a>.
<img loading="lazy" src="/images/2025-01-29_SAM/foreground_background.png" alt="foreground_background"  />
</p>
<p>In the image, the blue dot represents a positive point, while the red dot represents a negative point. So now you know that SAM can take negative points. It makes sense that we update both negative and positive points, but why do we care about missing points, <code>point_embedding[labels == -1]</code>? Let&rsquo;s say \(N=10\), but you only provide 5 points (including both positive and negative points). Behind the scenes, you are actually providing 5 real points plus 5 dummy (padding) points, so the total always adds up to 10. In other words, \(N\) must be the same across the batch. Since you are working with tensors—and a tensor is essentially a matrix—it must have a consistent shape across all dimensions.</p>
<h3 id="122-box-encoding">1.2.2. Box Encoding<a hidden class="anchor" aria-hidden="true" href="#122-box-encoding">#</a></h3>
<p>The bounding box is defined by four coordinates \((x_1, y_1, x_2,y_2)\), representing the top-left and bottom-right corners.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PromptEncoder</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(
</span></span><span style="display:flex;"><span>      <span style="color:#75715e">#some arguements...</span>
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_embed_boxes</span>(self, boxes: torch<span style="color:#f92672">.</span>Tensor) <span style="color:#f92672">-&gt;</span> torch<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Embeds box prompts.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        boxes <span style="color:#f92672">=</span> boxes <span style="color:#f92672">+</span> <span style="color:#ae81ff">0.5</span>  <span style="color:#75715e"># Shift to center of pixel</span>
</span></span><span style="display:flex;"><span>        coords <span style="color:#f92672">=</span> boxes<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        corner_embedding <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>pe_layer<span style="color:#f92672">.</span>forward_with_coords(coords, self<span style="color:#f92672">.</span>input_image_size)
</span></span><span style="display:flex;"><span>        corner_embedding[:, <span style="color:#ae81ff">0</span>, :] <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>point_embeddings[<span style="color:#ae81ff">2</span>]<span style="color:#f92672">.</span>weight
</span></span><span style="display:flex;"><span>        corner_embedding[:, <span style="color:#ae81ff">1</span>, :] <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>point_embeddings[<span style="color:#ae81ff">3</span>]<span style="color:#f92672">.</span>weight
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> corner_embedding
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(
</span></span><span style="display:flex;"><span>        self,
</span></span><span style="display:flex;"><span>        points: Optional[Tuple[torch<span style="color:#f92672">.</span>Tensor, torch<span style="color:#f92672">.</span>Tensor]],
</span></span><span style="display:flex;"><span>        boxes: Optional[torch<span style="color:#f92672">.</span>Tensor],
</span></span><span style="display:flex;"><span>        masks: Optional[torch<span style="color:#f92672">.</span>Tensor],
</span></span><span style="display:flex;"><span>    ) <span style="color:#f92672">-&gt;</span> Tuple[torch<span style="color:#f92672">.</span>Tensor, torch<span style="color:#f92672">.</span>Tensor]:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> boxes <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            box_embeddings <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_embed_boxes(boxes)
</span></span><span style="display:flex;"><span>            sparse_embeddings <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([sparse_embeddings, box_embeddings], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div><p>Similar to points, these coordinates are mapped to positional encodings. However, unlike <code>point_embedding</code>, which consists of <code>N</code> points, <code>corner_embedding</code> represents only one bounding box per image. You can see this from the line:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>coords <span style="color:#f92672">=</span> boxes<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>)
</span></span></code></pre></div><p>which reshapes the input into \(B \times 2\). Here, the last two dimensions represent the (x, y) coordinates of the two corners (top-left and bottom-right) of the bounding box.</p>
<p>Before we dive in, let me step back. When I first read the paper, I wondered how the model could take three different types of inputs interchangeably. Normally, if model requires \(n\) types of inputs, you have to provide \(n\) inputs. But with SAM, you can simply provide a point, a box, or a mask—just one is enough.</p>
<h2 id="training-algorithm-for-sam">Training Algorithm for SAM<a hidden class="anchor" aria-hidden="true" href="#training-algorithm-for-sam">#</a></h2>
<p>Interactive segmentation reuqires point or bounidng box input as prmopts. The prompts are sampled from the ground truth</p>
<p>Are we training the model or making the dataset</p>
<h2 id="image-encoder">Image Encoder<a hidden class="anchor" aria-hidden="true" href="#image-encoder">#</a></h2>
<h2 id="prompt-encoder">Prompt Encoder<a hidden class="anchor" aria-hidden="true" href="#prompt-encoder">#</a></h2>
<h2 id="mask-decoder">Mask decoder<a hidden class="anchor" aria-hidden="true" href="#mask-decoder">#</a></h2>
<h2 id="reference">Reference<a hidden class="anchor" aria-hidden="true" href="#reference">#</a></h2>
<ul>
<li><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kirillov_Segment_Anything_ICCV_2023_paper.html">Segment Anything</a></li>
<li><a href="https://arxiv.org/pdf/2305.08196">A Comprehensive Survey on Segment Anything Model for Vision and Beyond</a></li>
<li><a href="https://www.youtube.com/watch?v=OhxJkqD1vuE&amp;t=280s">Explaining the Segment Anything Model - Network architecture, Dataset, Training</a></li>
<li><a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/ipr2.12419">Medical image segmentation using deep learning: A survey</a></li>
<li><a href="https://towardsdatascience.com/how-does-the-segment-anything-models-sam-s-encoder-work-003a8a6e3f8b">How Does the Segment-Anything Model’s (SAM’s) Encoder Work?</a></li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/segmentation/">Segmentation</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">Baam&#39;s Techlog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
