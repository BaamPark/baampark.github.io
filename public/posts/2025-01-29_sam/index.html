<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Segment Anything, the first large-scale foundation model for segmentation | Baam&#39;s Techlog</title>
<meta name="keywords" content="Segmentation">
<meta name="description" content="

Segment Anything (SAM) has drawn massive attention in the computer vision community, accumulating an impressive 8,000 citations. Segmentation has long been a crucial yet challenging aspect of computer vision. One of the biggest hurdles? Annotation. Unlike simple bounding boxes, which only require marking the object’s general location, segmentation demands precise pixel-level annotations—an incredibly tedious and time-consuming task for annotators. SAM is one of the first large-scale foundation models for segmentation. What makes SAM truly great is that it’s a promptable model. This means you can use it for various tasks without the need for fine-tuning—also known as zero-shot learning. Unlike LLM, here the prompts for the SAM are points, bounding boxes, and masks. In this post, we’re going to explore the key components of SAM. This guide will break things down in a simple and easy-to-follow way. Let’s get started! 🚀.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/2025-01-29_sam/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.54405a410796490bc874ab6181fac9b675753cc2b91375d8f882566459eca428.css" integrity="sha256-VEBaQQeWSQvIdKthgfrJtnV1PMK5E3XY&#43;IJWZFnspCg=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/2025-01-29_sam/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Baam&#39;s Techlog (Alt + H)">Baam&#39;s Techlog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://example.org" title="Search">
                    <span>Search</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/cv/" title="CV">
                    <span>CV</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Segment Anything, the first large-scale foundation model for segmentation
    </h1>
    <div class="post-meta"><span title='2025-01-29 13:49:47 -0500 EST'>January 29, 2025</span>

</div>
  </header> 
  <div class="post-content"><p><img loading="lazy" src="/images/2025-01-29_SAM/intro.png" alt="SAM"  />

<a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kirillov_Segment_Anything_ICCV_2023_paper.html">Segment Anything (SAM)</a> has drawn massive attention in the computer vision community, accumulating an impressive 8,000 citations. Segmentation has long been a crucial yet challenging aspect of computer vision. One of the biggest hurdles? Annotation. Unlike simple bounding boxes, which only require marking the object’s general location, segmentation demands precise pixel-level annotations—an incredibly tedious and time-consuming task for annotators. SAM is one of the first large-scale foundation models for segmentation. What makes SAM truly great is that it’s a promptable model. This means you can use it for various tasks without the need for fine-tuning—also known as zero-shot learning. Unlike LLM, here the prompts for the SAM are points, bounding boxes, and masks. In this post, we’re going to explore the key components of SAM. This guide will break things down in a simple and easy-to-follow way. Let’s get started! 🚀.</p>
<h1 id="1-sam-architecture-overview">1. SAM Architecture Overview<a hidden class="anchor" aria-hidden="true" href="#1-sam-architecture-overview">#</a></h1>
<p><img loading="lazy" src="/images/2025-01-29_SAM/SAM_architecture.png" alt="SAM"  />

The SAM (Segment Anything Model) architecture consists of three main components: an image encoder, a prompt encoder, and a mask decoder. The image encoder processes the input image to generate an embedding, while the prompt encoder takes user-provided prompts (such as points, boxes, or text) to refine the segmentation. The mask decoder then combines these embeddings and prompts to produce multiple valid segmentation masks, each with an associated confidence score.</p>
<h2 id="2-image-encoder">2. Image Encoder<a hidden class="anchor" aria-hidden="true" href="#2-image-encoder">#</a></h2>
<p><img loading="lazy" src="/images/2025-01-29_SAM/MAE.png" alt="MAE"  />

The image encoder of SAM (Segment Anything Model) is quite straightforward. The authors pre-trained the Vision Transformer (ViT) using Masked Autoencoder (MAE)—both of which are widely recognized techniques in the computer vision community.</p>
<p>ViT is one of the pioneering large-scale foundation models for image classification. Meanwhile, MAE is well known for its effectiveness in pre-training models. The idea behind MAE is simple yet powerful: it randomly applies zero-masking to some image patches before passing them through the encoder. The decoder then attempts to reconstruct the masked patches, forcing the model to develop a deeper understanding of image structures. Essentially, the image is embedded into feature space \(\mathbb{R}^{256 \times 64 \times 64}\).</p>
<h2 id="3-prompt-encoder">3. Prompt Encoder<a hidden class="anchor" aria-hidden="true" href="#3-prompt-encoder">#</a></h2>
<p>The prompt encoder takes three types inputs: points, bounding boxes, text, and mask. Points, bounding boxes, and text are treated sparse prompt. The mask is treated as dense prompt. However, the authors said that text prompts are just an exploration, so we won&rsquo;t cover them in this article. The prompt encoder has two jobs mainly: <strong>sparse prompt embedding</strong> and <strong>dense prompt embedding</strong>. However, if you see the <code>PromptEncoder</code> implementation, you will notice there is one more thing it returns, which is <strong>image positional embedding</strong>. We will learn how these three embeddings are processed.</p>
<h2 id="31-image-positional-embedding">3.1. Image Positional Embedding<a hidden class="anchor" aria-hidden="true" href="#31-image-positional-embedding">#</a></h2>
<blockquote>
<p>To ensure the decoder has access to critical geometric information the positional encodings are added to the image embedding whenever they participate in an attention layer. - <em>SAM, Segment Anything Model and Task Details, Lightweight mask decoder</em></p></blockquote>
<p>The authors said positional encodings are added to the image embedding. It encodes positional information for the entire image feature grid. The concept of positional encoding was originated from transformer. Transformers use self-attention to process inputs, but unlike RNNs or CNNs, they do not inherently capture positional information (i.e. permutation-invariant). Positional encoding is added to the input embeddings to provide a sense of order in the sequence. The image below shows how positional embedding and image embedding are added in transformer.</p>
<p><img loading="lazy" src="/images/2025-01-29_SAM/embeddings.png" alt="embeddings"  />
</p>
<p>What would be the dimension of positional encoding? It&rsquo;s \(256 \times 64 \times 64\), which matches the dimension of the image embedding. This is because the image embedding and image positional embedding are added together element-wise. Let&rsquo;s review a part of the <code>PromptEncoder</code> implementation.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PromptEncoder</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__():
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#skip...</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>pe_layer <span style="color:#f92672">=</span> PositionEmbeddingRandom(embed_dim <span style="color:#f92672">//</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_dense_pe</span>(self) <span style="color:#f92672">-&gt;</span> torch<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>pe_layer(self<span style="color:#f92672">.</span>image_embedding_size)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PositionEmbeddingRandom</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, num_pos_feats: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">64</span>, scale: Optional[float] <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> scale <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span> <span style="color:#f92672">or</span> scale <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">0.0</span>:
</span></span><span style="display:flex;"><span>            scale <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>register_buffer(
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;positional_encoding_gaussian_matrix&#34;</span>,
</span></span><span style="display:flex;"><span>            scale <span style="color:#f92672">*</span> torch<span style="color:#f92672">.</span>randn((<span style="color:#ae81ff">2</span>, num_pos_feats)),
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_pe_encoding</span>(self, coords: torch<span style="color:#f92672">.</span>Tensor) <span style="color:#f92672">-&gt;</span> torch<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>        coords <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> coords <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>        coords <span style="color:#f92672">=</span> coords <span style="color:#f92672">@</span> self<span style="color:#f92672">.</span>positional_encoding_gaussian_matrix
</span></span><span style="display:flex;"><span>        coords <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>pi <span style="color:#f92672">*</span> coords
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># outputs d_1 x ... x d_n x C shape</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> torch<span style="color:#f92672">.</span>cat([torch<span style="color:#f92672">.</span>sin(coords), torch<span style="color:#f92672">.</span>cos(coords)], dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, size: Tuple[int, int]) <span style="color:#f92672">-&gt;</span> torch<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>        h, w <span style="color:#f92672">=</span> size
</span></span><span style="display:flex;"><span>        device: Any <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>positional_encoding_gaussian_matrix<span style="color:#f92672">.</span>device
</span></span><span style="display:flex;"><span>        grid <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>ones((h, w), device<span style="color:#f92672">=</span>device, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32)
</span></span><span style="display:flex;"><span>        y_embed <span style="color:#f92672">=</span> grid<span style="color:#f92672">.</span>cumsum(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>) <span style="color:#f92672">-</span> <span style="color:#ae81ff">0.5</span>
</span></span><span style="display:flex;"><span>        x_embed <span style="color:#f92672">=</span> grid<span style="color:#f92672">.</span>cumsum(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>) <span style="color:#f92672">-</span> <span style="color:#ae81ff">0.5</span>
</span></span><span style="display:flex;"><span>        y_embed <span style="color:#f92672">=</span> y_embed <span style="color:#f92672">/</span> h
</span></span><span style="display:flex;"><span>        x_embed <span style="color:#f92672">=</span> x_embed <span style="color:#f92672">/</span> w
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        pe <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_pe_encoding(torch<span style="color:#f92672">.</span>stack([x_embed, y_embed], dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> pe<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># C x H x W</span>
</span></span></code></pre></div><p>In essence, <code>get_dense_pe</code> function returns image positional embedding, which later will be passed to the mask decoder. In <code>forward</code> function, it shows how image positional embeddings are constructed for the entire 64 x 64 feature grid into four steps</p>
<ol>
<li>Creates a coordinate grid for the feature map.</li>
<li>Normalizes x and y coordinates to \([0,1]\).</li>
<li>Centers the ccoordinates such that \([-0.5, 0.5]\)</li>
<li>Apply positional encoding</li>
</ol>
<p>When creating the grid, it first initializes 2d tensor filled with ones. For x and y axis, <code>cumsum</code> computes cumulative sum along each axis. For example, if <code>h=3, w = 3</code>, then cumsum does:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x_embed <span style="color:#f92672">=</span> grid<span style="color:#f92672">.</span>cumsum(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>) <span style="color:#f92672">-</span> <span style="color:#ae81ff">0.5</span>
</span></span></code></pre></div>\[
\begin{bmatrix}
1 & 2 & 3 \\
1 & 2 & 3 \\
1 & 2 & 3
\end{bmatrix}
\]<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>y_embed <span style="color:#f92672">=</span> grid<span style="color:#f92672">.</span>cumsum(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>) <span style="color:#f92672">-</span> <span style="color:#ae81ff">0.5</span>
</span></span></code></pre></div>\[
\begin{bmatrix}
1 & 1 & 1 \\
2 & 2 & 2 \\
3 & 3 & 3
\end{bmatrix}
\]<p>After normalization and certering the coordinates, it performs positional encoding. Mathmatically, postional encdoing is given by:</p>
\[
    \text{PE}(x,y) = \sin(2\pi W \begin{bmatrix} x \\ y \end{bmatrix}) \oplus \cos(2\pi W \begin{bmatrix} x \\ y \end{bmatrix})
\]<p>
where:</p>
<ul>
<li>\(\begin{bmatrix} x \\ y \end{bmatrix} \in \mathbb{R}^{B \times H \times W \times 2}\) is a stacked grid feature.</li>
<li>\(W \in \mathbb{R}^{2 \times d}\) is the Gaussian projection matrix that maps 2D coordinates to a higher-dimensional space.</li>
<li>\(\oplus\) refers to concatenation along the feature dimension.</li>
<li>\(\text{PE}(x,y) \in \mathbb{R}^{B \times H \times W \times 2d}\) is the final positional encoding</li>
</ul>
<p>Now that we understand how image positional embeddings are computed. These embeddings, along with sparse and dense prompts, will be passed to the mask decoder to guide segmentation.</p>
<h3 id="32-point-embedding">3.2. Point Embedding<a hidden class="anchor" aria-hidden="true" href="#32-point-embedding">#</a></h3>
<p>We can pass \(N\) number of points per image to SAM. Each point acts as a spatial cue, helping the model focus on specific regions of interest within the image. These points are then transformed into high-dimensional-sparse embeddings.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PromptEncoder</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(
</span></span><span style="display:flex;"><span>      <span style="color:#75715e">#some arguements...</span>
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>embed_dim <span style="color:#f92672">=</span> embed_dim
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>input_image_size <span style="color:#f92672">=</span> input_image_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>image_embedding_size <span style="color:#f92672">=</span> image_embedding_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>pe_layer <span style="color:#f92672">=</span> PositionEmbeddingRandom(embed_dim <span style="color:#f92672">//</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        point_embeddings <span style="color:#f92672">=</span> [nn<span style="color:#f92672">.</span>Embedding(<span style="color:#ae81ff">1</span>, embed_dim) <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>num_point_embeddings)]
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>point_embeddings <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleList(point_embeddings)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_embed_points</span>(
</span></span><span style="display:flex;"><span>        self,
</span></span><span style="display:flex;"><span>        points: torch<span style="color:#f92672">.</span>Tensor,
</span></span><span style="display:flex;"><span>        labels: torch<span style="color:#f92672">.</span>Tensor,
</span></span><span style="display:flex;"><span>        pad: bool,
</span></span><span style="display:flex;"><span>    ) <span style="color:#f92672">-&gt;</span> torch<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Embeds point prompts.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        points <span style="color:#f92672">=</span> points <span style="color:#f92672">+</span> <span style="color:#ae81ff">0.5</span>  <span style="color:#75715e"># Shift to center of pixel</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> pad:
</span></span><span style="display:flex;"><span>            padding_point <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros((points<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>), device<span style="color:#f92672">=</span>points<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>            padding_label <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>torch<span style="color:#f92672">.</span>ones((labels<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#ae81ff">1</span>), device<span style="color:#f92672">=</span>labels<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>            points <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([points, padding_point], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>            labels <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([labels, padding_label], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        point_embedding <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>pe_layer<span style="color:#f92672">.</span>forward_with_coords(points, self<span style="color:#f92672">.</span>input_image_size)
</span></span><span style="display:flex;"><span>        point_embedding[labels <span style="color:#f92672">==</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>        point_embedding[labels <span style="color:#f92672">==</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>not_a_point_embed<span style="color:#f92672">.</span>weight
</span></span><span style="display:flex;"><span>        point_embedding[labels <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>] <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>point_embeddings[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>weight
</span></span><span style="display:flex;"><span>        point_embedding[labels <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>] <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>point_embeddings[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>weight
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> point_embedding
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(
</span></span><span style="display:flex;"><span>        self,
</span></span><span style="display:flex;"><span>        points: Optional[Tuple[torch<span style="color:#f92672">.</span>Tensor, torch<span style="color:#f92672">.</span>Tensor]],
</span></span><span style="display:flex;"><span>        boxes: Optional[torch<span style="color:#f92672">.</span>Tensor],
</span></span><span style="display:flex;"><span>        masks: Optional[torch<span style="color:#f92672">.</span>Tensor],
</span></span><span style="display:flex;"><span>    ) <span style="color:#f92672">-&gt;</span> Tuple[torch<span style="color:#f92672">.</span>Tensor, torch<span style="color:#f92672">.</span>Tensor]:
</span></span><span style="display:flex;"><span>        bs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_get_batch_size(points, boxes, masks)
</span></span><span style="display:flex;"><span>        sparse_embeddings <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>empty((bs, <span style="color:#ae81ff">0</span>, self<span style="color:#f92672">.</span>embed_dim), device<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>_get_device()) <span style="color:#75715e">#place holder</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> points <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            coords, labels <span style="color:#f92672">=</span> points
</span></span><span style="display:flex;"><span>            point_embeddings <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_embed_points(coords, labels, pad<span style="color:#f92672">=</span>(boxes <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span>))
</span></span><span style="display:flex;"><span>            sparse_embeddings <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([sparse_embeddings, point_embeddings], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div><p>Okay, let&rsquo;s go into detail on how points and boxes are encoded.
<code>self.pe_layer</code>, which is an object of <code>PositionEmbeddingRandom</code>, maps coordinates into a higher-dimensional space. The member function <code>forward_with_coords</code> performs normalization, linear projection, and sinusoidal transformation, same as we did for image positional embedding.</p>
\[
\text{PE}(x, y) = \sin\left( 2\pi \cdot W \cdot \begin{bmatrix} 2x - 1 \\ 2y - 1 \end{bmatrix} \right) \oplus \cos\left( 2\pi \cdot W \cdot \begin{bmatrix} 2x - 1 \\ 2y - 1 \end{bmatrix} \right)
\]<p>where:</p>
<ul>
<li>\(x,y \in \mathbb{R}^{B \times N \times 2}\) is input coordinates (batch of
𝑁 points per image).</li>
<li>\(W \in \mathbb{R}^{2 \times d}\)</li>
<li>\(\text{PE}(x,y) \in \mathbb{R} ^{B \times N \times 2d}\)</li>
</ul>
<p>Before we analyze what happens after executing <code>point_embedding = self.pe_layer.forward_with_coords(points, self.input_image_size)</code> is excuted, let&rsquo;s first discuss positive (foreground) points and negative (background) points. Actually, SAM has a feature that I haven&rsquo;t mentioned yet—you can provide background points. A background point is a point that you are not interested in and explicitly mark as not part of the object. See the image below, or you can test this in the <a href="https://segment-anything.com/demo#">demo</a>.
<img loading="lazy" src="/images/2025-01-29_SAM/foreground_background.png" alt="foreground_background"  />
</p>
<p>In the image, the blue dot represents a positive point, while the red dot represents a negative point. But wait! why do we need labels for the forward pass? In this context, labels are not ground truth segmentation masks. Instead, they indicate whether each click is a positive (foreground) or negative (background) point when passing inputs to the prompt encoder. So, the label is an array of size \(N\), where each entry is either 1 (positive) or 0 (negative).</p>
<p>As <code>sparse_embeddings</code> is an empty tensor that has zero dimension in sequnence dimension, the concatenation with <code>point_embeddings</code> doesn&rsquo;t affect the shape of tensor.</p>
<h3 id="33-box-encoding">3.3. Box Encoding<a hidden class="anchor" aria-hidden="true" href="#33-box-encoding">#</a></h3>
<p>The bounding box is defined by four coordinates \((x_1, y_1, x_2,y_2)\), representing the top-left and bottom-right corners.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PromptEncoder</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(
</span></span><span style="display:flex;"><span>      <span style="color:#75715e">#some arguements...</span>
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_embed_points</span>(self, points, labels, pad):
</span></span><span style="display:flex;"><span>         <span style="color:#75715e">#skip...</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> point_embedding
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_embed_boxes</span>(self, boxes: torch<span style="color:#f92672">.</span>Tensor) <span style="color:#f92672">-&gt;</span> torch<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Embeds box prompts.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        boxes <span style="color:#f92672">=</span> boxes <span style="color:#f92672">+</span> <span style="color:#ae81ff">0.5</span>  <span style="color:#75715e"># Shift to center of pixel</span>
</span></span><span style="display:flex;"><span>        coords <span style="color:#f92672">=</span> boxes<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        corner_embedding <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>pe_layer<span style="color:#f92672">.</span>forward_with_coords(coords, self<span style="color:#f92672">.</span>input_image_size)
</span></span><span style="display:flex;"><span>        corner_embedding[:, <span style="color:#ae81ff">0</span>, :] <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>point_embeddings[<span style="color:#ae81ff">2</span>]<span style="color:#f92672">.</span>weight
</span></span><span style="display:flex;"><span>        corner_embedding[:, <span style="color:#ae81ff">1</span>, :] <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>point_embeddings[<span style="color:#ae81ff">3</span>]<span style="color:#f92672">.</span>weight
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> corner_embedding
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(
</span></span><span style="display:flex;"><span>        self,
</span></span><span style="display:flex;"><span>        points: Optional[Tuple[torch<span style="color:#f92672">.</span>Tensor, torch<span style="color:#f92672">.</span>Tensor]],
</span></span><span style="display:flex;"><span>        boxes: Optional[torch<span style="color:#f92672">.</span>Tensor],
</span></span><span style="display:flex;"><span>        masks: Optional[torch<span style="color:#f92672">.</span>Tensor],
</span></span><span style="display:flex;"><span>    ) <span style="color:#f92672">-&gt;</span> Tuple[torch<span style="color:#f92672">.</span>Tensor, torch<span style="color:#f92672">.</span>Tensor]:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        bs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_get_batch_size(points, boxes, masks)
</span></span><span style="display:flex;"><span>        sparse_embeddings <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>empty((bs, <span style="color:#ae81ff">0</span>, self<span style="color:#f92672">.</span>embed_dim), device<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>_get_device())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> points <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            coords, labels <span style="color:#f92672">=</span> points
</span></span><span style="display:flex;"><span>            point_embeddings <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_embed_points(coords, labels, pad<span style="color:#f92672">=</span>(boxes <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span>))
</span></span><span style="display:flex;"><span>            sparse_embeddings <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([sparse_embeddings, point_embeddings], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> boxes <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            box_embeddings <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_embed_boxes(boxes)
</span></span><span style="display:flex;"><span>            sparse_embeddings <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([sparse_embeddings, box_embeddings], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div><p>Similar to points, these coordinates are mapped to positional encodings using sinusoidal transformation. However, unlike <code>point_embedding</code>, which consists of <code>N</code> points, <code>corner_embedding</code> represents only one bounding box per image. You can see this from the line, <code>coords = boxes.reshape(-1, 2, 2)</code>, which reshapes the input into \(B \times 2\). Here, the last two dimensions represent the (x, y) coordinates of the two corners (top-left and bottom-right) of the bounding box.
After mapping the box to positional embedding, we add learnable parameters to the top-left and bottom-right corners.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>        corner_embedding[:, <span style="color:#ae81ff">0</span>, :] <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>point_embeddings[<span style="color:#ae81ff">2</span>]<span style="color:#f92672">.</span>weight
</span></span><span style="display:flex;"><span>        corner_embedding[:, <span style="color:#ae81ff">1</span>, :] <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>point_embeddings[<span style="color:#ae81ff">3</span>]<span style="color:#f92672">.</span>weight
</span></span></code></pre></div><p>Eventually, <code>box_embeddings</code> will have the dimension of \(\mathbb{R}^{B,2,2d}\).</p>
\[
    \text{PE}_{\text{sparse}} \in \mathbb{R}^{B \times N+2 \times C}.
 \]<p>I have a question for you. What will be the dimension of \(\text{PE}_{\text{sparse}}\) when only points prompt is given without bounding box. What about only bounding box is given? We have three input scenarios</p>
<ul>
<li>\(N\) points prompts, No bounding box ➡️ \(\mathbb{R}^{B \times N \times C}\)</li>
<li>No points prompts, one bounding box ➡️ \(\mathbb{R}^{B \times 2 \times C}\)</li>
<li>\(N\) points prompts, one bounding box ➡️ \(\mathbb{R}^{B \times N + 2 \times C}\)</li>
</ul>
<p>Something is odd. How we can forward pass tensor that has different sequence length (middle dimension) for each pass? If you can&rsquo;t answer this question, you can read my <a href="https://baampark.github.io/posts/2025-01-28_variable_sequence/">previous post</a>. In short, there is no <code>nn.Linear</code> (project layer) in prompt encoder so we don&rsquo;t need to care about variable-length sequnces.</p>
<h3 id="34-dense-prompt-encoding">3.4. Dense prompt encoding<a hidden class="anchor" aria-hidden="true" href="#34-dense-prompt-encoding">#</a></h3>
<p>Unlike sparse prompts, which are first mapped to an embedding space using <strong>positional encoding</strong>, dense prompts are directly projected using convolutions and then summed element-wise with the image embedding.The input mask is a binary tensor \( M \) of shape:</p>
\[
M \in \mathbb{R}^{B \times 1 \times 256 \times 256}
\]<p>
where:</p>
<ul>
<li>\( B \) is the batch size.</li>
<li>The <strong>single channel</strong> (1) represents a binary mask (foreground vs. background).</li>
<li>\( 256 \times 256 \) is a fixed spatial resolution for masks in SAM.</li>
</ul>
<p>In the Prompt Encoder, the mask undergoes convolutional transformations to extract meaningful features. This is done in the <code>_embed_masks</code> function:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PromptEncoder</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(
</span></span><span style="display:flex;"><span>        self,
</span></span><span style="display:flex;"><span>        embed_dim: int,
</span></span><span style="display:flex;"><span>        image_embedding_size: Tuple[int, int],
</span></span><span style="display:flex;"><span>        input_image_size: Tuple[int, int],
</span></span><span style="display:flex;"><span>        mask_in_chans: int,
</span></span><span style="display:flex;"><span>        activation: Type[nn<span style="color:#f92672">.</span>Module] <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>GELU,
</span></span><span style="display:flex;"><span>    ) 
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>mask_downscaling <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">1</span>, mask_in_chans <span style="color:#f92672">//</span> <span style="color:#ae81ff">4</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),
</span></span><span style="display:flex;"><span>            LayerNorm2d(mask_in_chans <span style="color:#f92672">//</span> <span style="color:#ae81ff">4</span>),
</span></span><span style="display:flex;"><span>            activation(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(mask_in_chans <span style="color:#f92672">//</span> <span style="color:#ae81ff">4</span>, mask_in_chans, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),
</span></span><span style="display:flex;"><span>            LayerNorm2d(mask_in_chans),
</span></span><span style="display:flex;"><span>            activation(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(mask_in_chans, embed_dim, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_embed_masks</span>(self, masks: torch<span style="color:#f92672">.</span>Tensor) <span style="color:#f92672">-&gt;</span> torch<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Embeds mask input.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        mask_embedding <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>mask_downscaling(masks)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> mask_embedding
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(
</span></span><span style="display:flex;"><span>        self,
</span></span><span style="display:flex;"><span>        points: Optional[Tuple[torch<span style="color:#f92672">.</span>Tensor, torch<span style="color:#f92672">.</span>Tensor]],
</span></span><span style="display:flex;"><span>        boxes: Optional[torch<span style="color:#f92672">.</span>Tensor],
</span></span><span style="display:flex;"><span>        masks: Optional[torch<span style="color:#f92672">.</span>Tensor],
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#skip sparse prompt</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> masks <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            dense_embeddings <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_embed_masks(masks)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            dense_embeddings <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>no_mask_embed<span style="color:#f92672">.</span>weight<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">1</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>expand(
</span></span><span style="display:flex;"><span>                bs, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>image_embedding_size[<span style="color:#ae81ff">0</span>], self<span style="color:#f92672">.</span>image_embedding_size[<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> sparse_embeddings, dense_embeddings
</span></span></code></pre></div><p><code>mask_downscaling</code> is a learnable CNN module that reduces the resolution of the mask while increasing its feature depth. This converts the binary mask into an embedding space that aligns with the image features. The resulting mask embedding has the shape: \(\mathbb{R}^{B \times C \times H' \times W'}\), where \(C=256, H'=64, W'=64\). Now the mask embedding dimension matches the image embedding so that both can be used together in the mask decoder.</p>
<p>However, if no mask is given (masks=None), SAM instead uses a learnable &ldquo;no-mask&rdquo; embedding. <code>self.no_mask_embed.weight</code> is a learnable tensor representing a default mask embedding when a mask is not given. It is reshaped and expanded to match the required shape, \(\mathbb{R}^{B \times C \times H' \times W'}\). This ensures that even when no mask is provided, the model still has a valid dense prompt embedding.</p>
<h2 id="mask-decoder">Mask Decoder<a hidden class="anchor" aria-hidden="true" href="#mask-decoder">#</a></h2>
<h2 id="reference">Reference<a hidden class="anchor" aria-hidden="true" href="#reference">#</a></h2>
<ul>
<li><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kirillov_Segment_Anything_ICCV_2023_paper.html">Segment Anything</a></li>
<li><a href="https://arxiv.org/pdf/2305.08196">A Comprehensive Survey on Segment Anything Model for Vision and Beyond</a></li>
<li><a href="https://www.youtube.com/watch?v=OhxJkqD1vuE&amp;t=280s">Explaining the Segment Anything Model - Network architecture, Dataset, Training</a></li>
<li><a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/ipr2.12419">Medical image segmentation using deep learning: A survey</a></li>
<li><a href="https://towardsdatascience.com/how-does-the-segment-anything-models-sam-s-encoder-work-003a8a6e3f8b">How Does the Segment-Anything Model’s (SAM’s) Encoder Work?</a></li>
<li><a href="https://www.youtube.com/watch?v=n13-r_eStb0">Transformer self-attention padding and causal masking technique</a></li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/segmentation/">Segmentation</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">Baam&#39;s Techlog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
