<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Segment Anything, the first large-scale foundation model for segmentation | Baam&#39;s Techlog</title>
<meta name="keywords" content="Segmentation">
<meta name="description" content="Segment Anything (SAM) has drawn massive attention in the computer vision community, accumulating an impressive 8,000 citations. Segmentation has long been a crucial yet challenging aspect of computer vision. One of the biggest hurdles? Annotation. Unlike simple bounding boxes, which only require marking the object’s general location, segmentation demands precise pixel-level annotations—an incredibly tedious and time-consuming task for annotators. SAM is one of the first large-scale foundation models for segmentation.">
<meta name="author" content="">
<link rel="canonical" href="https://baampark.github.io/posts/2025-01-29_sam/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css" integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://baampark.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://baampark.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://baampark.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://baampark.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://baampark.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://baampark.github.io/posts/2025-01-29_sam/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
<meta property="og:title" content="Segment Anything, the first large-scale foundation model for segmentation" />
<meta property="og:description" content="Segment Anything (SAM) has drawn massive attention in the computer vision community, accumulating an impressive 8,000 citations. Segmentation has long been a crucial yet challenging aspect of computer vision. One of the biggest hurdles? Annotation. Unlike simple bounding boxes, which only require marking the object’s general location, segmentation demands precise pixel-level annotations—an incredibly tedious and time-consuming task for annotators. SAM is one of the first large-scale foundation models for segmentation." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://baampark.github.io/posts/2025-01-29_sam/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-01-29T13:49:47-05:00" />
<meta property="article:modified_time" content="2025-01-29T13:49:47-05:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Segment Anything, the first large-scale foundation model for segmentation"/>
<meta name="twitter:description" content="Segment Anything (SAM) has drawn massive attention in the computer vision community, accumulating an impressive 8,000 citations. Segmentation has long been a crucial yet challenging aspect of computer vision. One of the biggest hurdles? Annotation. Unlike simple bounding boxes, which only require marking the object’s general location, segmentation demands precise pixel-level annotations—an incredibly tedious and time-consuming task for annotators. SAM is one of the first large-scale foundation models for segmentation."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://baampark.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Segment Anything, the first large-scale foundation model for segmentation",
      "item": "https://baampark.github.io/posts/2025-01-29_sam/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Segment Anything, the first large-scale foundation model for segmentation",
  "name": "Segment Anything, the first large-scale foundation model for segmentation",
  "description": "Segment Anything (SAM) has drawn massive attention in the computer vision community, accumulating an impressive 8,000 citations. Segmentation has long been a crucial yet challenging aspect of computer vision. One of the biggest hurdles? Annotation. Unlike simple bounding boxes, which only require marking the object’s general location, segmentation demands precise pixel-level annotations—an incredibly tedious and time-consuming task for annotators. SAM is one of the first large-scale foundation models for segmentation.",
  "keywords": [
    "Segmentation"
  ],
  "articleBody": " Segment Anything (SAM) has drawn massive attention in the computer vision community, accumulating an impressive 8,000 citations. Segmentation has long been a crucial yet challenging aspect of computer vision. One of the biggest hurdles? Annotation. Unlike simple bounding boxes, which only require marking the object’s general location, segmentation demands precise pixel-level annotations—an incredibly tedious and time-consuming task for annotators. SAM is one of the first large-scale foundation models for segmentation. What makes SAM truly great is that it’s a promptable model. This means you can use it for various tasks without the need for fine-tuning—also known as zero-shot learning. Unlike LLM, here the prompts for the SAM are points, bounding boxes, and masks. In this post, we’re going to explore the key components of SAM. This guide will break things down in a simple and easy-to-follow way. Let’s get started! 🚀.\n1. SAM Architecture Overview The SAM (Segment Anything Model) architecture consists of three main components: an image encoder, a prompt encoder, and a mask decoder. The image encoder processes the input image to generate an embedding, while the prompt encoder takes user-provided prompts (such as points, boxes, or text) to refine the segmentation. The mask decoder then combines these embeddings and prompts to produce multiple valid segmentation masks, each with an associated confidence score.\n2. Image Encoder The image encoder of SAM (Segment Anything Model) is quite straightforward. The authors pre-trained the Vision Transformer (ViT) using Masked Autoencoder (MAE)—both of which are widely recognized techniques in the computer vision community.\nViT is one of the pioneering large-scale foundation models for image classification. Meanwhile, MAE is well known for its effectiveness in pre-training models. The idea behind MAE is simple yet powerful: it randomly applies zero-masking to some image patches before passing them through the encoder. The decoder then attempts to reconstruct the masked patches, forcing the model to develop a deeper understanding of image structures. Essentially, the \\(1024 \\times 1024\\) image is embedded into feature space \\(\\mathbb{R}^{256 \\times 64 \\times 64}\\).\n3. Prompt Encoder The prompt encoder takes three types inputs: points, bounding boxes, text, and mask. Points, bounding boxes, and text are treated sparse prompt. The mask is treated as dense prompt. However, the authors said that text prompts are just an exploration, so we won’t cover them in this article. The prompt encoder has two jobs mainly: sparse prompt embedding and dense prompt embedding. However, if you see the PromptEncoder implementation, you will notice there is one more thing it returns, which is image positional embedding. We will learn how these three embeddings are processed.\n3.1. Image Positional Embedding To ensure the decoder has access to critical geometric information the positional encodings are added to the image embedding whenever they participate in an attention layer. - SAM, Segment Anything Model and Task Details, Lightweight mask decoder\nThe authors said positional encodings are added to the image embedding. It encodes positional information for the entire image feature grid. The concept of positional encoding was originated from transformer. Transformers use self-attention to process inputs, but unlike RNNs or CNNs, they do not inherently capture positional information (i.e. permutation-invariant). Positional encoding is added to the input embeddings to provide a sense of order in the sequence. The image below shows how positional embedding and image embedding are added in transformer.\nWhat would be the dimension of positional encoding? It’s \\(256 \\times 64 \\times 64\\), which matches the dimension of the image embedding. This is because the image embedding and image positional embedding are added together element-wise. Let’s review a part of the PromptEncoder implementation.\nclass PromptEncoder(nn.Module): def __init__(): #skip... self.pe_layer = PositionEmbeddingRandom(embed_dim // 2) def get_dense_pe(self) -\u003e torch.Tensor: return self.pe_layer(self.image_embedding_size).unsqueeze(0) class PositionEmbeddingRandom(nn.Module): def __init__(self, num_pos_feats: int = 64, scale: Optional[float] = None) -\u003e None: super().__init__() if scale is None or scale \u003c= 0.0: scale = 1.0 self.register_buffer( \"positional_encoding_gaussian_matrix\", scale * torch.randn((2, num_pos_feats)), ) def _pe_encoding(self, coords: torch.Tensor) -\u003e torch.Tensor: coords = 2 * coords - 1 coords = coords @ self.positional_encoding_gaussian_matrix coords = 2 * np.pi * coords # outputs d_1 x ... x d_n x C shape return torch.cat([torch.sin(coords), torch.cos(coords)], dim=-1) def forward(self, size: Tuple[int, int]) -\u003e torch.Tensor: h, w = size device: Any = self.positional_encoding_gaussian_matrix.device grid = torch.ones((h, w), device=device, dtype=torch.float32) y_embed = grid.cumsum(dim=0) - 0.5 x_embed = grid.cumsum(dim=1) - 0.5 y_embed = y_embed / h x_embed = x_embed / w pe = self._pe_encoding(torch.stack([x_embed, y_embed], dim=-1)) return pe.permute(2, 0, 1) # C x H x W In essence, get_dense_pe function returns image positional embedding, which later will be passed to the mask decoder. In forward function, it shows how image positional embeddings are constructed for the entire 64 x 64 feature grid into four steps\nCreates a coordinate grid for the feature map. Normalizes x and y coordinates to \\([0,1]\\). Centers the ccoordinates such that \\([-0.5, 0.5]\\) Apply positional encoding When creating the grid, it first initializes 2d tensor filled with ones. For x and y axis, cumsum computes cumulative sum along each axis. For example, if h=3, w = 3, then cumsum does:\nx_embed = grid.cumsum(dim=1) - 0.5 \\[ \\begin{bmatrix} 1 \u0026 2 \u0026 3 \\\\ 1 \u0026 2 \u0026 3 \\\\ 1 \u0026 2 \u0026 3 \\end{bmatrix} \\] y_embed = grid.cumsum(dim=0) - 0.5 \\[ \\begin{bmatrix} 1 \u0026 1 \u0026 1 \\\\ 2 \u0026 2 \u0026 2 \\\\ 3 \u0026 3 \u0026 3 \\end{bmatrix} \\] After normalization and certering the coordinates, it performs positional encoding. Mathmatically, postional encdoing is given by:\n\\[ \\text{PE}(x,y) = \\sin(2\\pi W \\begin{bmatrix} x \\\\ y \\end{bmatrix}) \\oplus \\cos(2\\pi W \\begin{bmatrix} x \\\\ y \\end{bmatrix}) \\] where:\n\\(\\begin{bmatrix} x \\\\ y \\end{bmatrix} \\in \\mathbb{R}^{B \\times H \\times W \\times 2}\\) is a stacked grid feature. \\(W \\in \\mathbb{R}^{2 \\times d}\\) is the Gaussian projection matrix that maps 2D coordinates to a higher-dimensional space. \\(\\oplus\\) refers to concatenation along the feature dimension. \\(\\text{PE}(x,y) \\in \\mathbb{R}^{B \\times H \\times W \\times 2d}\\) is the final positional encoding Now that we understand how image positional embeddings are computed. These embeddings, along with sparse and dense prompts, will be passed to the mask decoder to guide segmentation.\n3.2. Point Embedding We can pass \\(N\\) number of points per image to SAM. Each point acts as a spatial cue, helping the model focus on specific regions of interest within the image. These points are then transformed into high-dimensional-sparse embeddings.\nclass PromptEncoder(nn.Module): def __init__( #some arguements... ) super().__init__() self.embed_dim = embed_dim self.input_image_size = input_image_size self.image_embedding_size = image_embedding_size self.pe_layer = PositionEmbeddingRandom(embed_dim // 2) point_embeddings = [nn.Embedding(1, embed_dim) for i in range(self.num_point_embeddings)] self.point_embeddings = nn.ModuleList(point_embeddings) def _embed_points( self, points: torch.Tensor, labels: torch.Tensor, pad: bool, ) -\u003e torch.Tensor: \"\"\"Embeds point prompts.\"\"\" points = points + 0.5 # Shift to center of pixel if pad: padding_point = torch.zeros((points.shape[0], 1, 2), device=points.device) padding_label = -torch.ones((labels.shape[0], 1), device=labels.device) points = torch.cat([points, padding_point], dim=1) labels = torch.cat([labels, padding_label], dim=1) point_embedding = self.pe_layer.forward_with_coords(points, self.input_image_size) point_embedding[labels == -1] = 0.0 point_embedding[labels == -1] += self.not_a_point_embed.weight point_embedding[labels == 0] += self.point_embeddings[0].weight point_embedding[labels == 1] += self.point_embeddings[1].weight return point_embedding def forward( self, points: Optional[Tuple[torch.Tensor, torch.Tensor]], boxes: Optional[torch.Tensor], masks: Optional[torch.Tensor], ) -\u003e Tuple[torch.Tensor, torch.Tensor]: bs = self._get_batch_size(points, boxes, masks) sparse_embeddings = torch.empty((bs, 0, self.embed_dim), device=self._get_device()) #place holder if points is not None: coords, labels = points point_embeddings = self._embed_points(coords, labels, pad=(boxes is None)) sparse_embeddings = torch.cat([sparse_embeddings, point_embeddings], dim=1) Okay, let’s go into detail on how points and boxes are encoded. self.pe_layer, which is an object of PositionEmbeddingRandom, maps coordinates into a higher-dimensional space. The member function forward_with_coords performs normalization, linear projection, and sinusoidal transformation, same as we did for image positional embedding.\n\\[ \\text{PE}(x, y) = \\sin\\left( 2\\pi \\cdot W \\cdot \\begin{bmatrix} 2x - 1 \\\\ 2y - 1 \\end{bmatrix} \\right) \\oplus \\cos\\left( 2\\pi \\cdot W \\cdot \\begin{bmatrix} 2x - 1 \\\\ 2y - 1 \\end{bmatrix} \\right) \\] where:\n\\(x,y \\in \\mathbb{R}^{B \\times N \\times 2}\\) is input coordinates (batch of 𝑁 points per image). \\(W \\in \\mathbb{R}^{2 \\times d}\\) \\(\\text{PE}(x,y) \\in \\mathbb{R} ^{B \\times N \\times 2d}\\) Before we analyze what happens after executing point_embedding = self.pe_layer.forward_with_coords(points, self.input_image_size) is excuted, let’s first discuss positive (foreground) points and negative (background) points. Actually, SAM has a feature that I haven’t mentioned yet—you can provide background points. A background point is a point that you are not interested in and explicitly mark as not part of the object. See the image below, or you can test this in the demo. In the image, the blue dot represents a positive point, while the red dot represents a negative point. But wait! why do we need labels for the forward pass? In this context, labels are not ground truth segmentation masks. Instead, they indicate whether each click is a positive (foreground) or negative (background) point when passing inputs to the prompt encoder. So, the label is an array of size \\(N\\), where each entry is either 1 (positive) or 0 (negative).\nAs sparse_embeddings is an empty tensor that has zero dimension in sequnence dimension, the concatenation with point_embeddings doesn’t affect the shape of tensor.\n3.3. Box Encoding The bounding box is defined by four coordinates \\((x_1, y_1, x_2,y_2)\\), representing the top-left and bottom-right corners.\nclass PromptEncoder(nn.Module): def __init__( #some arguements... ) def _embed_points(self, points, labels, pad): #skip... return point_embedding def _embed_boxes(self, boxes: torch.Tensor) -\u003e torch.Tensor: \"\"\"Embeds box prompts.\"\"\" boxes = boxes + 0.5 # Shift to center of pixel coords = boxes.reshape(-1, 2, 2) corner_embedding = self.pe_layer.forward_with_coords(coords, self.input_image_size) corner_embedding[:, 0, :] += self.point_embeddings[2].weight corner_embedding[:, 1, :] += self.point_embeddings[3].weight return corner_embedding def forward( self, points: Optional[Tuple[torch.Tensor, torch.Tensor]], boxes: Optional[torch.Tensor], masks: Optional[torch.Tensor], ) -\u003e Tuple[torch.Tensor, torch.Tensor]: bs = self._get_batch_size(points, boxes, masks) sparse_embeddings = torch.empty((bs, 0, self.embed_dim), device=self._get_device()) if points is not None: coords, labels = points point_embeddings = self._embed_points(coords, labels, pad=(boxes is None)) sparse_embeddings = torch.cat([sparse_embeddings, point_embeddings], dim=1) if boxes is not None: box_embeddings = self._embed_boxes(boxes) sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=1) Similar to points, these coordinates are mapped to positional encodings using sinusoidal transformation. However, unlike point_embedding, which consists of N points, corner_embedding represents only one bounding box per image. You can see this from the line, coords = boxes.reshape(-1, 2, 2), which reshapes the input into \\(B \\times 2\\). Here, the last two dimensions represent the (x, y) coordinates of the two corners (top-left and bottom-right) of the bounding box. After mapping the box to positional embedding, we add learnable parameters to the top-left and bottom-right corners.\ncorner_embedding[:, 0, :] += self.point_embeddings[2].weight corner_embedding[:, 1, :] += self.point_embeddings[3].weight Eventually, box_embeddings will have the dimension of \\(\\mathbb{R}^{B,2,2d}\\).\nLet’s take a look at how sparse_embeddings are udpated. sparse_embedding is initiallized with empty tensor with the dimension of \\(\\mathbb{R}^{B \\times N \\times C}\\) where \\(C=2d\\). If both points and a box are provided as input, the prompt encoder concatenates sparse_embeddings with point_embeddings and box_embeddings, updating its shape accordingly. Eventually, the final sparse prompt embedding’s dimension will be: \\[ \\text{PE}_{\\text{sparse}} \\in \\mathbb{R}^{B \\times N+2 \\times C}. \\] I have a question for you. What will be the dimension of \\(\\text{PE}_{\\text{sparse}}\\) when only points prompt is given without bounding box. What about only bounding box is given? We have three input scenarios\n\\(N\\) points prompts, No bounding box ➡️ \\(\\mathbb{R}^{B \\times N \\times C}\\) No points prompts, one bounding box ➡️ \\(\\mathbb{R}^{B \\times 2 \\times C}\\) \\(N\\) points prompts, one bounding box ➡️ \\(\\mathbb{R}^{B \\times N + 2 \\times C}\\) Something is odd. How we can forward pass tensor that has different sequence length (middle dimension) for each pass? If you can’t answer this question, you can read my previous post. In short, there is no nn.Linear (project layer) in prompt encoder so we don’t need to care about variable-length sequnces.\n3.4. Dense prompt encoding Unlike sparse prompts, which are first mapped to an embedding space using positional encoding, dense prompts are directly projected using convolutions and then summed element-wise with the image embedding.The input mask is a binary tensor \\( M \\) of shape:\n\\[ M \\in \\mathbb{R}^{B \\times 1 \\times 256 \\times 256} \\] where:\n\\( B \\) is the batch size. The single channel (1) represents a binary mask (foreground vs. background). \\( 256 \\times 256 \\) is a fixed spatial resolution for masks in SAM. In the Prompt Encoder, the mask undergoes convolutional transformations to extract meaningful features. This is done in the _embed_masks function:\nclass PromptEncoder(nn.Module): def __init__( self, embed_dim: int, image_embedding_size: Tuple[int, int], input_image_size: Tuple[int, int], mask_in_chans: int, activation: Type[nn.Module] = nn.GELU, ) self.mask_downscaling = nn.Sequential( nn.Conv2d(1, mask_in_chans // 4, kernel_size=2, stride=2), LayerNorm2d(mask_in_chans // 4), activation(), nn.Conv2d(mask_in_chans // 4, mask_in_chans, kernel_size=2, stride=2), LayerNorm2d(mask_in_chans), activation(), nn.Conv2d(mask_in_chans, embed_dim, kernel_size=1), ) def _embed_masks(self, masks: torch.Tensor) -\u003e torch.Tensor: \"\"\"Embeds mask input.\"\"\" mask_embedding = self.mask_downscaling(masks) return mask_embedding def forward( self, points: Optional[Tuple[torch.Tensor, torch.Tensor]], boxes: Optional[torch.Tensor], masks: Optional[torch.Tensor], ) #skip sparse prompt if masks is not None: dense_embeddings = self._embed_masks(masks) else: dense_embeddings = self.no_mask_embed.weight.reshape(1, -1, 1, 1).expand( bs, -1, self.image_embedding_size[0], self.image_embedding_size[1] ) return sparse_embeddings, dense_embeddings mask_downscaling is a learnable CNN module that reduces the resolution of the mask while increasing its feature depth. This converts the binary mask into an embedding space that aligns with the image features. The resulting mask embedding has the shape: \\(\\mathbb{R}^{B \\times C \\times H' \\times W'}\\), where \\(C=256, H'=64, W'=64\\). Now the mask embedding dimension matches the image embedding so that both can be used together in the mask decoder.\nHowever, if no mask is given (masks=None), SAM instead uses a learnable “no-mask” embedding. self.no_mask_embed.weight is a learnable tensor representing a default mask embedding when a mask is not given. It is reshaped and expanded to match the required shape, \\(\\mathbb{R}^{B \\times C \\times H' \\times W'}\\). This ensures that even when no mask is provided, the model still has a valid dense prompt embedding.\n4. Mask Decoder So far, we have got four embeddings before we pass them to mask decoder:\nimage embedding image positional embedding sparse prompt embedding dense prompt embedding The mask decoder returns two objects: a mask and an IoU confidence score. Before we go deeper, let me ask how familiar you are with the transformer decoder. Before I studid this paper, I was not familiar with the transformer decoder, as I mostly worked with ViT or Swin Transformer, which only use the encoder of a transformer. Let me give you a quick recap about transformer decoder. A Transformer decoder takes “output embedding” as input. The output embedding representation is refined through attention mechanism. In the next training step, the highest logit token is mapped back to an output embedding. In the decoder’s attention stage, the model attends to the encoder’s output. This process is called cross-attention.\nNow that we’ve covered the basics of the Transformer decoder, let’s dive into SAM’s mask decoder. Unlike text generation models, where the decoder outputs a sequence of tokens, SAM’s mask decoder is designed to predict segmentation masks based on mask tokens. SAM’s mask decoder follows a similar structure to a Transformer decoder but is tailored for image segmentation. The key difference is that instead of processing text tokens, the decoder refines mask tokens to generate segmentation masks.\n4.1. Input Processing for Mask Decoder The decoder starts with a set of learnable mask tokens and an IoU token. These tokens act as placeholders, similar to how DETR initializes object queries for object detection.\nclass MaskDecoder(nn.Module): def __init__(): #skip parameters #skip self.iou_token = nn.Embedding(1, transformer_dim) self.num_mask_tokens = num_multimask_outputs + 1 self.mask_tokens = nn.Embedding(self.num_mask_tokens, transformer_dim) The IoU token has dimension of \\(\\mathbb{R}^{1 \\times 256}\\) and mask token has dimension of \\(\\mathbb{R}^{4 \\times 256}\\).\nYou might wonder why the sequence dimension of mask token is four. SAM produces three masks by default considering a single input prompt may be ambiguous. This means even if you provides single point as prompt, SAM will give you three masks. Then why four not three? The default mask token is added to the three tokens. This token is used when an user doesn’t want multi-mask option.\nmasks, _, _ = predictor.predict( point_coords=input_point, point_labels=input_label, multimask_output=False, ) This ensures that SAM always has a fallback “default mask” in addition to the three multimask outputs. The first token is used when multimask_output is off. The three tokens are used when multimask_output is on.\nclass MaskDecoder(nn.Module): def forward(): #skip parameters if multimask_output: mask_slice = slice(1, None) # Selects the three multimask outputs else: mask_slice = slice(0, 1) # Selects only the first mask (default) masks = masks[:, mask_slice, :, :] The IoU tokens and mask tokens are concatenated with sparse prompt embeddings before passing them through the Transformer.\nclass MaskDecoder(nn.Module): def forward(): #skip parameters masks, iou_pred = self.predict_masks( image_embeddings=image_embeddings, image_pe=image_pe, sparse_prompt_embeddings=sparse_prompt_embeddings, dense_prompt_embeddings=dense_prompt_embeddings, ) def predict_masks( self, image_embeddings: torch.Tensor, image_pe: torch.Tensor, sparse_prompt_embeddings: torch.Tensor, dense_prompt_embeddings: torch.Tensor, ): output_tokens = torch.cat([self.iou_token.weight, self.mask_tokens.weight], dim=0) output_tokens = output_tokens.unsqueeze(0).expand(sparse_prompt_embeddings.size(0), -1, -1) tokens = torch.cat((output_tokens, sparse_prompt_embeddings), dim=1) tokens = torch.cat((output_tokens, sparse_prompt_embeddings), dim=1) src = torch.repeat_interleave(image_embeddings, tokens.shape[0], dim=0) src = src + dense_prompt_embeddings pos_src = torch.repeat_interleave(image_pe, tokens.shape[0], dim=0) # Run the transformer hs, src = self.transformer(src, pos_src, tokens) tokens tensor has shape of \\(\\mathbb{R}^{B \\times (N + 5) \\times 256}\\) where \\(N\\) is the number of sparse prompts. src = torch.repeat_interleave(image_embeddings, tokens.shape[0], dim=0) expands image embedding in batch dimension from \\(\\mathbb{R}^{B \\times 256 \\times 64 \\times 64}\\) to \\(\\mathbb{R}^{B' \\times 256 \\times 64 \\times 64}\\) if tokens batch size and image_embeddings batch size are different. But I am still not sure why they wrote this line. I assume these two batch sizes are always the same. Lastly, it adds image_embeddings to dense_prompt_embeddings. Now, we are done for input processing before passing to the decoder. self.transformer takes three inputs:\nsrc: image embedding + dense prompt pos_src: image positional embedding tokens: mask token \\(\\oplus\\) IoU token \\(\\oplus\\)sparse prompt embeddings 4.2. TwoWayAttention Transformer SAM’s mask decoder utilizes a TwoWayTransformer, which differs from a standard transformer decoder by incorporating two cross-attention stages: (1) tokens attending to image features and (2) image features attending to tokens. This bidirectional attention mechanism allows the model to effectively refine mask predictions by leveraging both sparse and dense prompts. The TwoWayTransformer consists of multiple layers (depth) of TwoWayAttentionBlock modules, followed by a final attention layer for mask prediction.\nThe TwoWayTransformer takes three main inputs:\nimage_embedding (B, 256, 64, 64): Image features with dense prompt (i.e. \\(I + M\\)).\nimage_pe (B, 256, 64, 64): Positional encodings for image features.\npoint_embedding (B, N+5, 256): Encoded sparse prompts.\nThe image embedding is first flattened from (B, 256, H, W) → (B, HW, 256) so that it can interact with the mask tokens.\nbs, c, h, w = image_embedding.shape image_embedding = image_embedding.flatten(2).permute(0, 2, 1) image_pe = image_pe.flatten(2).permute(0, 2, 1) Next, the query tokens (mask tokens + IoU token) interact with the image features via two stacked TwoWayAttentionBlock layers:\nqueries = point_embedding keys = image_embedding for layer in self.layers: queries, keys = layer( queries=queries, keys=keys, query_pe=point_embedding, key_pe=image_pe, ) We are passing image embedding and image positional embedding for keys and key_pe. But query_pe is just copy of query. Why are passing the two arguments for two different parameters? Well, we don’t have a separate postional encoding for point_embedding, which is concatenation of IoU tokens, mask tokens, and sparse prompt embeddings. However, the sparse prompt embedding was computed using positional encoding. Even if we are passing the point_embedding itself as positional encdoing, it has chance to learn positional information through attention mechanism. Instead, the embeddings themselves serve both as features and positional encodings, query_pe = point_embedding.\nLet’s break down the two way attention block. The below diagram is a visualizaation of the two way attention block.\nSelf-Attention (Tokens)\nIf it’s the first layer, positional encoding is skipped. Otherwise, the positional encoding (query_pe) is added before passing through the self-attention layer. Cross-Attention (Tokens → Image Embeddings)\nTokens (queries) attend to image embeddings (keys). This allows the sparse prompts (mask tokens, IoU tokens) to interact with image features. MLP Block\nThe sparse queries are passed through an MLP block for further refinement. Cross-Attention (Image Embeddings → Tokens)\nNow, the image features (keys) attend back to the sparse queries (queries). This lets the image embeddings influence the sparse tokens. After the two layers, a final cross-attention layer is applied where queries and keys interact again:\nq = queries + point_embedding k = keys + image_pe attn_out = self.final_attn_token_to_image(q=q, k=k, v=keys) queries = queries + attn_out queries = self.norm_final_attn(queries) return queries, keys In the end, two way transformer returns tokens (IoU tokens, mask tokens, and sparse embedding) and image embedding. I recommends to check the implementation source code.\n4.3. Final Output class MaskDecoder(nn.Module): def init(): #skip self.output_upscaling = nn.Sequential( nn.ConvTranspose2d(transformer_dim, transformer_dim // 4, kernel_size=2, stride=2), LayerNorm2d(transformer_dim // 4), activation(), nn.ConvTranspose2d(transformer_dim // 4, transformer_dim // 8, kernel_size=2, stride=2), activation(), ) self.output_hypernetworks_mlps = nn.ModuleList( [ MLP(transformer_dim, transformer_dim, transformer_dim // 8, 3) for i in range(self.num_mask_tokens) ] ) self.iou_prediction_head = MLP( transformer_dim, iou_head_hidden_dim, self.num_mask_tokens, iou_head_depth ) def predict_masks(): #skip... hs, src = self.transformer(src, pos_src, tokens) iou_token_out = hs[:, 0, :] mask_tokens_out = hs[:, 1 : (1 + self.num_mask_tokens), :] # Upscale mask embeddings and predict masks using the mask tokens src = src.transpose(1, 2).view(b, c, h, w) upscaled_embedding = self.output_upscaling(src) hyper_in_list: List[torch.Tensor] = [] for i in range(self.num_mask_tokens): hyper_in_list.append(self.output_hypernetworks_mlps[i](mask_tokens_out[:, i, :])) hyper_in = torch.stack(hyper_in_list, dim=1) b, c, h, w = upscaled_embedding.shape masks = (hyper_in @ upscaled_embedding.view(b, c, h * w)).view(b, -1, h, w) iou_pred = self.iou_prediction_head(iou_token_out) return masks, iou_pred After transformer processed the image embedding and tokens, we extracts IoU token and mask token from tokens. The mask_tokens_out dimension is \\(\\mathbb{R}^{B \\times 4 \\times 256}\\). We want masks to be 4-dimensional shape, \\(\\mathbb{R}^{B \\times 4 \\times H \\times W}\\). The mask tokens are transformed into mask predictions via hypernetworks, and the upscaled image features are used for final mask refinement.\nsrc represents the transformed image embeddings after passing through the transformer. We reshape src dimension form (B, HW, C) to (B, C, H, W). self.output_upscaling(src) applies an upscaling operation using two transposed convolution layers.\nmask_tokens_out is of shape (B, 4, 256). Each mask token, mask_tokens_out[:, i, :], is passed through a hypernetwork MLP. self.output_hypernetworks_mlps[i] is an MLP that processes each mask token separately. The matrix multiplication of hyper_in by upscaled_embedding, followed by reshaping, results in masks shaped (B, 4, H, W). Another MLP maps it to the final IoU prediction scores, indicating the confidence of each mask.\nDiscusssion After reading the entire paper and exploring other references, I found myself wondering—why is SAM receiving so much praise? Given its high citation count and widespread adoption in both industry and academia, it’s clear that SAM is considered a game-changer. But why? Interactive segmentation and transformer-based architectures aren’t new concepts. Researchers have been exploring these areas for years. So, what makes SAM stand out?\nThe key lies in its large-scale dataset and model training. The team behind SAM didn’t just build another segmentation model; they demonstrated that scaling up both the dataset and the model itself leads to remarkable performance gains. This aligns with the proven scaling laws in deep learning, where larger models trained on massive datasets tend to generalize better and unlock new capabilities. SAM isn’t just an incremental improvement—it’s a demonstration of how foundation models in computer vision can follow the same trajectory as large language models, fundamentally shifting how we approach image segmentation.\nConclusion The Segment Anything Model (SAM) represents a significant advancement in the field of computer vision, particularly in image segmentation. By leveraging a promptable architecture, SAM eliminates the need for task-specific fine-tuning, enabling zero-shot learning across various segmentation tasks. Its three core components—the image encoder, prompt encoder, and mask decoder—work in harmony to generate precise segmentation masks based on user-provided prompts such as points, boxes, or masks. SAM’s ability to handle sparse and dense prompts, combined with its efficient use of positional embeddings and transformer-based decoding, makes it a versatile and powerful tool for segmentation.\nReference Segment Anything A Comprehensive Survey on Segment Anything Model for Vision and Beyond Explaining the Segment Anything Model - Network architecture, Dataset, Training Medical image segmentation using deep learning: A survey How Does the Segment-Anything Model’s (SAM’s) Encoder Work? Transformer self-attention padding and causal masking technique ",
  "wordCount" : "3970",
  "inLanguage": "en",
  "datePublished": "2025-01-29T13:49:47-05:00",
  "dateModified": "2025-01-29T13:49:47-05:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://baampark.github.io/posts/2025-01-29_sam/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Baam's Techlog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://baampark.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://baampark.github.io/" accesskey="h" title="Baam&#39;s Techlog (Alt + H)">Baam&#39;s Techlog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://baampark.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://baampark.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Segment Anything, the first large-scale foundation model for segmentation
    </h1>
    <div class="post-meta"><span title='2025-01-29 13:49:47 -0500 EST'>January 29, 2025</span>

</div>
  </header> 
  <div class="post-content"><p><img loading="lazy" src="/images/2025-01-29_SAM/intro.png" alt="SAM"  />

<a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kirillov_Segment_Anything_ICCV_2023_paper.html">Segment Anything (SAM)</a> has drawn massive attention in the computer vision community, accumulating an impressive 8,000 citations. Segmentation has long been a crucial yet challenging aspect of computer vision. One of the biggest hurdles? Annotation. Unlike simple bounding boxes, which only require marking the object’s general location, segmentation demands precise pixel-level annotations—an incredibly tedious and time-consuming task for annotators. SAM is one of the first large-scale foundation models for segmentation. What makes SAM truly great is that it’s a promptable model. This means you can use it for various tasks without the need for fine-tuning—also known as zero-shot learning. Unlike LLM, here the prompts for the SAM are points, bounding boxes, and masks. In this post, we’re going to explore the key components of SAM. This guide will break things down in a simple and easy-to-follow way. Let’s get started! 🚀.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h1 id="1-sam-architecture-overview">1. SAM Architecture Overview<a hidden class="anchor" aria-hidden="true" href="#1-sam-architecture-overview">#</a></h1>
<p><img loading="lazy" src="/images/2025-01-29_SAM/SAM_architecture.png" alt="SAM"  />

The SAM (Segment Anything Model) architecture consists of three main components: an image encoder, a prompt encoder, and a mask decoder. The image encoder processes the input image to generate an embedding, while the prompt encoder takes user-provided prompts (such as points, boxes, or text) to refine the segmentation. The mask decoder then combines these embeddings and prompts to produce multiple valid segmentation masks, each with an associated confidence score.</p>
<h2 id="2-image-encoder">2. Image Encoder<a hidden class="anchor" aria-hidden="true" href="#2-image-encoder">#</a></h2>
<p><img loading="lazy" src="/images/2025-01-29_SAM/MAE.png" alt="MAE"  />

The image encoder of SAM (Segment Anything Model) is quite straightforward. The authors pre-trained the Vision Transformer (ViT) using Masked Autoencoder (MAE)—both of which are widely recognized techniques in the computer vision community.</p>
<p>ViT is one of the pioneering large-scale foundation models for image classification. Meanwhile, MAE is well known for its effectiveness in pre-training models. The idea behind MAE is simple yet powerful: it randomly applies zero-masking to some image patches before passing them through the encoder. The decoder then attempts to reconstruct the masked patches, forcing the model to develop a deeper understanding of image structures. Essentially, the \(1024 \times 1024\) image is embedded into feature space \(\mathbb{R}^{256 \times 64 \times 64}\).</p>
<h2 id="3-prompt-encoder">3. Prompt Encoder<a hidden class="anchor" aria-hidden="true" href="#3-prompt-encoder">#</a></h2>
<p>The prompt encoder takes three types inputs: points, bounding boxes, text, and mask. Points, bounding boxes, and text are treated sparse prompt. The mask is treated as dense prompt. However, the authors said that text prompts are just an exploration, so we won&rsquo;t cover them in this article. The prompt encoder has two jobs mainly: <strong>sparse prompt embedding</strong> and <strong>dense prompt embedding</strong>. However, if you see the <code>PromptEncoder</code> implementation, you will notice there is one more thing it returns, which is <strong>image positional embedding</strong>. We will learn how these three embeddings are processed.</p>
<h2 id="31-image-positional-embedding">3.1. Image Positional Embedding<a hidden class="anchor" aria-hidden="true" href="#31-image-positional-embedding">#</a></h2>
<blockquote>
<p>To ensure the decoder has access to critical geometric information the positional encodings are added to the image embedding whenever they participate in an attention layer. - <em>SAM, Segment Anything Model and Task Details, Lightweight mask decoder</em></p>
</blockquote>
<p>The authors said positional encodings are added to the image embedding. It encodes positional information for the entire image feature grid. The concept of positional encoding was originated from transformer. Transformers use self-attention to process inputs, but unlike RNNs or CNNs, they do not inherently capture positional information (i.e. permutation-invariant). Positional encoding is added to the input embeddings to provide a sense of order in the sequence. The image below shows how positional embedding and image embedding are added in transformer.</p>
<p><img loading="lazy" src="/images/2025-01-29_SAM/embeddings.png" alt="embeddings"  />
</p>
<p>What would be the dimension of positional encoding? It&rsquo;s \(256 \times 64 \times 64\), which matches the dimension of the image embedding. This is because the image embedding and image positional embedding are added together element-wise. Let&rsquo;s review a part of the <code>PromptEncoder</code> implementation.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PromptEncoder</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__():
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#skip...</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>pe_layer <span style="color:#f92672">=</span> PositionEmbeddingRandom(embed_dim <span style="color:#f92672">//</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_dense_pe</span>(self) <span style="color:#f92672">-&gt;</span> torch<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>pe_layer(self<span style="color:#f92672">.</span>image_embedding_size)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PositionEmbeddingRandom</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, num_pos_feats: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">64</span>, scale: Optional[float] <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> scale <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span> <span style="color:#f92672">or</span> scale <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">0.0</span>:
</span></span><span style="display:flex;"><span>            scale <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>register_buffer(
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;positional_encoding_gaussian_matrix&#34;</span>,
</span></span><span style="display:flex;"><span>            scale <span style="color:#f92672">*</span> torch<span style="color:#f92672">.</span>randn((<span style="color:#ae81ff">2</span>, num_pos_feats)),
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_pe_encoding</span>(self, coords: torch<span style="color:#f92672">.</span>Tensor) <span style="color:#f92672">-&gt;</span> torch<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>        coords <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> coords <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>        coords <span style="color:#f92672">=</span> coords <span style="color:#f92672">@</span> self<span style="color:#f92672">.</span>positional_encoding_gaussian_matrix
</span></span><span style="display:flex;"><span>        coords <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>pi <span style="color:#f92672">*</span> coords
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># outputs d_1 x ... x d_n x C shape</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> torch<span style="color:#f92672">.</span>cat([torch<span style="color:#f92672">.</span>sin(coords), torch<span style="color:#f92672">.</span>cos(coords)], dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, size: Tuple[int, int]) <span style="color:#f92672">-&gt;</span> torch<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>        h, w <span style="color:#f92672">=</span> size
</span></span><span style="display:flex;"><span>        device: Any <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>positional_encoding_gaussian_matrix<span style="color:#f92672">.</span>device
</span></span><span style="display:flex;"><span>        grid <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>ones((h, w), device<span style="color:#f92672">=</span>device, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32)
</span></span><span style="display:flex;"><span>        y_embed <span style="color:#f92672">=</span> grid<span style="color:#f92672">.</span>cumsum(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>) <span style="color:#f92672">-</span> <span style="color:#ae81ff">0.5</span>
</span></span><span style="display:flex;"><span>        x_embed <span style="color:#f92672">=</span> grid<span style="color:#f92672">.</span>cumsum(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>) <span style="color:#f92672">-</span> <span style="color:#ae81ff">0.5</span>
</span></span><span style="display:flex;"><span>        y_embed <span style="color:#f92672">=</span> y_embed <span style="color:#f92672">/</span> h
</span></span><span style="display:flex;"><span>        x_embed <span style="color:#f92672">=</span> x_embed <span style="color:#f92672">/</span> w
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        pe <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_pe_encoding(torch<span style="color:#f92672">.</span>stack([x_embed, y_embed], dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> pe<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># C x H x W</span>
</span></span></code></pre></div><p>In essence, <code>get_dense_pe</code> function returns image positional embedding, which later will be passed to the mask decoder. In <code>forward</code> function, it shows how image positional embeddings are constructed for the entire 64 x 64 feature grid into four steps</p>
<ol>
<li>Creates a coordinate grid for the feature map.</li>
<li>Normalizes x and y coordinates to \([0,1]\).</li>
<li>Centers the ccoordinates such that \([-0.5, 0.5]\)</li>
<li>Apply positional encoding</li>
</ol>
<p>When creating the grid, it first initializes 2d tensor filled with ones. For x and y axis, <code>cumsum</code> computes cumulative sum along each axis. For example, if <code>h=3, w = 3</code>, then cumsum does:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x_embed <span style="color:#f92672">=</span> grid<span style="color:#f92672">.</span>cumsum(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>) <span style="color:#f92672">-</span> <span style="color:#ae81ff">0.5</span>
</span></span></code></pre></div>\[
\begin{bmatrix}
1 & 2 & 3 \\
1 & 2 & 3 \\
1 & 2 & 3
\end{bmatrix}
\]
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>y_embed <span style="color:#f92672">=</span> grid<span style="color:#f92672">.</span>cumsum(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>) <span style="color:#f92672">-</span> <span style="color:#ae81ff">0.5</span>
</span></span></code></pre></div>\[
\begin{bmatrix}
1 & 1 & 1 \\
2 & 2 & 2 \\
3 & 3 & 3
\end{bmatrix}
\]
<p>After normalization and certering the coordinates, it performs positional encoding. Mathmatically, postional encdoing is given by:</p>
\[
    \text{PE}(x,y) = \sin(2\pi W \begin{bmatrix} x \\ y \end{bmatrix}) \oplus \cos(2\pi W \begin{bmatrix} x \\ y \end{bmatrix})
\]
<p>
where:</p>
<ul>
<li>\(\begin{bmatrix} x \\ y \end{bmatrix} \in \mathbb{R}^{B \times H \times W \times 2}\) is a stacked grid feature.</li>
<li>\(W \in \mathbb{R}^{2 \times d}\) is the Gaussian projection matrix that maps 2D coordinates to a higher-dimensional space.</li>
<li>\(\oplus\) refers to concatenation along the feature dimension.</li>
<li>\(\text{PE}(x,y) \in \mathbb{R}^{B \times H \times W \times 2d}\) is the final positional encoding</li>
</ul>
<p>Now that we understand how image positional embeddings are computed. These embeddings, along with sparse and dense prompts, will be passed to the mask decoder to guide segmentation.</p>
<h3 id="32-point-embedding">3.2. Point Embedding<a hidden class="anchor" aria-hidden="true" href="#32-point-embedding">#</a></h3>
<p>We can pass \(N\) number of points per image to SAM. Each point acts as a spatial cue, helping the model focus on specific regions of interest within the image. These points are then transformed into high-dimensional-sparse embeddings.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PromptEncoder</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(
</span></span><span style="display:flex;"><span>      <span style="color:#75715e">#some arguements...</span>
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>embed_dim <span style="color:#f92672">=</span> embed_dim
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>input_image_size <span style="color:#f92672">=</span> input_image_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>image_embedding_size <span style="color:#f92672">=</span> image_embedding_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>pe_layer <span style="color:#f92672">=</span> PositionEmbeddingRandom(embed_dim <span style="color:#f92672">//</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        point_embeddings <span style="color:#f92672">=</span> [nn<span style="color:#f92672">.</span>Embedding(<span style="color:#ae81ff">1</span>, embed_dim) <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>num_point_embeddings)]
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>point_embeddings <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleList(point_embeddings)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_embed_points</span>(
</span></span><span style="display:flex;"><span>        self,
</span></span><span style="display:flex;"><span>        points: torch<span style="color:#f92672">.</span>Tensor,
</span></span><span style="display:flex;"><span>        labels: torch<span style="color:#f92672">.</span>Tensor,
</span></span><span style="display:flex;"><span>        pad: bool,
</span></span><span style="display:flex;"><span>    ) <span style="color:#f92672">-&gt;</span> torch<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Embeds point prompts.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        points <span style="color:#f92672">=</span> points <span style="color:#f92672">+</span> <span style="color:#ae81ff">0.5</span>  <span style="color:#75715e"># Shift to center of pixel</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> pad:
</span></span><span style="display:flex;"><span>            padding_point <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros((points<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>), device<span style="color:#f92672">=</span>points<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>            padding_label <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>torch<span style="color:#f92672">.</span>ones((labels<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#ae81ff">1</span>), device<span style="color:#f92672">=</span>labels<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>            points <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([points, padding_point], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>            labels <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([labels, padding_label], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        point_embedding <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>pe_layer<span style="color:#f92672">.</span>forward_with_coords(points, self<span style="color:#f92672">.</span>input_image_size)
</span></span><span style="display:flex;"><span>        point_embedding[labels <span style="color:#f92672">==</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>        point_embedding[labels <span style="color:#f92672">==</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>not_a_point_embed<span style="color:#f92672">.</span>weight
</span></span><span style="display:flex;"><span>        point_embedding[labels <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>] <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>point_embeddings[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>weight
</span></span><span style="display:flex;"><span>        point_embedding[labels <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>] <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>point_embeddings[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>weight
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> point_embedding
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(
</span></span><span style="display:flex;"><span>        self,
</span></span><span style="display:flex;"><span>        points: Optional[Tuple[torch<span style="color:#f92672">.</span>Tensor, torch<span style="color:#f92672">.</span>Tensor]],
</span></span><span style="display:flex;"><span>        boxes: Optional[torch<span style="color:#f92672">.</span>Tensor],
</span></span><span style="display:flex;"><span>        masks: Optional[torch<span style="color:#f92672">.</span>Tensor],
</span></span><span style="display:flex;"><span>    ) <span style="color:#f92672">-&gt;</span> Tuple[torch<span style="color:#f92672">.</span>Tensor, torch<span style="color:#f92672">.</span>Tensor]:
</span></span><span style="display:flex;"><span>        bs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_get_batch_size(points, boxes, masks)
</span></span><span style="display:flex;"><span>        sparse_embeddings <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>empty((bs, <span style="color:#ae81ff">0</span>, self<span style="color:#f92672">.</span>embed_dim), device<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>_get_device()) <span style="color:#75715e">#place holder</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> points <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            coords, labels <span style="color:#f92672">=</span> points
</span></span><span style="display:flex;"><span>            point_embeddings <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_embed_points(coords, labels, pad<span style="color:#f92672">=</span>(boxes <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span>))
</span></span><span style="display:flex;"><span>            sparse_embeddings <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([sparse_embeddings, point_embeddings], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div><p>Okay, let&rsquo;s go into detail on how points and boxes are encoded.
<code>self.pe_layer</code>, which is an object of <code>PositionEmbeddingRandom</code>, maps coordinates into a higher-dimensional space. The member function <code>forward_with_coords</code> performs normalization, linear projection, and sinusoidal transformation, same as we did for image positional embedding.</p>
\[
\text{PE}(x, y) = \sin\left( 2\pi \cdot W \cdot \begin{bmatrix} 2x - 1 \\ 2y - 1 \end{bmatrix} \right) \oplus \cos\left( 2\pi \cdot W \cdot \begin{bmatrix} 2x - 1 \\ 2y - 1 \end{bmatrix} \right)
\]
<p>where:</p>
<ul>
<li>\(x,y \in \mathbb{R}^{B \times N \times 2}\) is input coordinates (batch of
𝑁 points per image).</li>
<li>\(W \in \mathbb{R}^{2 \times d}\)</li>
<li>\(\text{PE}(x,y) \in \mathbb{R} ^{B \times N \times 2d}\)</li>
</ul>
<p>Before we analyze what happens after executing <code>point_embedding = self.pe_layer.forward_with_coords(points, self.input_image_size)</code> is excuted, let&rsquo;s first discuss positive (foreground) points and negative (background) points. Actually, SAM has a feature that I haven&rsquo;t mentioned yet—you can provide background points. A background point is a point that you are not interested in and explicitly mark as not part of the object. See the image below, or you can test this in the <a href="https://segment-anything.com/demo#">demo</a>.
<img loading="lazy" src="/images/2025-01-29_SAM/foreground_background.png" alt="foreground_background"  />
</p>
<p>In the image, the blue dot represents a positive point, while the red dot represents a negative point. But wait! why do we need labels for the forward pass? In this context, labels are not ground truth segmentation masks. Instead, they indicate whether each click is a positive (foreground) or negative (background) point when passing inputs to the prompt encoder. So, the label is an array of size \(N\), where each entry is either 1 (positive) or 0 (negative).</p>
<p>As <code>sparse_embeddings</code> is an empty tensor that has zero dimension in sequnence dimension, the concatenation with <code>point_embeddings</code> doesn&rsquo;t affect the shape of tensor.</p>
<h3 id="33-box-encoding">3.3. Box Encoding<a hidden class="anchor" aria-hidden="true" href="#33-box-encoding">#</a></h3>
<p>The bounding box is defined by four coordinates \((x_1, y_1, x_2,y_2)\), representing the top-left and bottom-right corners.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PromptEncoder</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(
</span></span><span style="display:flex;"><span>      <span style="color:#75715e">#some arguements...</span>
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_embed_points</span>(self, points, labels, pad):
</span></span><span style="display:flex;"><span>         <span style="color:#75715e">#skip...</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> point_embedding
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_embed_boxes</span>(self, boxes: torch<span style="color:#f92672">.</span>Tensor) <span style="color:#f92672">-&gt;</span> torch<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Embeds box prompts.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        boxes <span style="color:#f92672">=</span> boxes <span style="color:#f92672">+</span> <span style="color:#ae81ff">0.5</span>  <span style="color:#75715e"># Shift to center of pixel</span>
</span></span><span style="display:flex;"><span>        coords <span style="color:#f92672">=</span> boxes<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        corner_embedding <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>pe_layer<span style="color:#f92672">.</span>forward_with_coords(coords, self<span style="color:#f92672">.</span>input_image_size)
</span></span><span style="display:flex;"><span>        corner_embedding[:, <span style="color:#ae81ff">0</span>, :] <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>point_embeddings[<span style="color:#ae81ff">2</span>]<span style="color:#f92672">.</span>weight
</span></span><span style="display:flex;"><span>        corner_embedding[:, <span style="color:#ae81ff">1</span>, :] <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>point_embeddings[<span style="color:#ae81ff">3</span>]<span style="color:#f92672">.</span>weight
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> corner_embedding
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(
</span></span><span style="display:flex;"><span>        self,
</span></span><span style="display:flex;"><span>        points: Optional[Tuple[torch<span style="color:#f92672">.</span>Tensor, torch<span style="color:#f92672">.</span>Tensor]],
</span></span><span style="display:flex;"><span>        boxes: Optional[torch<span style="color:#f92672">.</span>Tensor],
</span></span><span style="display:flex;"><span>        masks: Optional[torch<span style="color:#f92672">.</span>Tensor],
</span></span><span style="display:flex;"><span>    ) <span style="color:#f92672">-&gt;</span> Tuple[torch<span style="color:#f92672">.</span>Tensor, torch<span style="color:#f92672">.</span>Tensor]:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        bs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_get_batch_size(points, boxes, masks)
</span></span><span style="display:flex;"><span>        sparse_embeddings <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>empty((bs, <span style="color:#ae81ff">0</span>, self<span style="color:#f92672">.</span>embed_dim), device<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>_get_device())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> points <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            coords, labels <span style="color:#f92672">=</span> points
</span></span><span style="display:flex;"><span>            point_embeddings <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_embed_points(coords, labels, pad<span style="color:#f92672">=</span>(boxes <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span>))
</span></span><span style="display:flex;"><span>            sparse_embeddings <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([sparse_embeddings, point_embeddings], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> boxes <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            box_embeddings <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_embed_boxes(boxes)
</span></span><span style="display:flex;"><span>            sparse_embeddings <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([sparse_embeddings, box_embeddings], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div><p>Similar to points, these coordinates are mapped to positional encodings using sinusoidal transformation. However, unlike <code>point_embedding</code>, which consists of <code>N</code> points, <code>corner_embedding</code> represents only one bounding box per image. You can see this from the line, <code>coords = boxes.reshape(-1, 2, 2)</code>, which reshapes the input into \(B \times 2\). Here, the last two dimensions represent the (x, y) coordinates of the two corners (top-left and bottom-right) of the bounding box.
After mapping the box to positional embedding, we add learnable parameters to the top-left and bottom-right corners.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>        corner_embedding[:, <span style="color:#ae81ff">0</span>, :] <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>point_embeddings[<span style="color:#ae81ff">2</span>]<span style="color:#f92672">.</span>weight
</span></span><span style="display:flex;"><span>        corner_embedding[:, <span style="color:#ae81ff">1</span>, :] <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>point_embeddings[<span style="color:#ae81ff">3</span>]<span style="color:#f92672">.</span>weight
</span></span></code></pre></div><p>Eventually, <code>box_embeddings</code> will have the dimension of \(\mathbb{R}^{B,2,2d}\).</p>
<p>Let&rsquo;s take a look at how <code>sparse_embeddings</code> are udpated. <code>sparse_embedding</code> is initiallized with empty tensor with the dimension of \(\mathbb{R}^{B \times N \times C}\) where \(C=2d\).
If both points and a box are provided as input, the prompt encoder concatenates <code>sparse_embeddings</code> with <code>point_embeddings</code> and <code>box_embeddings</code>, updating its shape accordingly. Eventually, the final sparse prompt embedding&rsquo;s dimension will be:
</p>
\[
    \text{PE}_{\text{sparse}} \in \mathbb{R}^{B \times N+2 \times C}.
 \]
<p>I have a question for you. What will be the dimension of \(\text{PE}_{\text{sparse}}\) when only points prompt is given without bounding box. What about only bounding box is given? We have three input scenarios</p>
<ul>
<li>\(N\) points prompts, No bounding box ➡️ \(\mathbb{R}^{B \times N \times C}\)</li>
<li>No points prompts, one bounding box ➡️ \(\mathbb{R}^{B \times 2 \times C}\)</li>
<li>\(N\) points prompts, one bounding box ➡️ \(\mathbb{R}^{B \times N + 2 \times C}\)</li>
</ul>
<p>Something is odd. How we can forward pass tensor that has different sequence length (middle dimension) for each pass? If you can&rsquo;t answer this question, you can read my <a href="https://baampark.github.io/posts/2025-01-28_variable_sequence/">previous post</a>. In short, there is no <code>nn.Linear</code> (project layer) in prompt encoder so we don&rsquo;t need to care about variable-length sequnces.</p>
<h3 id="34-dense-prompt-encoding">3.4. Dense prompt encoding<a hidden class="anchor" aria-hidden="true" href="#34-dense-prompt-encoding">#</a></h3>
<p>Unlike sparse prompts, which are first mapped to an embedding space using <strong>positional encoding</strong>, dense prompts are directly projected using convolutions and then summed element-wise with the image embedding.The input mask is a binary tensor \( M \) of shape:</p>
\[
M \in \mathbb{R}^{B \times 1 \times 256 \times 256}
\]
<p>
where:</p>
<ul>
<li>\( B \) is the batch size.</li>
<li>The <strong>single channel</strong> (1) represents a binary mask (foreground vs. background).</li>
<li>\( 256 \times 256 \) is a fixed spatial resolution for masks in SAM.</li>
</ul>
<p>In the Prompt Encoder, the mask undergoes convolutional transformations to extract meaningful features. This is done in the <code>_embed_masks</code> function:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PromptEncoder</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(
</span></span><span style="display:flex;"><span>        self,
</span></span><span style="display:flex;"><span>        embed_dim: int,
</span></span><span style="display:flex;"><span>        image_embedding_size: Tuple[int, int],
</span></span><span style="display:flex;"><span>        input_image_size: Tuple[int, int],
</span></span><span style="display:flex;"><span>        mask_in_chans: int,
</span></span><span style="display:flex;"><span>        activation: Type[nn<span style="color:#f92672">.</span>Module] <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>GELU,
</span></span><span style="display:flex;"><span>    ) 
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>mask_downscaling <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">1</span>, mask_in_chans <span style="color:#f92672">//</span> <span style="color:#ae81ff">4</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),
</span></span><span style="display:flex;"><span>            LayerNorm2d(mask_in_chans <span style="color:#f92672">//</span> <span style="color:#ae81ff">4</span>),
</span></span><span style="display:flex;"><span>            activation(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(mask_in_chans <span style="color:#f92672">//</span> <span style="color:#ae81ff">4</span>, mask_in_chans, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),
</span></span><span style="display:flex;"><span>            LayerNorm2d(mask_in_chans),
</span></span><span style="display:flex;"><span>            activation(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(mask_in_chans, embed_dim, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_embed_masks</span>(self, masks: torch<span style="color:#f92672">.</span>Tensor) <span style="color:#f92672">-&gt;</span> torch<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Embeds mask input.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        mask_embedding <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>mask_downscaling(masks)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> mask_embedding
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(
</span></span><span style="display:flex;"><span>        self,
</span></span><span style="display:flex;"><span>        points: Optional[Tuple[torch<span style="color:#f92672">.</span>Tensor, torch<span style="color:#f92672">.</span>Tensor]],
</span></span><span style="display:flex;"><span>        boxes: Optional[torch<span style="color:#f92672">.</span>Tensor],
</span></span><span style="display:flex;"><span>        masks: Optional[torch<span style="color:#f92672">.</span>Tensor],
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#skip sparse prompt</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> masks <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            dense_embeddings <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_embed_masks(masks)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            dense_embeddings <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>no_mask_embed<span style="color:#f92672">.</span>weight<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">1</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>expand(
</span></span><span style="display:flex;"><span>                bs, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>image_embedding_size[<span style="color:#ae81ff">0</span>], self<span style="color:#f92672">.</span>image_embedding_size[<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> sparse_embeddings, dense_embeddings
</span></span></code></pre></div><p><code>mask_downscaling</code> is a learnable CNN module that reduces the resolution of the mask while increasing its feature depth. This converts the binary mask into an embedding space that aligns with the image features. The resulting mask embedding has the shape: \(\mathbb{R}^{B \times C \times H' \times W'}\), where \(C=256, H'=64, W'=64\). Now the mask embedding dimension matches the image embedding so that both can be used together in the mask decoder.</p>
<p>However, if no mask is given (masks=None), SAM instead uses a learnable &ldquo;no-mask&rdquo; embedding. <code>self.no_mask_embed.weight</code> is a learnable tensor representing a default mask embedding when a mask is not given. It is reshaped and expanded to match the required shape, \(\mathbb{R}^{B \times C \times H' \times W'}\). This ensures that even when no mask is provided, the model still has a valid dense prompt embedding.</p>
<h2 id="4-mask-decoder">4. Mask Decoder<a hidden class="anchor" aria-hidden="true" href="#4-mask-decoder">#</a></h2>
<p><img loading="lazy" src="/images/2025-01-29_SAM/decoder.png" alt="decoder"  />

So far, we have got four embeddings before we pass them to mask decoder:</p>
<ul>
<li>image embedding</li>
<li>image positional embedding</li>
<li>sparse prompt embedding</li>
<li>dense prompt embedding</li>
</ul>
<p>The mask decoder returns two objects: a mask and an IoU confidence score. Before we go deeper, let me ask how familiar you are with the transformer decoder. Before I studid this paper, I was not familiar with the transformer decoder, as I mostly worked with ViT or Swin Transformer, which only use the encoder of a transformer. Let me give you a quick recap about transformer decoder. A Transformer decoder takes &ldquo;output embedding&rdquo; as input. The output embedding representation is refined through attention mechanism. In the next training step, the highest logit token is mapped back to an output embedding. In the decoder’s attention stage, the model attends to the encoder’s output. This process is called <strong>cross-attention</strong>.</p>
<p><img loading="lazy" src="/images/2025-01-29_SAM/transformer_abstract.png" alt="transformer abstract"  />
</p>
<p>Now that we’ve covered the basics of the Transformer decoder, let’s dive into SAM’s mask decoder. Unlike text generation models, where the decoder outputs a sequence of tokens, SAM&rsquo;s mask decoder is designed to predict segmentation masks based on mask tokens. SAM’s mask decoder follows a similar structure to a Transformer decoder but is tailored for image segmentation. The key difference is that instead of processing text tokens, the decoder refines mask tokens to generate segmentation masks.</p>
<h3 id="41-input-processing-for-mask-decoder">4.1. Input Processing for Mask Decoder<a hidden class="anchor" aria-hidden="true" href="#41-input-processing-for-mask-decoder">#</a></h3>
<p>The decoder starts with a set of learnable mask tokens and an IoU token. These tokens act as placeholders, similar to how DETR initializes object queries for object detection.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MaskDecoder</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(): <span style="color:#75715e">#skip parameters</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#skip</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>iou_token <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Embedding(<span style="color:#ae81ff">1</span>, transformer_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>num_mask_tokens <span style="color:#f92672">=</span> num_multimask_outputs <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>mask_tokens <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Embedding(self<span style="color:#f92672">.</span>num_mask_tokens, transformer_dim)
</span></span></code></pre></div><p>The IoU token has dimension of \(\mathbb{R}^{1 \times 256}\) and mask token has dimension of \(\mathbb{R}^{4 \times 256}\).</p>
<p>You might wonder why the sequence dimension of mask token is four. SAM produces three masks by default considering a single input prompt may be ambiguous. This means even if you provides single point as prompt, SAM will give you three masks. Then why four not three? The default mask token is added to the three tokens. This token is used when an user doesn&rsquo;t want multi-mask option.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>masks, _, _ <span style="color:#f92672">=</span> predictor<span style="color:#f92672">.</span>predict(
</span></span><span style="display:flex;"><span>    point_coords<span style="color:#f92672">=</span>input_point,
</span></span><span style="display:flex;"><span>    point_labels<span style="color:#f92672">=</span>input_label,
</span></span><span style="display:flex;"><span>    multimask_output<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>,
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>This ensures that SAM always has a fallback &ldquo;default mask&rdquo; in addition to the three multimask outputs. The first token is used when <code>multimask_output</code> is off. The three tokens are used when <code>multimask_output</code> is on.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MaskDecoder</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(): <span style="color:#75715e">#skip parameters</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> multimask_output:
</span></span><span style="display:flex;"><span>            mask_slice <span style="color:#f92672">=</span> slice(<span style="color:#ae81ff">1</span>, <span style="color:#66d9ef">None</span>)  <span style="color:#75715e"># Selects the three multimask outputs</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            mask_slice <span style="color:#f92672">=</span> slice(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># Selects only the first mask (default)</span>
</span></span><span style="display:flex;"><span>        masks <span style="color:#f92672">=</span> masks[:, mask_slice, :, :]
</span></span></code></pre></div><p>The IoU tokens and mask tokens are concatenated with sparse prompt embeddings before passing them through the Transformer.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MaskDecoder</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(): <span style="color:#75715e">#skip parameters</span>
</span></span><span style="display:flex;"><span>        masks, iou_pred <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>predict_masks(
</span></span><span style="display:flex;"><span>            image_embeddings<span style="color:#f92672">=</span>image_embeddings,
</span></span><span style="display:flex;"><span>            image_pe<span style="color:#f92672">=</span>image_pe,
</span></span><span style="display:flex;"><span>            sparse_prompt_embeddings<span style="color:#f92672">=</span>sparse_prompt_embeddings,
</span></span><span style="display:flex;"><span>            dense_prompt_embeddings<span style="color:#f92672">=</span>dense_prompt_embeddings,
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict_masks</span>(
</span></span><span style="display:flex;"><span>        self,
</span></span><span style="display:flex;"><span>        image_embeddings: torch<span style="color:#f92672">.</span>Tensor,
</span></span><span style="display:flex;"><span>        image_pe: torch<span style="color:#f92672">.</span>Tensor,
</span></span><span style="display:flex;"><span>        sparse_prompt_embeddings: torch<span style="color:#f92672">.</span>Tensor,
</span></span><span style="display:flex;"><span>        dense_prompt_embeddings: torch<span style="color:#f92672">.</span>Tensor,
</span></span><span style="display:flex;"><span>    ):
</span></span><span style="display:flex;"><span>        output_tokens <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([self<span style="color:#f92672">.</span>iou_token<span style="color:#f92672">.</span>weight, self<span style="color:#f92672">.</span>mask_tokens<span style="color:#f92672">.</span>weight], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>        output_tokens <span style="color:#f92672">=</span> output_tokens<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>expand(sparse_prompt_embeddings<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>), <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        tokens <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat((output_tokens, sparse_prompt_embeddings), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        tokens <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat((output_tokens, sparse_prompt_embeddings), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        src <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>repeat_interleave(image_embeddings, tokens<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>        src <span style="color:#f92672">=</span> src <span style="color:#f92672">+</span> dense_prompt_embeddings
</span></span><span style="display:flex;"><span>        pos_src <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>repeat_interleave(image_pe, tokens<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Run the transformer</span>
</span></span><span style="display:flex;"><span>        hs, src <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transformer(src, pos_src, tokens)
</span></span></code></pre></div><p><code>tokens</code> tensor has shape of \(\mathbb{R}^{B \times (N + 5) \times 256}\) where \(N\) is the number of sparse prompts. <code>src = torch.repeat_interleave(image_embeddings, tokens.shape[0], dim=0)</code> expands image embedding in batch dimension from \(\mathbb{R}^{B \times 256 \times 64 \times 64}\) to \(\mathbb{R}^{B' \times 256 \times 64 \times 64}\) if <code>tokens</code> batch size and <code>image_embeddings</code> batch size are different. But I am still not sure why they wrote this line. I assume these two batch sizes are always the same. Lastly, it adds <code>image_embeddings</code> to <code>dense_prompt_embeddings</code>. Now, we are done for input processing before passing to the decoder. <code>self.transformer</code> takes three inputs:</p>
<ul>
<li><code>src</code>: image embedding + dense prompt</li>
<li><code>pos_src</code>: image positional embedding</li>
<li><code>tokens</code>:   mask token \(\oplus\) IoU token \(\oplus\)sparse prompt embeddings</li>
</ul>
<h3 id="42-twowayattention-transformer">4.2. TwoWayAttention Transformer<a hidden class="anchor" aria-hidden="true" href="#42-twowayattention-transformer">#</a></h3>
<p>SAM’s mask decoder utilizes a TwoWayTransformer, which differs from a standard transformer decoder by incorporating two cross-attention stages: (1) tokens attending to image features and (2) image features attending to tokens. This bidirectional attention mechanism allows the model to effectively refine mask predictions by leveraging both sparse and dense prompts. The TwoWayTransformer consists of multiple layers (depth) of TwoWayAttentionBlock modules, followed by a final attention layer for mask prediction.</p>
<p>The TwoWayTransformer takes three main inputs:</p>
<ul>
<li>
<p><code>image_embedding</code> (B, 256, 64, 64): Image features with dense prompt (i.e. \(I + M\)).</p>
</li>
<li>
<p><code>image_pe</code> (B, 256, 64, 64): Positional encodings for image features.</p>
</li>
<li>
<p><code>point_embedding</code> (B, N+5, 256): Encoded sparse prompts.</p>
</li>
</ul>
<p>The image embedding is first flattened from (B, 256, H, W) → (B, HW, 256) so that it can interact with the mask tokens.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>bs, c, h, w <span style="color:#f92672">=</span> image_embedding<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span>image_embedding <span style="color:#f92672">=</span> image_embedding<span style="color:#f92672">.</span>flatten(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>image_pe <span style="color:#f92672">=</span> image_pe<span style="color:#f92672">.</span>flatten(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)
</span></span></code></pre></div><p>Next, the query tokens (mask tokens + IoU token) interact with the image features via two stacked TwoWayAttentionBlock layers:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>queries <span style="color:#f92672">=</span> point_embedding
</span></span><span style="display:flex;"><span>keys <span style="color:#f92672">=</span> image_embedding
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> layer <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>layers:
</span></span><span style="display:flex;"><span>    queries, keys <span style="color:#f92672">=</span> layer(
</span></span><span style="display:flex;"><span>        queries<span style="color:#f92672">=</span>queries,
</span></span><span style="display:flex;"><span>        keys<span style="color:#f92672">=</span>keys,
</span></span><span style="display:flex;"><span>        query_pe<span style="color:#f92672">=</span>point_embedding,
</span></span><span style="display:flex;"><span>        key_pe<span style="color:#f92672">=</span>image_pe,
</span></span><span style="display:flex;"><span>    )
</span></span></code></pre></div><p>We are passing image embedding and image positional embedding for <code>keys</code> and <code>key_pe</code>. But <code>query_pe</code> is just copy of <code>query</code>. Why are passing the two arguments for two different parameters? Well, we don&rsquo;t have a separate postional encoding for <code>point_embedding</code>, which is concatenation of IoU tokens, mask tokens, and sparse prompt embeddings. However, the sparse prompt embedding was computed using positional encoding. Even if we are passing the <code>point_embedding</code> itself as positional encdoing, it has chance to learn positional information through attention mechanism. Instead, the embeddings themselves serve both as features and positional encodings, <code>query_pe = point_embedding</code>.</p>
<p>Let&rsquo;s break down the two way attention block. The below diagram is a visualizaation of the two way attention block.</p>
<p><img loading="lazy" src="/images/2025-01-29_SAM/twoWayAttention.png" alt="twoWayAttention"  />
</p>
<ol>
<li>
<p>Self-Attention (Tokens)</p>
<ul>
<li>If it&rsquo;s the first layer, positional encoding is skipped.</li>
<li>Otherwise, the positional encoding (query_pe) is added before passing through the self-attention layer.</li>
</ul>
</li>
<li>
<p>Cross-Attention (Tokens → Image Embeddings)</p>
<ul>
<li>Tokens (queries) attend to image embeddings (keys).</li>
<li>This allows the sparse prompts (mask tokens, IoU tokens) to interact with image features.</li>
</ul>
</li>
<li>
<p>MLP Block</p>
<ul>
<li>The sparse queries are passed through an MLP block for further refinement.</li>
</ul>
</li>
<li>
<p>Cross-Attention (Image Embeddings → Tokens)</p>
<ul>
<li>Now, the image features (keys) attend back to the sparse queries (queries).</li>
<li>This lets the image embeddings influence the sparse tokens.</li>
</ul>
</li>
</ol>
<p>After the two layers, a final cross-attention layer is applied where queries and keys interact again:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>q <span style="color:#f92672">=</span> queries <span style="color:#f92672">+</span> point_embedding
</span></span><span style="display:flex;"><span>k <span style="color:#f92672">=</span> keys <span style="color:#f92672">+</span> image_pe
</span></span><span style="display:flex;"><span>attn_out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>final_attn_token_to_image(q<span style="color:#f92672">=</span>q, k<span style="color:#f92672">=</span>k, v<span style="color:#f92672">=</span>keys)
</span></span><span style="display:flex;"><span>queries <span style="color:#f92672">=</span> queries <span style="color:#f92672">+</span> attn_out
</span></span><span style="display:flex;"><span>queries <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>norm_final_attn(queries)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">return</span> queries, keys
</span></span></code></pre></div><p>In the end, two way transformer returns tokens (IoU tokens, mask tokens, and sparse embedding) and image embedding. I recommends to check the implementation <a href="https://github.com/facebookresearch/segment-anything/blob/main/segment_anything/modeling/transformer.py">source code</a>.</p>
<h3 id="43-final-output">4.3. Final Output<a hidden class="anchor" aria-hidden="true" href="#43-final-output">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MaskDecoder</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">init</span>():
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#skip</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>output_upscaling <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ConvTranspose2d(transformer_dim, transformer_dim <span style="color:#f92672">//</span> <span style="color:#ae81ff">4</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),
</span></span><span style="display:flex;"><span>            LayerNorm2d(transformer_dim <span style="color:#f92672">//</span> <span style="color:#ae81ff">4</span>),
</span></span><span style="display:flex;"><span>            activation(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ConvTranspose2d(transformer_dim <span style="color:#f92672">//</span> <span style="color:#ae81ff">4</span>, transformer_dim <span style="color:#f92672">//</span> <span style="color:#ae81ff">8</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),
</span></span><span style="display:flex;"><span>            activation(),
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>output_hypernetworks_mlps <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleList(
</span></span><span style="display:flex;"><span>            [
</span></span><span style="display:flex;"><span>                MLP(transformer_dim, transformer_dim, transformer_dim <span style="color:#f92672">//</span> <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">3</span>)
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>num_mask_tokens)
</span></span><span style="display:flex;"><span>            ]
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>iou_prediction_head <span style="color:#f92672">=</span> MLP(
</span></span><span style="display:flex;"><span>            transformer_dim, iou_head_hidden_dim, self<span style="color:#f92672">.</span>num_mask_tokens, iou_head_depth
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict_masks</span>():
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#skip...</span>
</span></span><span style="display:flex;"><span>        hs, src <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transformer(src, pos_src, tokens)
</span></span><span style="display:flex;"><span>        iou_token_out <span style="color:#f92672">=</span> hs[:, <span style="color:#ae81ff">0</span>, :]
</span></span><span style="display:flex;"><span>        mask_tokens_out <span style="color:#f92672">=</span> hs[:, <span style="color:#ae81ff">1</span> : (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>num_mask_tokens), :]
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Upscale mask embeddings and predict masks using the mask tokens</span>
</span></span><span style="display:flex;"><span>        src <span style="color:#f92672">=</span> src<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>view(b, c, h, w)
</span></span><span style="display:flex;"><span>        upscaled_embedding <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>output_upscaling(src)
</span></span><span style="display:flex;"><span>        hyper_in_list: List[torch<span style="color:#f92672">.</span>Tensor] <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>num_mask_tokens):
</span></span><span style="display:flex;"><span>            hyper_in_list<span style="color:#f92672">.</span>append(self<span style="color:#f92672">.</span>output_hypernetworks_mlps[i](mask_tokens_out[:, i, :]))
</span></span><span style="display:flex;"><span>        hyper_in <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>stack(hyper_in_list, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        b, c, h, w <span style="color:#f92672">=</span> upscaled_embedding<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span>        masks <span style="color:#f92672">=</span> (hyper_in <span style="color:#f92672">@</span> upscaled_embedding<span style="color:#f92672">.</span>view(b, c, h <span style="color:#f92672">*</span> w))<span style="color:#f92672">.</span>view(b, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, h, w)
</span></span><span style="display:flex;"><span>        iou_pred <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>iou_prediction_head(iou_token_out)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> masks, iou_pred
</span></span></code></pre></div><p>After transformer processed the image embedding and tokens, we extracts IoU token and mask token from <code>tokens</code>. The <code>mask_tokens_out</code> dimension is \(\mathbb{R}^{B \times 4 \times 256}\). We want masks to be 4-dimensional shape, \(\mathbb{R}^{B \times 4 \times H \times W}\). The mask tokens are transformed into mask predictions via hypernetworks, and the upscaled image features are used for final mask refinement.</p>
<p><code>src</code> represents the transformed image embeddings after passing through the transformer. We reshape <code>src</code> dimension form (B, HW, C)  to (B, C, H, W). <code>self.output_upscaling(src)</code> applies an upscaling operation using two transposed convolution layers.</p>
<p>mask_tokens_out is of shape (B, 4, 256). Each mask token, <code>mask_tokens_out[:, i, :]</code>, is passed through a hypernetwork MLP. self.output_hypernetworks_mlps[i] is an MLP that processes each mask token separately. The matrix multiplication of <code>hyper_in</code> by <code>upscaled_embedding</code>, followed by reshaping, results in <code>masks</code> shaped (B, 4, H, W). Another MLP maps it to the final IoU prediction scores, indicating the confidence of each mask.</p>
<h2 id="discusssion">Discusssion<a hidden class="anchor" aria-hidden="true" href="#discusssion">#</a></h2>
<p>After reading the entire paper and exploring other references, I found myself wondering—why is SAM receiving so much praise? Given its high citation count and widespread adoption in both industry and academia, it’s clear that SAM is considered a game-changer. But why? Interactive segmentation and transformer-based architectures aren’t new concepts. Researchers have been exploring these areas for years. So, what makes SAM stand out?</p>
<p>The key lies in its large-scale dataset and model training. The team behind SAM didn’t just build another segmentation model; they demonstrated that scaling up both the dataset and the model itself leads to remarkable performance gains. This aligns with the proven scaling laws in deep learning, where larger models trained on massive datasets tend to generalize better and unlock new capabilities. SAM isn’t just an incremental improvement—it’s a demonstration of how foundation models in computer vision can follow the same trajectory as large language models, fundamentally shifting how we approach image segmentation.</p>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>The Segment Anything Model (SAM) represents a significant advancement in the field of computer vision, particularly in image segmentation. By leveraging a promptable architecture, SAM eliminates the need for task-specific fine-tuning, enabling zero-shot learning across various segmentation tasks. Its three core components—the image encoder, prompt encoder, and mask decoder—work in harmony to generate precise segmentation masks based on user-provided prompts such as points, boxes, or masks. SAM&rsquo;s ability to handle sparse and dense prompts, combined with its efficient use of positional embeddings and transformer-based decoding, makes it a versatile and powerful tool for segmentation.</p>
<h2 id="reference">Reference<a hidden class="anchor" aria-hidden="true" href="#reference">#</a></h2>
<ul>
<li><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kirillov_Segment_Anything_ICCV_2023_paper.html">Segment Anything</a></li>
<li><a href="https://arxiv.org/pdf/2305.08196">A Comprehensive Survey on Segment Anything Model for Vision and Beyond</a></li>
<li><a href="https://www.youtube.com/watch?v=OhxJkqD1vuE&amp;t=280s">Explaining the Segment Anything Model - Network architecture, Dataset, Training</a></li>
<li><a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/ipr2.12419">Medical image segmentation using deep learning: A survey</a></li>
<li><a href="https://towardsdatascience.com/how-does-the-segment-anything-models-sam-s-encoder-work-003a8a6e3f8b">How Does the Segment-Anything Model’s (SAM’s) Encoder Work?</a></li>
<li><a href="https://www.youtube.com/watch?v=n13-r_eStb0">Transformer self-attention padding and causal masking technique</a></li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://baampark.github.io/tags/segmentation/">Segmentation</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://baampark.github.io/">Baam&#39;s Techlog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
