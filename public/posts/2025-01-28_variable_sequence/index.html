<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>How Transformers Handle Variable-length Sequnces | Baam&#39;s Techlog</title>
<meta name="keywords" content="NLP, Transformer">
<meta name="description" content="&ldquo;Transformer models don&rsquo;t require a fixed sequence length.&rdquo; Since most of my projects revolve around computer vision, this was very confusing to me. In computer vision models, images are always preprocessed to a fixed size before being fed into deep learning models. Otherwise, you will encounter matrix multiplication error. In this post, we will learn how transofrmer handles variable-length sequnces.
Self-attention - Q, K, V Linear Projection into Embedding Space Let&rsquo;s see basic CNN code example.">
<meta name="author" content="">
<link rel="canonical" href="https://baampark.github.io/posts/2025-01-28_variable_sequence/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css" integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://baampark.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://baampark.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://baampark.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://baampark.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://baampark.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://baampark.github.io/posts/2025-01-28_variable_sequence/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
<meta property="og:title" content="How Transformers Handle Variable-length Sequnces" />
<meta property="og:description" content="&ldquo;Transformer models don&rsquo;t require a fixed sequence length.&rdquo; Since most of my projects revolve around computer vision, this was very confusing to me. In computer vision models, images are always preprocessed to a fixed size before being fed into deep learning models. Otherwise, you will encounter matrix multiplication error. In this post, we will learn how transofrmer handles variable-length sequnces.
Self-attention - Q, K, V Linear Projection into Embedding Space Let&rsquo;s see basic CNN code example." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://baampark.github.io/posts/2025-01-28_variable_sequence/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-01-27T13:49:47-05:00" />
<meta property="article:modified_time" content="2025-01-27T13:49:47-05:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="How Transformers Handle Variable-length Sequnces"/>
<meta name="twitter:description" content="&ldquo;Transformer models don&rsquo;t require a fixed sequence length.&rdquo; Since most of my projects revolve around computer vision, this was very confusing to me. In computer vision models, images are always preprocessed to a fixed size before being fed into deep learning models. Otherwise, you will encounter matrix multiplication error. In this post, we will learn how transofrmer handles variable-length sequnces.
Self-attention - Q, K, V Linear Projection into Embedding Space Let&rsquo;s see basic CNN code example."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://baampark.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "How Transformers Handle Variable-length Sequnces",
      "item": "https://baampark.github.io/posts/2025-01-28_variable_sequence/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "How Transformers Handle Variable-length Sequnces",
  "name": "How Transformers Handle Variable-length Sequnces",
  "description": "\u0026ldquo;Transformer models don\u0026rsquo;t require a fixed sequence length.\u0026rdquo; Since most of my projects revolve around computer vision, this was very confusing to me. In computer vision models, images are always preprocessed to a fixed size before being fed into deep learning models. Otherwise, you will encounter matrix multiplication error. In this post, we will learn how transofrmer handles variable-length sequnces.\nSelf-attention - Q, K, V Linear Projection into Embedding Space Let\u0026rsquo;s see basic CNN code example.",
  "keywords": [
    "NLP", "Transformer"
  ],
  "articleBody": " “Transformer models don’t require a fixed sequence length.” Since most of my projects revolve around computer vision, this was very confusing to me. In computer vision models, images are always preprocessed to a fixed size before being fed into deep learning models. Otherwise, you will encounter matrix multiplication error. In this post, we will learn how transofrmer handles variable-length sequnces.\nSelf-attention - Q, K, V Linear Projection into Embedding Space Let’s see basic CNN code example.\nclass SimpleCNN(nn.Module): def __init__(self, input_channels=3, num_classes=10): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=16, kernel_size=3, padding=1) # (B, 16, H, W) self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1) # (B, 32, H/2, W/2) self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1) # (B, 64, H/4, W/4) self.pool = nn.MaxPool2d(kernel_size=2, stride=2) # Reduces spatial size by half self.fc1 = nn.Linear(64 * 4 * 4, 128) # Assuming input images are 32x32 self.fc2 = nn.Linear(128, num_classes) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = self.pool(F.relu(self.conv3(x))) B, C, H, W = x.shape x = x.view(B, C * H * W) x = F.relu(self.fc1(x)) x = self.fc2(x) # Output logits return x B, C, H, W = 32, 3, 32, 32 # Batch of 32 RGB images (32x32 pixels) num_classes = 10 # e.g., CIFAR-10 dataset model = SimpleCNN(input_channels=C, num_classes=num_classes) The line x = x.view(B, C * H * W) flattens height, and width dimension. If you pass input tensor torch.randn(B, C, 52, 33), you will see an error because self.fc1 layer is a matrix \\(W \\in \\mathbb{R}^{128, 1024}\\), which requires a specific feature dimension.\nIn natual langauge processing (NLP) mdoels, the input shape is \\(B, N, C\\) where \\(N\\) can be arbitrary. This type of input is called a variable-length sequence, which is more common in NLP. The model cannot handle variable-length input if the first dimension of nn.Linear weight matrix is \\(N \\times C\\). Let’s see how transofrmer handle variable-length sequences during the self-attention.\nclass SelfAttention(nn.Module): def __init__(self, embed_dim: int): # size is hidden size super(SelfAttention, self).__init__() self.query = nn.Linear(embed_dim, embed_dim) self.key = nn.Linear(embed_dim, embed_dim) self.value = nn.Linear(embed_dim, embed_dim) def forward(self, input_tensor: torch.Tensor): q, k, v = self.query(input_tensor), self.key(input_tensor), self.value(input_tensor) scale = q.size(1) ** 0.5 scores = torch.bmm(q, k.transpose(1, 2)) / scale scores = F.softmax(scores, dim=-1) output = torch.bmm(scores, v) return output The weight matrices \\(W_Q, W_K, W_V \\in \\mathbb{R}^{C, C}\\) mean that nn.Linear does not expect the feature tensor to be flattened. Since the linear projection layer’s Q, K, and V matrix dimensions depend on the feature embedding dimensionn \\(C\\), there will be no multiplication error. The linear projection of Q, K, and V preserves the sequence length, allowing the model to handle variable-length inputs.\nSelf-attention - Padding and Masking What about we include batch? Let’s say a batch sized four and forward pass to transformer.\n“This is a short sentence” \\(N=7\\) “This one is much longer and contains more words” \\(N=8\\) “Tiny” \\(N=3\\) “More words, more sequnces” \\(N=6\\) The sequence lengths vary within the batch. You can’t feed this batch to the model due to inconsistent sequence dimension. Transformers require input of shape \\(\\mathbb{R}^{B \\times T_{\\text{max}} \\times C}\\) where \\(T_{\\text{max}}\\) is the length of the longest sequence in the batch. We can simply address inconsistency by using padding. In our example, the longest sequence length within the batch is 8. We can add paddings to the shorter sequences so that all sequences have a uniform length of 8.\nHowever, padding introduces irrelevant tokens that should not contribute to the model’s computations. To handle this, transformers use attention masks, which indicate which tokens are real and which are padding. Let’s see how self-attention is performed from the below image.\nIn the given image, we see how padding masks are applied to ensure that padded tokens do not interfere with the self-attention mechanism. Since attention scores are computed using a dot product of queries and keys, padding tokens would otherwise contribute to the output and affect model performance. By adding a mask filled with negative infinity (-∞) for padding positions, the softmax function effectively zeroes out their influence. This ensures that only meaningful tokens participate in the attention computation while maintaining a uniform sequence length across the batch.\nConclussion Deep learning models don’t require strict input dimensions, but they do need careful design to handle variable-sized inputs effectively. By strategically using padding and attention masks, transformers can process sequences of different lengths without introducing errors in matrix operations. We learned how padding ensures uniform input sizes across a batch and how attention masks prevent padded tokens from affecting self-attention computations. Understanding these techniques is essential for efficiently training and deploying transformer-based models in NLP and beyond.\n",
  "wordCount" : "768",
  "inLanguage": "en",
  "datePublished": "2025-01-27T13:49:47-05:00",
  "dateModified": "2025-01-27T13:49:47-05:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://baampark.github.io/posts/2025-01-28_variable_sequence/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Baam's Techlog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://baampark.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://baampark.github.io/" accesskey="h" title="Baam&#39;s Techlog (Alt + H)">Baam&#39;s Techlog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://baampark.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://baampark.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      How Transformers Handle Variable-length Sequnces
    </h1>
    <div class="post-meta"><span title='2025-01-27 13:49:47 -0500 EST'>January 27, 2025</span>

</div>
  </header> 
  <div class="post-content"><p><img loading="lazy" src="/images/2025-01-28_variable_sequence/cover.png" alt="cover"  />

&ldquo;Transformer models don&rsquo;t require a fixed sequence length.&rdquo; Since most of my projects revolve around computer vision, this was very confusing to me. In computer vision models, images are always preprocessed to a fixed size before being fed into deep learning models. Otherwise, you will encounter matrix multiplication error. In this post, we will learn how transofrmer handles variable-length sequnces.</p>
<h2 id="self-attention---q-k-v-linear-projection-into-embedding-space">Self-attention - Q, K, V Linear Projection into Embedding Space<a hidden class="anchor" aria-hidden="true" href="#self-attention---q-k-v-linear-projection-into-embedding-space">#</a></h2>
<p>Let&rsquo;s see basic CNN code example.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SimpleCNN</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, input_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, num_classes<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>):
</span></span><span style="display:flex;"><span>        super(SimpleCNN, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(in_channels<span style="color:#f92672">=</span>input_channels, out_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># (B, 16, H, W)</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(in_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>, out_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># (B, 32, H/2, W/2)</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv3 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(in_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>, out_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># (B, 64, H/4, W/4)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>pool <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>MaxPool2d(kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)  <span style="color:#75715e"># Reduces spatial size by half</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">64</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">4</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">128</span>)  <span style="color:#75715e"># Assuming input images are 32x32</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">128</span>, num_classes)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>pool(F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>conv1(x)))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>pool(F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>conv2(x)))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>pool(F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>conv3(x)))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        B, C, H, W <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(B, C <span style="color:#f92672">*</span> H <span style="color:#f92672">*</span> W)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>fc1(x))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>fc2(x)  <span style="color:#75715e"># Output logits</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>B, C, H, W <span style="color:#f92672">=</span> <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">32</span>  <span style="color:#75715e"># Batch of 32 RGB images (32x32 pixels)</span>
</span></span><span style="display:flex;"><span>num_classes <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>  <span style="color:#75715e"># e.g., CIFAR-10 dataset</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> SimpleCNN(input_channels<span style="color:#f92672">=</span>C, num_classes<span style="color:#f92672">=</span>num_classes)
</span></span></code></pre></div><p>The line <code>x = x.view(B, C * H * W)</code> flattens height, and width dimension. If you pass input tensor <code>torch.randn(B, C, 52, 33)</code>, you will see an error because <code>self.fc1</code> layer is a matrix \(W \in \mathbb{R}^{128, 1024}\), which requires a specific feature dimension.</p>
<p>In natual langauge processing (NLP) mdoels, the input shape is \(B, N, C\) where \(N\) can be arbitrary. This type of input is called a <strong>variable-length sequence</strong>, which is more common in NLP. The model cannot handle variable-length input if the first dimension of <code>nn.Linear</code> weight matrix is \(N \times C\). Let&rsquo;s see how transofrmer handle variable-length sequences during the self-attention.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SelfAttention</span>(nn<span style="color:#f92672">.</span>Module):  
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, embed_dim: int): <span style="color:#75715e"># size is hidden size </span>
</span></span><span style="display:flex;"><span>        super(SelfAttention, self)<span style="color:#f92672">.</span>__init__()  
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>query <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(embed_dim, embed_dim)  
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>key <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(embed_dim, embed_dim)  
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>value <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(embed_dim, embed_dim)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input_tensor: torch<span style="color:#f92672">.</span>Tensor):  
</span></span><span style="display:flex;"><span>        q, k, v <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>query(input_tensor), self<span style="color:#f92672">.</span>key(input_tensor), self<span style="color:#f92672">.</span>value(input_tensor)  
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>        scale <span style="color:#f92672">=</span> q<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">1</span>) <span style="color:#f92672">**</span> <span style="color:#ae81ff">0.5</span>  
</span></span><span style="display:flex;"><span>        scores <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>bmm(q, k<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)) <span style="color:#f92672">/</span> scale  
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>        scores <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>softmax(scores, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        output <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>bmm(scores, v)  
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> output
</span></span></code></pre></div><p>The weight matrices \(W_Q, W_K, W_V \in \mathbb{R}^{C, C}\)
mean that nn.Linear does not expect the feature tensor to be flattened. Since the linear projection layer&rsquo;s Q, K, and V matrix dimensions depend on the feature embedding dimensionn \(C\), there will be no multiplication error. The linear projection of Q, K, and V preserves the sequence length, allowing the model to handle variable-length inputs.</p>
<h2 id="self-attention---padding-and-masking">Self-attention - Padding and Masking<a hidden class="anchor" aria-hidden="true" href="#self-attention---padding-and-masking">#</a></h2>
<p>What about we include batch? Let&rsquo;s say a batch sized four and forward pass to transformer.</p>
<ul>
<li>&ldquo;This is a short sentence&rdquo; \(N=7\)</li>
<li>&ldquo;This one is much longer and contains more words&rdquo; \(N=8\)</li>
<li>&ldquo;Tiny&rdquo; \(N=3\)</li>
<li>&ldquo;More words, more sequnces&rdquo; \(N=6\)</li>
</ul>
<p>The sequence lengths vary within the batch. You can&rsquo;t feed this batch to the model due to inconsistent sequence dimension. Transformers require input of shape \(\mathbb{R}^{B \times T_{\text{max}} \times C}\) where \(T_{\text{max}}\) is the length of the longest sequence in the batch. We can simply address inconsistency by using padding. In our example, the longest sequence length within the batch is 8. We can add paddings to the shorter sequences so that all sequences have a uniform length of 8.</p>
<p>However, padding introduces irrelevant tokens that should not contribute to the model&rsquo;s computations. To handle this, transformers use attention masks, which indicate which tokens are real and which are padding. Let&rsquo;s see how self-attention is performed from the below image.</p>
<p><img loading="lazy" src="/images/2025-01-28_variable_sequence/masking_padding.png" alt="masking and padding"  />
</p>
<p>In the given image, we see how padding masks are applied to ensure that padded tokens do not interfere with the self-attention mechanism. Since attention scores are computed using a dot product of queries and keys, padding tokens would otherwise contribute to the output and affect model performance. By adding a mask filled with negative infinity (-∞) for padding positions, the softmax function effectively zeroes out their influence. This ensures that only meaningful tokens participate in the attention computation while maintaining a uniform sequence length across the batch.</p>
<h2 id="conclussion">Conclussion<a hidden class="anchor" aria-hidden="true" href="#conclussion">#</a></h2>
<p>Deep learning models don’t require strict input dimensions, but they do need careful design to handle variable-sized inputs effectively. By strategically using padding and attention masks, transformers can process sequences of different lengths without introducing errors in matrix operations. We learned how padding ensures uniform input sizes across a batch and how attention masks prevent padded tokens from affecting self-attention computations. Understanding these techniques is essential for efficiently training and deploying transformer-based models in NLP and beyond.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://baampark.github.io/tags/nlp/">NLP</a></li>
      <li><a href="https://baampark.github.io/tags/transformer/">Transformer</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://baampark.github.io/">Baam&#39;s Techlog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
