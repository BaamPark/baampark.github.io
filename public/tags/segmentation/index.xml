<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Segmentation on Baam&#39;s Techlog</title>
    <link>http://localhost:1313/tags/segmentation/</link>
    <description>Recent content in Segmentation on Baam&#39;s Techlog</description>
    <generator>Hugo -- 0.143.1</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Feb 2025 12:19:07 -0500</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/segmentation/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Segment Anything 2 vs. SAM1: Whatâ€™s New and Why It Matters</title>
      <link>http://localhost:1313/posts/2025-02-06_sam2/</link>
      <pubDate>Thu, 06 Feb 2025 12:19:07 -0500</pubDate>
      <guid>http://localhost:1313/posts/2025-02-06_sam2/</guid>
      <description>&lt;p&gt;In my last post, we explored how Segment Anything (SAM) works in image segmentation, breaking down the key components of its model architecture. SAM achieved great success in image segmentation, demonstrating two key strengths: its foundation as a large-scale model trained on an extensive dataset and its ability to be promptable, allowing users to generate segmentations with flexible inputs.&lt;/p&gt;
&lt;p&gt;In Jul 2024, &lt;a href=&#34;https://arxiv.org/abs/2408.00714&#34;&gt;&amp;ldquo;SAM 2: Segment Anything in Images and Videos&amp;rdquo;&lt;/a&gt; was published. SAM2&lt;/p&gt;</description>
    </item>
    <item>
      <title>Segment Anything, the first large-scale foundation model for segmentation</title>
      <link>http://localhost:1313/posts/2025-01-29_sam/</link>
      <pubDate>Wed, 29 Jan 2025 13:49:47 -0500</pubDate>
      <guid>http://localhost:1313/posts/2025-01-29_sam/</guid>
      <description>&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/images/2025-01-29_SAM/intro.png&#34; alt=&#34;SAM&#34;  /&gt;

&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023/html/Kirillov_Segment_Anything_ICCV_2023_paper.html&#34;&gt;Segment Anything (SAM)&lt;/a&gt; has drawn massive attention in the computer vision community, accumulating an impressive 8,000 citations. Segmentation has long been a crucial yet challenging aspect of computer vision. One of the biggest hurdles? Annotation. Unlike simple bounding boxes, which only require marking the objectâ€™s general location, segmentation demands precise pixel-level annotationsâ€”an incredibly tedious and time-consuming task for annotators. SAM is one of the first large-scale foundation models for segmentation. What makes SAM truly great is that itâ€™s a promptable model. This means you can use it for various tasks without the need for fine-tuningâ€”also known as zero-shot learning. Unlike LLM, here the prompts for the SAM are points, bounding boxes, and masks. In this post, weâ€™re going to explore the key components of SAM. This guide will break things down in a simple and easy-to-follow way. Letâ€™s get started! ðŸš€.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
