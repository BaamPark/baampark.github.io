<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>ML on Baam&#39;s Techlog</title>
    <link>http://localhost:1313/tags/ml/</link>
    <description>Recent content in ML on Baam&#39;s Techlog</description>
    <generator>Hugo -- 0.143.0</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 18 Jul 2024 04:05:02 -0400</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/ml/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Low Rank Adaptation</title>
      <link>http://localhost:1313/posts/2024-07-18_lora/</link>
      <pubDate>Thu, 18 Jul 2024 04:05:02 -0400</pubDate>
      <guid>http://localhost:1313/posts/2024-07-18_lora/</guid>
      <description>&lt;h1 id=&#34;why-low-rank-adaptation-matters-a-closer-look-at-its-impact-on-machine-learning&#34;&gt;Why Low Rank Adaptation Matters: A Closer Look at Its Impact on Machine Learning&lt;/h1&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/images/2024-07-18_LoRA/trainingStep.png&#34; alt=&#34;Alt text&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;Low Rank Adaptation (LoRA) is a fine-tuning technique designed to efficiently update and adapt large pre-trained models, such as language or diffusion models, without retraining them entirely. Low Rank Adaptation was proposed in 2021 by Edward Hu et al. They demonstrated that LoRA significantly reduces the number of trainable parameters and GPU memory requirements. But how is that possible? In this blog post, we will explore LoRA and understand the foundational principles underlying its concept.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
